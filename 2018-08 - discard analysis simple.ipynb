{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# Enables figures loading outside of browser.\n",
    "# If not run, figures will load inline.\n",
    "%matplotlib\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Depends on: pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')\n",
    "\n",
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def run_query(query, project='mlab-sandbox'):\n",
    "    #print query\n",
    "    client = bigquery.Client(project=project)\n",
    "    job = client.query(query)\n",
    "\n",
    "    results = collections.defaultdict(list)\n",
    "    for row in job.result(timeout=300):\n",
    "        for key in row.keys():\n",
    "            results[key].append(row.get(key))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def unlog(x, pos):\n",
    "    \"\"\"Formats the x axis for histograms taken on the log of values.\"\"\"\n",
    "    v = math.pow(10, x)\n",
    "    frac, whole = math.modf(v)\n",
    "    if frac > 0:\n",
    "        return '%.1f' % v\n",
    "    else:\n",
    "        return '%d' % whole\n",
    "\n",
    "logFormatter = matplotlib.ticker.FuncFormatter(unlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(\n",
    "    df, xname, yname,\n",
    "    fig_by='', axes_by='', group_by='',\n",
    "    figsize=(6,8), axes=(1,1),\n",
    "    xlabel='', ylabel='',\n",
    "    xlim=(), ylim=(),\n",
    "    fx=list, fy=list,\n",
    "    ylog=False, title='', legend={}):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        xname: str, name of column to use as x-axis.\n",
    "        yname: str, name of column to use as y-axis.\n",
    "        fig_by: str, name of column to split data into multiple figures.\n",
    "        axes_by: str, name of column to arrange into a single panel.\n",
    "        group_by: str, name of column to plot common split_by and group_by columns.\n",
    "        figsize: (int, int), dimensions of figure.\n",
    "        axes: (int, int), arrangement of axes within figure.\n",
    "        xlabel: str, \n",
    "        ylabel: str, \n",
    "        fx: func,\n",
    "        fy: func,\n",
    "        xlim: (xmin, xmax),\n",
    "        ylim: (ymin, ymax),\n",
    "        ylog: bool,\n",
    "        title: str,\n",
    "        legend: **legend_args,\n",
    "    \"\"\"\n",
    "    for fname in sorted(set(['default'] if not fig_by else df[fig_by])):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.subplots(axes[0], axes[1], squeeze=False)\n",
    "        ax_index = list(itertools.product(range(axes[0]), range(axes[1])))\n",
    "        \n",
    "        #print fig_by, fname\n",
    "        df_fig = df if fname == 'default' else df[df[fig_by] == fname]\n",
    "        for p, a in enumerate(sorted(set(['default'] if not axes_by else df_fig[axes_by]))):\n",
    "            #print axes_by, a\n",
    "            df_axes = df_fig if a == 'default' else df_fig[df_fig[axes_by] == a]\n",
    "\n",
    "            i, j = ax_index[p]\n",
    "            for g in sorted(set(['default'] if not group_by else df_axes[group_by])):\n",
    "                df_g = df_axes if g == 'default' else df_axes[df_axes[group_by] == g]\n",
    "\n",
    "                x = fx(df_g[xname])\n",
    "                y = fy(df_g[yname])\n",
    "\n",
    "                ax[i][j].scatter(x, y, s=1, label=g)\n",
    "\n",
    "            if i != len(ax)-1:\n",
    "                ax[i][j].set_xticklabels([])\n",
    "\n",
    "            if ylabel:\n",
    "                print a\n",
    "                ax[i][j].set_ylabel(ylabel.format(a.upper()))\n",
    "            if xlabel:\n",
    "                ax[i][j].set_xlabel(xlabel)\n",
    "\n",
    "            if xlim:\n",
    "                ax[i][j].set_xlim(xlim)\n",
    "            if ylim:\n",
    "                ax[i][j].set_ylim(ylim)\n",
    "            ax[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "            ax[i][j].grid(color='#dddddd')\n",
    "            ax[i][j].legend(fontsize='x-small', **legend)\n",
    "            if ylog:\n",
    "                ax[i][j].semilogy()\n",
    "\n",
    "        fig.suptitle(title.format(fname))\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLINK UTILIZATION OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_pct = run_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,  \n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(50)] as bytes_50th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(90)] as bytes_90th\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab1.[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(\n",
    "    df_disco_pct, 'ts', 'bytes_50th', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    title='Daily Median Uplink Utilization',\n",
    "    ylabel=\"Median Uplink {0}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e4, 1e9),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':3, 'ncol':7, 'columnspacing':1},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites = [\n",
    "#    ['dfw'],\n",
    "#    ['lga'],\n",
    "#    ['nuq'],\n",
    "#]\n",
    "#\n",
    "#axes = [\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],\n",
    "#]\n",
    "#\n",
    "#fig = plt.figure(figsize=(6, 8))\n",
    "#for i, site_row in enumerate(sites):\n",
    "#    for j, site in enumerate(site_row):\n",
    "#        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "#        axes[i][j].set_ylabel('Median Uplink ' + site.upper())##\n",
    "#\n",
    "#        if i != len(sites)-1:\n",
    "#            axes[i][j].set_xticklabels([])\n",
    "#        for h in set(df_disco_pct['hostname']):\n",
    "#            if ('mlab1.' + site) in h:\n",
    "#                ds = df_disco_pct[ (df_disco_pct['hostname'] == h) & (df_disco_pct['bytes_50th'] > 1e5) ]\n",
    "#                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "#                axes[i][j].scatter(d, ds['bytes_50th'], s=1, label=h[6:11])\n",
    "#                \n",
    "#        axes[i][j].set_ylim(1e4, 1e9)\n",
    "#        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "#        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "#        axes[i][j].grid(color='#dddddd')\n",
    "#        axes[i][j].legend(loc=3, ncol=7, fontsize='x-small', columnspacing=1)\n",
    "#        axes[i][j].semilogy()\n",
    "#\n",
    "#fig.suptitle('Daily Median Uplink Utilization')\n",
    "#fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packets Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_packets = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "    \n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "    \n",
    "  WHERE\n",
    "    metric LIKE 'switch.unicast.uplink.tx'\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    \n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(value) AS total\n",
    "  \n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "  \n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "  \n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "  \n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_disco_packets, 'ts', 'total', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    title='Daily Packets',\n",
    "    ylabel=\"Total Packets {0}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e7, 1e10),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':3, 'ncol':7, 'columnspacing':1},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites = [\n",
    "#    ['dfw'],\n",
    "#    ['lga'],\n",
    "#    ['nuq'],\n",
    "#]\n",
    "#\n",
    "#axes = [\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],\n",
    "#]\n",
    "#\n",
    "#fig = plt.figure(figsize=(6, 8))\n",
    "#\n",
    "#for i, site_row in enumerate(sites):\n",
    "#    for j, site in enumerate(site_row):\n",
    "#        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "#        axes[i][j].set_ylabel('Total Packets ' + site.upper())\n",
    "#        if i != len(sites)-1:\n",
    "#            axes[i][j].set_xticklabels([])\n",
    "#\n",
    "#        for h in set(df_disco_packets['hostname']):\n",
    "#            if ('mlab1.' + site) in h:\n",
    "#                ds = df_disco_packets[ (df_disco_packets['hostname'] == h) & (df_disco_packets['total'] > 100) ]\n",
    "#                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "#                axes[i][j].scatter(d, ds['total'], s=1, label=h[6:11])\n",
    "#\n",
    "#        axes[i][j].set_ylim(1e7, 1e10)\n",
    "#        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "#        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "#        axes[i][j].grid(color='#dddddd')\n",
    "#        axes[i][j].legend(loc=3, ncol=7, fontsize='x-small')\n",
    "#        axes[i][j].semilogy()\n",
    "#        \n",
    "#fig.suptitle('Daily Packets')\n",
    "#fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DISCARDS OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "    \n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "    \n",
    "  WHERE\n",
    "    metric LIKE 'switch.discards.uplink.tx'\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    \n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(value) AS total_discards\n",
    "  \n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "  \n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "  \n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "  \n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Total Discards ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "        #else:\n",
    "        #    axes[i][j].set_xlabel('Date')\n",
    "            \n",
    "        for h in set(df_disco['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco[ (df_disco['hostname'] == h) & (df_disco['total_discards'] > 100) ]\n",
    "                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "                axes[i][j].scatter(d, ds['total_discards'], s=1, label=h[6:11])\n",
    "\n",
    "        axes[i][j].set_ylim(100, 1000000)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.suptitle('Daily Packet Discards')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily DISCO discard ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_ratio = run_query(\"\"\"\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    (metric LIKE 'switch.discards.uplink.tx' OR metric LIKE 'switch.unicast.uplink.tx')\n",
    "    AND (hostname LIKE '%mlab1.lga%' OR hostname LIKE '%mlab1.dfw%' OR hostname LIKE '%mlab1.nuq%')\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  hostname,\n",
    "  day,\n",
    "  ts,\n",
    "  IF(total > 0, discards / total, 0) as ratio\n",
    "FROM (\n",
    "SELECT\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(IF(metric = \"switch.discards.uplink.tx\", value, 0)) AS discards,\n",
    "  SUM(IF(metric = \"switch.unicast.uplink.tx\", value, 0)) AS total\n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "HAVING\n",
    "  discards < total\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    ")\n",
    "GROUP BY\n",
    "  hostname, day, ts, ratio\n",
    "HAVING\n",
    "  ratio < 0.01\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_disco_ratio, 'ts', 'ratio', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    title='Daily Packet Loss Ratio',\n",
    "    ylabel=\"Discard Ratio {0}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e-6, 1e-1),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites = [\n",
    "#    ['dfw'],\n",
    "#    ['lga'],\n",
    "#    ['nuq'],\n",
    "#]\n",
    "#\n",
    "#axes = [\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],\n",
    "#]\n",
    "#\n",
    "#fig = plt.figure(figsize=(6, 8))\n",
    "#\n",
    "#for i, site_row in enumerate(sites):\n",
    "#    for j, site in enumerate(site_row):\n",
    "#        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "#        axes[i][j].set_ylabel('Discard Ratio ' + site.upper())\n",
    "#        if i != len(sites)-1:\n",
    "#            axes[i][j].set_xticklabels([])\n",
    "#            \n",
    "#        for h in set(df_disco_ratio['hostname']):\n",
    "#            if ('mlab1.' + site) in h:\n",
    "#                ds = df_disco_ratio[ (df_disco_ratio['hostname'] == h) ]\n",
    "#                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "#                axes[i][j].scatter(d, ds['ratio'], s=1, label=h[6:11])\n",
    "#\n",
    "#        axes[i][j].set_ylim(1e-6, 1e-2)\n",
    "#        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "#        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "#        axes[i][j].grid(color='#dddddd')\n",
    "#        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "#        axes[i][j].semilogy()\n",
    "#\n",
    "#fig.suptitle('Daily Packet Loss Ratios')\n",
    "#fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDT Median Download Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_median = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "\n",
    "  FROM\n",
    "  \n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    \n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps\n",
    "  -- AVG(download_mbps) as download_mbps\n",
    "FROM\n",
    "  mlab_ndt\n",
    "\n",
    "GROUP BY\n",
    "  day,\n",
    "  hostname\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8032\n"
     ]
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "print len(df_ndt_median)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Mbps ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "            \n",
    "        for h in set(df_ndt_median['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_ndt_median[ (df_ndt_median['hostname'] == h) ]\n",
    "                d = [pd.to_datetime(t) for t in ds['day']]\n",
    "                axes[i][j].scatter(d, ds['download_mbps'], s=1, label=h[6:11])\n",
    "\n",
    "        #axes[i][j].set_ylim(100, 1000000)\n",
    "        axes[i][j].set_ylim(1, 100)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.suptitle('Median NDT Download Rates')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDT Segs Retrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT ENOUGH HISTORICAL NDT DATA TO GET FULL TIMELINE.\n",
    "\n",
    "df_ndt_retrans = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(log_time, DAY) AS day,\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  SAFE_DIVIDE(SUM(web100_log_entry.snap.SegsRetrans), SUM(web100_log_entry.snap.SegsOut)) AS median_ratio\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq|ord|atl|lax)\\d\\d\")\n",
    "  AND connection_spec.data_direction = 1\n",
    "  AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "GROUP BY\n",
    "  day,\n",
    "  hostname\n",
    "HAVING\n",
    "  median_ratio is not NULL\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_retrans = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as hostname,\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    log_time,\n",
    "    web100_log_entry.snap.SegsRetrans as SegsRetrans,\n",
    "    web100_log_entry.snap.SegsOut as SegsOut\n",
    "\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "\n",
    "  WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"(lga|dfw|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "  \n",
    "  GROUP BY\n",
    "    connection_spec.server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    SegsRetrans,\n",
    "    SegsOut\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r\"([a-z]{3})[0-9]{2}\") as metro,\n",
    "  REGEXP_EXTRACT(hostname, r\"([a-z]{3}[0-9]{2})\") as site,\n",
    "  day,\n",
    "  APPROX_QUANTILES(ratio, 101)[ORDINAL(50)] AS median_ratio,\n",
    "  count(*) as count\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    hostname,\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "    MAX(SAFE_DIVIDE(SegsRetrans, SegsOut)) as ratio\n",
    "\n",
    "  FROM\n",
    "    mlab_ndt\n",
    "\n",
    "  GROUP BY\n",
    "    hostname,\n",
    "    day,\n",
    "    remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, day\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites = [\n",
    "#    ['dfw'],\n",
    "#    ['lga'],\n",
    "#    ['nuq'],\n",
    "#]\n",
    "#\n",
    "#axes = [\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],#\n",
    "#\n",
    "#]\n",
    "#\n",
    "#print len(df_ndt_retrans)\n",
    "#\n",
    "#fig = plt.figure(figsize=(6, 8))\n",
    "#\n",
    "#for i, site_row in enumerate(sites):\n",
    "#    for j, site in enumerate(site_row):\n",
    "#        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "#        axes[i][j].set_ylabel('Retransmission ' + site.upper())\n",
    "#        if i != len(sites)-1:\n",
    "#            axes[i][j].set_xticklabels([])#\n",
    "#\n",
    "#        for s in sorted(set(df_ndt_retrans['site'])):\n",
    "#            if site in s:\n",
    "#                ds = df_ndt_retrans[ (df_ndt_retrans['site'] == s) ]\n",
    "#                d = [pd.to_datetime(t) for t in ds['day']]\n",
    "#                axes[i][j].scatter(d, ds['median_ratio'], s=1, label=s)\n",
    "#\n",
    "#        #axes[i][j].set_ylim(100, 1000000)\n",
    "#        #axes[i][j].set_ylim(1e-6, 1e-2)\n",
    "#        axes[i][j].set_ylim(0, 0.005)\n",
    "#        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "#        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "#        axes[i][j].grid(color='#dddddd')\n",
    "#        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "#        #axes[i][j].semilogy()\n",
    "#        \n",
    "#fig.suptitle('Median NDT SegsRetran/SegsOut Ratio')\n",
    "#fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_ndt_retrans, 'day', 'median_ratio', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    title='Median NDT SegsRetran/SegsOut Ratio Rates',\n",
    "    ylabel=\"Retransmission {0}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e-6, 1e-1),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box(x, y, text):\n",
    "    plt.text(x, y, text,\n",
    "        bbox=dict(boxstyle=\"round\",\n",
    "              ec=(.5, 0.5, 1., 0.25),\n",
    "              fc=(.5, 0.8, 1., 0.25),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23105\n"
     ]
    }
   ],
   "source": [
    "## COMBINED SegsRetrans & Switch Discards\n",
    "\n",
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "print len(df_ndt_retrans)\n",
    "\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Ratio ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "\n",
    "        c = 0\n",
    "        for s in sorted(set(df_ndt_retrans['site'])):\n",
    "            if site in s:\n",
    "                ds = df_ndt_retrans[ (df_ndt_retrans['site'] == s) ]\n",
    "                d = [pd.to_datetime(t) for t in ds['day']]\n",
    "                axes[i][j].scatter(d, ds['median_ratio'], s=1, label=s, c=colors[c])\n",
    "                c += 1\n",
    "\n",
    "        #axes[i][j].set_ylim(100, 1000000)\n",
    "        axes[i][j].set_ylim(1e-6, 1e-1)\n",
    "        #axes[i][j].set_ylim(0, 0.005)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):       \n",
    "\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "        c = 0\n",
    "        for h in set(df_disco_ratio['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco_ratio[ (df_disco_ratio['hostname'] == h) ]\n",
    "                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "                axes[i][j].scatter(d, ds['ratio'], s=1, label=h[6:11], c=colors[c])\n",
    "                c += 1\n",
    "\n",
    "box(pd.to_datetime(\"2016-10-30\"), 5e-3, u\"Segs Retransmit ↘\")\n",
    "box(pd.to_datetime(\"2016-10-30\"), 9e-6, u\"Switch Discards ↗\")\n",
    "                \n",
    "fig.suptitle('Retrans & Switch Discard Rates')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One Week Performance Distributions -- Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_dist = run_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    CASE\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "        ELSE 'what'\n",
    "    END as period,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND ( -- One week - Sunday to Saturday.\n",
    "          TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-25\")\n",
    "       OR TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-25\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'bps', u'day', u'metric', u'name', u'period', u'sts'], dtype='object')\n",
      "517644\n"
     ]
    }
   ],
   "source": [
    "print df_disco_dist.keys()\n",
    "print len(df_disco_dist)\n",
    "\n",
    "\n",
    "\n",
    "for h in set(df_disco_dist['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist['period'])):\n",
    "        ds = df_disco_dist[ (df_disco_dist['name'] == h) & (df_disco_dist['period'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One Day Performance Distributions -- before & after\n",
    "\n",
    "Harder to notice changes wrt before  after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_dist4 = run_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND (\n",
    "        TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-18\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-25\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-11\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-05\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-10\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'bps', u'day', u'metric', u'name', u'sts'], dtype='object')\n",
      "69120\n"
     ]
    }
   ],
   "source": [
    "print df_disco_dist4.keys()\n",
    "print len(df_disco_dist4)\n",
    "\n",
    "\n",
    "\n",
    "for h in set(df_disco_dist4['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist4['day'])):\n",
    "        ds = df_disco_dist4[ (df_disco_dist4['name'] == h) & (df_disco_dist4['day'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NDT test distributions - Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_ndt_dist = run_query(\"\"\"\n",
    "-- ALL NDT tests before and after the flow-control configuration change.\n",
    "   \n",
    "SELECT\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    CASE\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")\n",
    "       THEN 'before-2w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")\n",
    "       THEN 'before-1w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")\n",
    "       THEN 'after-1w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\")\n",
    "       THEN 'after-2w'\n",
    "     ELSE 'what'\n",
    "    END as period,\n",
    "\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,\n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /\n",
    "      (web100_log_entry.snap.SndLimTimeRwin +\n",
    "       web100_log_entry.snap.SndLimTimeCwnd +\n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as download_mbps   \n",
    "FROM\n",
    "   `measurement-lab.base_tables.ndt*`\n",
    "   \n",
    "WHERE       \n",
    "    ( -- One week - Sunday to Saturday.\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "    \n",
    "GROUP BY\n",
    "  name, period, ts, download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_ndt_dist = run_query(\"\"\"\n",
    "  -- Weekly cohort - NDT tests before and after the flow-control configuration change.\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-18\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-1w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-11\") THEN 'after-1w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)) AS download_mbps\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  ( -- One week - Sunday to Saturday.\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-18\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-11\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN( (\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\")\n",
    "        AND TIMESTAMP(\"2018-02-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\")\n",
    "        AND TIMESTAMP(\"2018-02-25\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\")\n",
    "        AND TIMESTAMP(\"2018-03-11\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\")\n",
    "        AND TIMESTAMP(\"2018-03-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)) )\n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_ndt_dist = run_query(\"\"\"\n",
    "  -- Two-week cohorts - NDT tests before and after the flow-control configuration change.\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  --web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "  (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  --MAX(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  --APPROX_QUANTILES(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)), 101)[ORDINAL(50)] as download_mbps\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  ( -- One week - Sunday to Saturday.\n",
    "       TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  --remote_ip\n",
    "  download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "  (    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps)\n",
    "    \n",
    "SELECT\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  --web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "  --(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  MAX(download_mbps) AS download_mbps\n",
    "  --APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps\n",
    "\n",
    "FROM\n",
    "  mlab_ndt\n",
    "WHERE\n",
    "  remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 10\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  remote_ip\n",
    "  --download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'download_mbps', u'name', u'period'], dtype='object')\n",
      "44464\n",
      "mlab1.dfw01 1465\n",
      "38 mlab1.dfw01 before-2w\n",
      "mlab1.dfw01 1447\n",
      "38 mlab1.dfw01 after-2w\n",
      "mlab1.dfw02 1433\n",
      "37 mlab1.dfw02 before-2w\n",
      "mlab1.dfw02 1438\n",
      "37 mlab1.dfw02 after-2w\n",
      "mlab1.dfw03 1431\n",
      "37 mlab1.dfw03 before-2w\n",
      "mlab1.dfw03 1434\n",
      "37 mlab1.dfw03 after-2w\n",
      "mlab1.dfw04 1428\n",
      "37 mlab1.dfw04 before-2w\n",
      "mlab1.dfw04 1440\n",
      "37 mlab1.dfw04 after-2w\n",
      "mlab1.dfw05 1836\n",
      "42 mlab1.dfw05 before-2w\n",
      "mlab1.dfw05 1855\n",
      "42 mlab1.dfw05 after-2w\n",
      "mlab1.dfw06 1434\n",
      "37 mlab1.dfw06 before-2w\n",
      "mlab1.dfw06 1439\n",
      "37 mlab1.dfw06 after-2w\n",
      "mlab1.lga02 525\n",
      "22 mlab1.lga02 before-2w\n",
      "mlab1.lga02 2153\n",
      "diff mlab1.lga02 Ks_2sampResult(statistic=0.1738075332315927, pvalue=1.231555712986171e-11)\n",
      "22 mlab1.lga02 after-2w\n",
      "mlab1.lga03 1699\n",
      "41 mlab1.lga03 before-2w\n",
      "mlab1.lga03 1675\n",
      "41 mlab1.lga03 after-2w\n",
      "mlab1.lga04 1689\n",
      "41 mlab1.lga04 before-2w\n",
      "mlab1.lga04 1689\n",
      "41 mlab1.lga04 after-2w\n",
      "mlab1.lga05 2048\n",
      "45 mlab1.lga05 before-2w\n",
      "mlab1.lga05 2028\n",
      "45 mlab1.lga05 after-2w\n",
      "mlab1.lga06 1698\n",
      "41 mlab1.lga06 before-2w\n",
      "mlab1.lga06 1691\n",
      "41 mlab1.lga06 after-2w\n",
      "mlab1.lga07 1621\n",
      "40 mlab1.lga07 before-2w\n",
      "mlab1.lga07 1616\n",
      "40 mlab1.lga07 after-2w\n",
      "mlab1.nuq02 637\n",
      "25 mlab1.nuq02 before-2w\n",
      "mlab1.nuq02 642\n",
      "25 mlab1.nuq02 after-2w\n",
      "mlab1.nuq03 598\n",
      "24 mlab1.nuq03 before-2w\n",
      "mlab1.nuq03 639\n",
      "same mlab1.nuq03 Ks_2sampResult(statistic=0.1618730407523511, pvalue=0.0003897447905872814)\n",
      "=================================\n",
      "24 mlab1.nuq03 after-2w\n",
      "mlab1.nuq04 587\n",
      "24 mlab1.nuq04 before-2w\n",
      "mlab1.nuq04 588\n",
      "24 mlab1.nuq04 after-2w\n",
      "mlab1.nuq05 643\n",
      "25 mlab1.nuq05 before-2w\n",
      "mlab1.nuq05 639\n",
      "25 mlab1.nuq05 after-2w\n",
      "mlab1.nuq06 638\n",
      "25 mlab1.nuq06 before-2w\n",
      "mlab1.nuq06 641\n",
      "25 mlab1.nuq06 after-2w\n"
     ]
    }
   ],
   "source": [
    "print df_ndt_dist.keys()\n",
    "print len(df_ndt_dist)\n",
    "\n",
    "def hist(vals, bin_count, log=True, cdf=False):\n",
    "    \"\"\"Produces hist or cdf values for smooth plots.\"\"\"\n",
    "    if log:\n",
    "        r = [math.log10(x) for x in vals]\n",
    "    else:\n",
    "        r = vals\n",
    "        \n",
    "    m, bins = np.histogram(r, bin_count, normed=True)\n",
    "    m = m.astype(float)\n",
    "\n",
    "    tops = m\n",
    "    if cdf:\n",
    "        tops = np.cumsum(m)\n",
    "        total = sum(m)\n",
    "        tops = [float(t) / total for t in tops ]\n",
    "    \n",
    "    return tops, bins\n",
    "\n",
    "seq = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
    "\n",
    "for site in set([v[6:9] for v in set(df_ndt_dist['name'])]):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "    for p, h in enumerate(sorted([h for h in set(df_ndt_dist['name']) if site in h])):\n",
    "        before = None\n",
    "        r_before = None\n",
    "\n",
    "        for day in ['before-2w', 'after-2w']:\n",
    "            ds = df_ndt_dist[ (df_ndt_dist['name'] == h) & (df_ndt_dist['period'] == day) ]\n",
    "            r = ds['download_mbps']\n",
    "            print h, len(r)\n",
    "            if not len(r):\n",
    "                continue\n",
    "\n",
    "            size = int(math.sqrt(len(r)))\n",
    "            \n",
    "            if day == 'after-2w':\n",
    "                size = before\n",
    "                # Test before vs after\n",
    "                result = stats.ks_2samp(r, r_before)\n",
    "                if result.pvalue < 0.01:\n",
    "                    print 'diff', h, result\n",
    "                \n",
    "                # Test itself.\n",
    "                a, b = train_test_split(r, test_size=0.5)\n",
    "                result = stats.ks_2samp(a, b)\n",
    "                if result.pvalue < 0.01:\n",
    "                    print 'same', h, result\n",
    "                    print '================================='\n",
    "\n",
    "            else:\n",
    "                before = size\n",
    "                r_before = r\n",
    "            \n",
    "                \n",
    "#            tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=True , cdf=True)\n",
    "            #tops, bins = hist(r, int(math.sqrt(len(r))), log=True , cdf=True)\n",
    "            print size, h, day\n",
    "            #tops, bins = hist(r, size, log=True , cdf=True)\n",
    "            tops, bins = hist(r, size, log=True , cdf=True)\n",
    "#            tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=False , cdf=True)           \n",
    "#            tops, bins = hist(r, len(r), log=False , cdf=True)            \n",
    "            \n",
    "\n",
    "            #tops_a, bins_a = hist(a, int(1 * math.sqrt(len(a))), log=True, cdf=True)\n",
    "            #tops_b, bins_b = hist(b, int(1 * math.sqrt(len(b))), log=True, cdf=True)\n",
    "            if p > len(seq)-1:\n",
    "                print 'skipping', h\n",
    "                continue\n",
    "            i, j = seq[p]\n",
    "#            print h, len(bins), len(tops)\n",
    "            axes[i, j].plot(bins[:-1], tops, label='cdf-'+h[6:11] + '-' + str(day))\n",
    "            #axes[i, j].plot(bins_a[:-1], tops_a, label=h[6:11] + '-' + str(day)+'-a')\n",
    "            #axes[i, j].plot(bins_b[:-1], tops_b, label=h[6:11] + '-' + str(day)+'-b')\n",
    "            axes[i, j].set_title(h[6:11])\n",
    "            #axes[i, j].set_xlim(-10, 1000)\n",
    "#            axes[i, j].set_xlim(math.log10(.25), math.log10(1000))\n",
    "            axes[i, j].set_xlim(math.log10(.1), math.log10(1000))\n",
    "            axes[i, j].grid(color='#dddddd')\n",
    "            axes[i, j].legend(loc=2, fontsize='x-small')\n",
    "            #axes[i, j].set_ylim(-0.1, 1.1)\n",
    "            axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('NDT Download Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(n), len(bins[:-1])\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "\n",
    "#axes.plot(bins[:-1], n)\n",
    "#fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "#print f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m, bins = np.histogram(r, int(math.sqrt(len(ds['download_mbps']))))\n",
    "#print m, bins, len(m), len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_variance = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "  (    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps)\n",
    "\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  remote_ip,\n",
    "  STDDEV(download_mbps) AS download_stddev\n",
    "\n",
    "FROM\n",
    "  mlab_ndt\n",
    "WHERE\n",
    "  remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 5\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 5\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  hostname,\n",
    "  period,\n",
    "  remote_ip\n",
    "  --download_mbps\n",
    "\n",
    "HAVING download_stddev is not NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63423\n"
     ]
    }
   ],
   "source": [
    "print len(df_ndt_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlab1.dfw01 1879\n",
      "mlab1.dfw01 1906\n",
      "mlab1.dfw02 2034\n",
      "mlab1.dfw02 2008\n",
      "mlab1.dfw03 1997\n",
      "mlab1.dfw03 2056\n",
      "mlab1.dfw04 1989\n",
      "mlab1.dfw04 2037\n",
      "mlab1.dfw05 2491\n",
      "mlab1.dfw05 2518\n",
      "mlab1.dfw06 2008\n",
      "mlab1.dfw06 2065\n",
      "mlab1.lga02 309\n",
      "mlab1.lga02 2980\n",
      "mlab1.lga03 2586\n",
      "mlab1.lga03 2466\n",
      "mlab1.lga04 2585\n",
      "mlab1.lga04 2464\n",
      "mlab1.lga05 3060\n",
      "mlab1.lga05 2920\n",
      "mlab1.lga06 2620\n",
      "mlab1.lga06 2443\n",
      "mlab1.lga07 2376\n",
      "mlab1.lga07 2233\n",
      "mlab1.nuq02 981\n",
      "mlab1.nuq02 987\n",
      "mlab1.nuq03 782\n",
      "mlab1.nuq03 992\n",
      "mlab1.nuq04 843\n",
      "mlab1.nuq04 837\n",
      "mlab1.nuq05 990\n",
      "mlab1.nuq05 996\n",
      "mlab1.nuq06 1014\n",
      "mlab1.nuq06 971\n"
     ]
    }
   ],
   "source": [
    "seq = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
    "\n",
    "skip = 'mlab1.lga02'\n",
    "for site in set([v[6:9] for v in set(df_ndt_variance['hostname'])]):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "    for p, h in enumerate(sorted([h for h in set(df_ndt_variance['hostname']) if site in h])):\n",
    "        before = None\n",
    "        for day in ['before-2w', 'after-2w']:\n",
    "            ds = df_ndt_variance[ (df_ndt_variance['hostname'] == h) & (df_ndt_variance['period'] == day) ]\n",
    "            r = ds['download_stddev']\n",
    "            print h, len(r)\n",
    "            if not len(r) or h == skip:\n",
    "                continue\n",
    "\n",
    "            size = int(math.sqrt(len(r)))\n",
    "            if day == 'after-2w':\n",
    "                size = before\n",
    "            else:\n",
    "                before = size\n",
    "\n",
    "            tops, bins = hist(r, size, log=True, cdf=False)\n",
    "            if p > len(seq)-1:\n",
    "                print 'skipping', h\n",
    "                continue\n",
    "            i, j = seq[p]\n",
    "\n",
    "            axes[i, j].plot(bins[:-1], tops, label='cdf-'+h[6:11] + '-' + str(day))\n",
    "    \n",
    "            axes[i, j].set_title(h[6:11])\n",
    "            #axes[i, j].set_xlim(-10, 1000)\n",
    "            #axes[i, j].set_xlim(math.log10(.25), math.log10(1000))\n",
    "            axes[i, j].set_xlim(math.log10(.01), math.log10(1000))\n",
    "            axes[i, j].grid(color='#dddddd')\n",
    "            axes[i, j].legend(loc=2, fontsize='x-small')\n",
    "            #axes[i, j].set_ylim(-0.1, 1.1)\n",
    "            axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Distribution of Stddev of NDT Downloads per remote_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'download_stddev', u'hostname', u'period', u'remote_ip'], dtype='object')\n",
      "[RangeIndex(start=0, stop=63423, step=1), Index([u'download_stddev', u'hostname', u'period', u'remote_ip'], dtype='object')]\n"
     ]
    }
   ],
   "source": [
    "print df_ndt_variance.keys()\n",
    "print df_ndt_variance.axes\n",
    "#print df_ndt_variance.query('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_all = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r\"([a-z]{3})[0-9]{2}\") as metro,\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r\"([a-z]{3}[0-9]{2})\") as site,\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    log_time,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "\n",
    "  FROM\n",
    "  \n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"(lga|dfw|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    AND anomalies.no_meta is not true\n",
    "    \n",
    "  \n",
    "  GROUP BY\n",
    "    connection_spec.server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  metro,\n",
    "  site,\n",
    "  day,\n",
    " --  AVG(download_mbps) as download_mbps,\n",
    "  APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps,\n",
    "  count(*) as count\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    metro,\n",
    "    site,\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "    -- APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps\n",
    "    MAX(download_mbps) as download_mbps\n",
    "  FROM\n",
    "    mlab_ndt\n",
    "\n",
    "  GROUP BY\n",
    "    metro, site, day, remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  metro, site, day\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_ndt_all, 'day', 'download_mbps', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    title='Median NDT Download Rates',\n",
    "    ylabel=\"Mbps {0}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(0, 50),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l],\n",
    "    legend={'loc':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites = [\n",
    "#    ['dfw'],\n",
    "#    ['lga'],\n",
    "#    ['nuq'],\n",
    "#]\n",
    "#\n",
    "#axes = [\n",
    "#    [None],\n",
    "#    [None],\n",
    "#    [None],\n",
    "#]\n",
    "#print len(df_ndt_all)\n",
    "#\n",
    "#fig = plt.figure(figsize=(6, 8))\n",
    "#\n",
    "#for i, site_row in enumerate(sites):\n",
    "#    for j, site in enumerate(site_row):\n",
    "#        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "#        axes[i][j].set_ylabel('Mbps ' + site.upper())\n",
    "#        if i != len(sites)-1:\n",
    "#            axes[i][j].set_xticklabels([])\n",
    "#        \n",
    "#        for s in set(df_ndt_all['site']):\n",
    "#            if site in s:\n",
    "#                ds = df_ndt_all[ (df_ndt_all['site'] == s) ]\n",
    "#                d = [pd.to_datetime(t) for t in ds['day']]\n",
    "#                axes[i][j].scatter(d, ds['download_mbps'], s=1, label=s)\n",
    "#\n",
    "#        #ds = df_ndt_all[ df_ndt_all['site'].str.contains(site) ]\n",
    "#        #d = [pd.to_datetime(t) for t in ds['day']]\n",
    "#        #axes[i][j].scatter(d, ds['download_mbps'], s=1, label='all')\n",
    "#\n",
    "#        #axes[i][j].set_ylim(100, 1000000)\n",
    "#        axes[i][j].set_ylim(0, 50)\n",
    "#        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "#        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "#        axes[i][j].grid(color='#dddddd')\n",
    "#        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "#        #axes[i][j].semilogy()\n",
    "\n",
    "#axes[1][0] = plt.subplot2grid((2, 1), (1, 0))\n",
    "#axes[1][0].plot(df_ndt_all['day'], df_ndt_all['count'])\n",
    "        \n",
    "#fig.suptitle('Median NDT Download Rates')\n",
    "#fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
