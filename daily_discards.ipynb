{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# Enables figures loading outside of browser.\n",
    "# If not run, figures will load inline.\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')\n",
    "\n",
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "import query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unlog(x, pos):\n",
    "    v = math.pow(10, x)\n",
    "    frac, whole = math.modf(v)\n",
    "    if frac > 0:\n",
    "        return '%.1f' % v\n",
    "    else:\n",
    "        return '%d' % whole\n",
    "\n",
    "logFormatter = matplotlib.ticker.FuncFormatter(unlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = query.sync_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  name AS hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(IF(metric = 'switch.discards.uplink.tx', value, 0)) AS total_discards,\n",
    "  SUM(IF(metric = 'switch.unicast.uplink.tx', value, 0)) AS total_packets,\n",
    "  SUM(IF(metric = 'switch.octets.uplink.tx', value, 0)) AS total_bytes,\n",
    "  COUNTIF(metric = 'switch.discards.uplink.tx' AND value > 0) / 8640 AS pct_discards\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `mlab-sandbox.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.discards.uplink.tx'\n",
    "    OR metric LIKE 'switch.unicast.uplink.tx'\n",
    "    OR metric LIKE 'switch.octets.uplink.tx'\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  name IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")\n",
    "\n",
    "df_disco = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DISCO RATES 90th PERCENTILE\n",
    "\n",
    "result = query.sync_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  name AS hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  \n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(50)] as bytes_50th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(90)] as bytes_90th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(98)] as bytes_98th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(99)] as bytes_99th,\n",
    "  MAX(value) as bytes_max\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `mlab-sandbox.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.octets.uplink.tx'\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  name IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")\n",
    "\n",
    "df_disco_max = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discards over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,'Discards over time')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw', 'lga', 'iad'],\n",
    "    ['sea', 'atl', 'den'],\n",
    "    ['mia', 'nuq', 'ord'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        if j != 0:\n",
    "            axes[i, j].set_yticklabels([])\n",
    "        if i != len(sites)-1:\n",
    "            axes[i, j].set_xticklabels([])\n",
    "        for h in set(df_disco['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco[ (df_disco['hostname'] == h) & (df_disco['total_discards'] > 100)& (df_disco['total_discards'] < 1000000)]\n",
    "                axes[i, j].plot_date(dates.epoch2num(ds['ts']), ds['total_discards'], ls='-', ms=0, label=h[6:11])\n",
    "\n",
    "        axes[i, j].set_title(site)\n",
    "        axes[i, j].set_ylim(100, 1000000)\n",
    "        axes[i, j].tick_params(axis='x', labelrotation=90)\n",
    "        axes[i, j].grid(color='#dddddd')\n",
    "        axes[i, j].legend(loc=4, fontsize='x-small')\n",
    "        axes[i, j].semilogy()\n",
    "        \n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "fig.suptitle('Discards over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Daily Rate over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,'Daily Avg Rate over time')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw', 'lga', 'iad'],\n",
    "    ['sea', 'atl', 'den'],\n",
    "    ['mia', 'nuq', 'ord'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        if j != 0:\n",
    "            axes[i, j].set_yticklabels([])\n",
    "        if i != len(sites)-1:\n",
    "            axes[i, j].set_xticklabels([])\n",
    "        for h in set(df_disco['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco[ (df_disco['hostname'] == h) ] # & (df_disco['total_discards'] > 100)& (df_disco['total_discards'] < 1000000)]\n",
    "                axes[i, j].plot_date(dates.epoch2num(ds['ts']), ds['total_bytes'] / 1000000 / 86400, ls='-', ms=0, label=h[6:11])\n",
    "\n",
    "        axes[i, j].set_title(site)\n",
    "        axes[i, j].set_ylim(1, 1000)\n",
    "        axes[i, j].tick_params(axis='x', labelrotation=90)\n",
    "        axes[i, j].grid(color='#dddddd')\n",
    "        axes[i, j].legend(loc=2, fontsize='x-small', ncol=3)\n",
    "        axes[i, j].semilogy()\n",
    "        \n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "fig.suptitle('Daily Avg Rate over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90th Percentile Over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sites = [\n",
    "    ['dfw', 'lga', 'iad'],\n",
    "    ['lax', 'atl', 'den'],\n",
    "    ['sea', 'nuq', 'ord'], # MIA is low utilization.\n",
    "]\n",
    "\n",
    "sites = [\n",
    "    ['dfw', 'lga', 'iad'],\n",
    "    ['lax', 'atl',  'nuq'], #  'ord', # MIA is low utilization. 'den', 'sea' low enough.\n",
    "]\n",
    "\n",
    "cols = len(sites[0])\n",
    "fig = plt.figure(figsize=(4 * cols, 6))\n",
    "axes = [\n",
    "    [None] * cols,\n",
    "    [None] * cols,\n",
    "    #[None] * cols,\n",
    "]\n",
    "\n",
    "for r, siter in enumerate(sites):\n",
    "    for c, site in enumerate(siter):\n",
    "        for x, rate in enumerate(['98th']):\n",
    "            axes[r][c] = plt.subplot2grid((2, cols), (r, c))\n",
    "            if c != 0:\n",
    "                axes[r][c].set_yticklabels([])\n",
    "            else:\n",
    "                axes[r][c].set_ylabel('Mbps')\n",
    "\n",
    "            if r != 1:\n",
    "                axes[r][c].set_xticklabels([])\n",
    "\n",
    "            prefix = 'mlab1.' + site\n",
    "            ds_sites = df_disco_max[ df_disco_max['hostname'].str.contains(prefix) ]\n",
    "            for h in sorted(set(ds_sites[ ds_sites['hostname'].str.contains(prefix) ]['hostname'])):\n",
    "                ds = ds_sites[ (ds_sites['hostname'].str.contains(h)) ]\n",
    "                axes[r][c].plot_date(dates.epoch2num(ds['ts']), ds['bytes_' + rate] * 8 / 10000000, ls='-', ms=0, label=h[6:11] + '-' +  rate)\n",
    "\n",
    "            axes[r][c].set_title(site)\n",
    "            axes[r][c].set_ylim(100, 1000)\n",
    "            axes[r][c].tick_params(axis='x', labelrotation=90)\n",
    "            axes[r][c].grid(color='#dddddd')\n",
    "            axes[r][c].legend(loc=2, fontsize='x-small', ncol=2)\n",
    "\n",
    "fig.suptitle('Daily Percentile Rates')\n",
    "#fig.tight_layout()\n",
    "#fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SS COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [ 'lga', nuq'], #  'ord', # MIA is low utilization. 'den', 'sea' low enough.\n",
    "\n",
    "sites = [\n",
    "    ['dfw', 'iad', 'lax', 'atl'],\n",
    "]\n",
    "\n",
    "cols = len(sites[0])\n",
    "fig = plt.figure(figsize=(4 * cols, 6))\n",
    "axes = [\n",
    "    [None] * cols,\n",
    "    [None] * cols,\n",
    "]\n",
    "\n",
    "for r, siter in enumerate(sites):\n",
    "    for c, site in enumerate(siter):\n",
    "        for x, rate in enumerate(['98th']):\n",
    "            r = 1\n",
    "            axes[r][c] = plt.subplot2grid((2, cols), (r, c))\n",
    "            if c != 0:\n",
    "                #axes[r][c].set_yticklabels([])\n",
    "                pass\n",
    "            else:\n",
    "                axes[r][c].set_ylabel('Connection Counts')\n",
    "\n",
    "            if r != 1:\n",
    "                axes[r][c].set_xticklabels([])\n",
    "\n",
    "            prefix = 'mlab1.' + site\n",
    "            ds_sites = df_ss_count[ df_ss_count['hostname'].str.contains(prefix) ]\n",
    "            for h in sorted(set(ds_sites[ ds_sites['hostname'].str.contains(prefix) ]['hostname'])):\n",
    "                ds = ds_sites[ (ds_sites['hostname'].str.contains(h)) ]\n",
    "                axes[r][c].plot_date(dates.epoch2num(ds['ts']), ds['count'], ls='-', ms=0, label=h[6:11])\n",
    "\n",
    "            axes[r][c].set_title(site)\n",
    "            axes[r][c].set_ylim(0, 25000)\n",
    "            axes[r][c].tick_params(axis='x', labelrotation=90)\n",
    "            axes[r][c].grid(color='#dddddd')\n",
    "            axes[r][c].legend(loc=2, fontsize='x-small', ncol=2)\n",
    "            \n",
    "    for c, site in enumerate(siter):\n",
    "        for r in [0]:\n",
    "            axes[r][c] = plt.subplot2grid((2, cols), (r, c))\n",
    "            if c != 0:\n",
    "                axes[r][c].set_yticklabels([])\n",
    "            else:\n",
    "                axes[r][c].set_ylabel('Mbps')\n",
    "\n",
    "            if r != 1:\n",
    "                axes[r][c].set_xticklabels([])\n",
    "\n",
    "            prefix = 'mlab1.' + site\n",
    "            ds_sites = df_disco_max[ df_disco_max['hostname'].str.contains(prefix) ]\n",
    "            for h in sorted(set(ds_sites[ ds_sites['hostname'].str.contains(prefix) ]['hostname'])):\n",
    "                ds = ds_sites[ (ds_sites['hostname'].str.contains(h)) ]\n",
    "                axes[r][c].plot_date(dates.epoch2num(ds['ts']), ds['bytes_' + rate] * 8 / 10000000, ls='-', ms=0, label=h[6:11] + '-' +  rate)\n",
    "\n",
    "            axes[r][c].set_title(site)\n",
    "            axes[r][c].set_ylim(100, 1000)\n",
    "            axes[r][c].tick_params(axis='x', labelrotation=90)\n",
    "            axes[r][c].grid(color='#dddddd')\n",
    "            axes[r][c].legend(loc=2, fontsize='x-small', ncol=2)\n",
    "\n",
    "fig.suptitle('Daily 98th Percentile Switch Traffic & TCP Connection Counts Per Metro')\n",
    "#fig.tight_layout()\n",
    "#fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'mlab1.dfw06', u'mlab1.dfw05', u'mlab1.dfw04', u'mlab1.dfw03', u'mlab1.dfw02', u'mlab1.dfw01'])\n"
     ]
    }
   ],
   "source": [
    "print set(df_disco_max[df_disco_max['hostname'].str.contains('mlab1.dfw')]['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent of Timebins with Discards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,u'Daily percentage of timebins with any discards')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = 'Daily percentage of timebins with any discards'\n",
    "sites = [\n",
    "    ['mlab1.dfw02', 'mlab1.dfw03', 'mlab1.dfw04'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(sites[0]))\n",
    "for i, hosts in enumerate(sites):\n",
    "    for j, host in enumerate(hosts): \n",
    "        ax = axes[j]\n",
    "        \n",
    "        ds = df_disco[ df_disco['hostname'] == host ]\n",
    "        ax.plot_date(dates.epoch2num(ds['ts']), ds['pct_discards'], ls='-', ms=0, label=host)\n",
    "        \n",
    "        ax.set_title(host)\n",
    "        ax.set_ylim(-0.01, .4)\n",
    "        ax.tick_params(axis='x', labelrotation=90)\n",
    "        ax.grid(color='#dddddd')\n",
    "        ax.legend(loc=4, fontsize='x-small')\n",
    "        \n",
    "        \n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,u'Daily percentage of timebins with any discards')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = 'Daily percentage of timebins with any discards'\n",
    "sites = [\n",
    "    ['mlab1.dfw02', 'mlab1.dfw03', 'mlab1.dfw04'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(sites[0]))\n",
    "for i, hosts in enumerate(sites):\n",
    "    for j, host in enumerate(hosts): \n",
    "        ax = axes[j]\n",
    "        \n",
    "        ds = df_disco[ df_disco['hostname'] == host ]\n",
    "        ax.plot_date(dates.epoch2num(ds['ts']), ds['total_packets'], ls='-', ms=0, label=host)\n",
    "        \n",
    "        ax.set_title(host)\n",
    "        #ax.set_ylim(-0.01, .4)\n",
    "        ax.tick_params(axis='x', labelrotation=90)\n",
    "        ax.grid(color='#dddddd')\n",
    "        ax.legend(loc=4, fontsize='x-small')\n",
    "        \n",
    "        \n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Packet Discard Ratios (Switch Loss Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,u'Switch Packet Loss Rate')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw', 'lga', 'iad'],\n",
    "    ['sea', 'atl', 'den'],\n",
    "    ['mia', 'nuq', 'ord'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i, j].set_title(site)\n",
    "        if j != 0:\n",
    "            axes[i, j].set_yticklabels([])\n",
    "        if i != len(sites)-1:\n",
    "            axes[i, j].set_xticklabels([])\n",
    "        if j == 0:\n",
    "            axes[i, j].set_ylabel('Daily Loss Ratio')\n",
    "\n",
    "        for h in set(df_disco['hostname']):\n",
    "            if 'mlab1.' + site in h:\n",
    "                ds = df_disco[ (df_disco['hostname'] == h) &\n",
    "                               (df_disco['total_discards'] > 100) &\n",
    "                               (df_disco['total_discards'] < 1000000) ]\n",
    "                ratio = ds['total_discards'] / ds['total_packets']\n",
    "                axes[i, j].plot_date(dates.epoch2num(ds['ts']), ratio, ls='-', ms=0, label=h[:11])\n",
    "        axes[i, j].set_ylim(10**-6, 10**-3)\n",
    "        axes[i, j].tick_params(axis='x', labelrotation=90)\n",
    "        axes[i, j].grid(color='#dddddd')\n",
    "        axes[i, j].legend(loc=4, fontsize='x-small')\n",
    "        axes[i, j].semilogy()\n",
    "        \n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "fig.suptitle('Switch Packet Loss Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Flow-Control Trial (measurement-lab.public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cat sidestream.sql | bq query --format=csv --max_rows=1000000 --nouse_legacy_sql > sidestream-trial-6w.csv\n",
    "#df = pd.read_csv('sidestream-trial-6w.csv')\n",
    "\n",
    "result = query.sync_query(\"\"\"\n",
    "#standardSQL                                                                    \n",
    "    -- Only works for mlab1 addresses. May not work on all machines.\n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "    CASE \n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 1 THEN 'ndt'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'samknows'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 9 THEN 'neubot'\n",
    "        ELSE 'other' \n",
    "    END AS slice,\n",
    "    CASE\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-01-26 00:00:00\", \"2018-01-27 00:00:00\") THEN '5w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-02 00:00:00\", \"2018-02-03 00:00:00\") THEN '4w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-09 00:00:00\", \"2018-02-10 00:00:00\") THEN '3w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\") THEN '2w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\") THEN '1w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\") THEN '0w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-09 00:00:00\", \"2018-03-10 00:00:00\") THEN '+1w'\n",
    "    ELSE 'unknown'                                                                    \n",
    "    END AS period,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,                                   \n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /                                \n",
    "      (web100_log_entry.snap.SndLimTimeRwin +                                     \n",
    "       web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as rate_mbps   \n",
    "FROM\n",
    "   -- `measurement-lab.public.sidestream`\n",
    "   `mlab-sandbox.batch.sidestream*`\n",
    "WHERE\n",
    "        (  betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-01-26 00:00:00\", \"2018-01-27 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-02 00:00:00\", \"2018-02-03 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-09 00:00:00\", \"2018-02-10 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-09 00:00:00\", \"2018-03-10 00:00:00\")\n",
    "        )\n",
    "  AND REGEXP_CONTAINS(test_id, r\"mlab1.(dfw\\d\\d)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "  AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "    (web100_log_entry.snap.State >= 5 AND                                       \n",
    "    web100_log_entry.snap.State <= 11))\n",
    "    \n",
    "GROUP BY\n",
    "  hostname, slice, period, ts, rate_mbps\n",
    "\"\"\")\n",
    "df_ss_trial = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = query.sync_query(\"\"\"\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "CASE\n",
    "WHEN betweenTimes(StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\") THEN CONCAT(sitename, '-2w')\n",
    "WHEN betweenTimes(StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\") THEN CONCAT(sitename, '-1w')\n",
    "WHEN betweenTimes(StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\") THEN CONCAT(sitename, '-0w (flow)')\n",
    "ELSE 'unknown'\n",
    "END AS test_period,\n",
    "\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(10)], 2) as q10,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(12)], 2) as q12,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(15)], 2) as q15,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(18)], 2) as q18,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(20)], 2) as q20,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(22)], 2) as q22,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(25)], 2) as q25,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(28)], 2) as q28,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(30)], 2) as q30,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(32)], 2) as q32,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(35)], 2) as q35,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(38)], 2) as q38,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(40)], 2) as q40,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(42)], 2) as q42,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(45)], 2) as q45,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(48)], 2) as q48,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(50)], 2) as q50,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(52)], 2) as q52,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(55)], 2) as q55,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(58)], 2) as q58,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(60)], 2) as q60,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(62)], 2) as q62,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(65)], 2) as q65,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(68)], 2) as q68,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(70)], 2) as q70,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(72)], 2) as q72,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(75)], 2) as q75,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(78)], 2) as q78,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(80)], 2) as q80,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(82)], 2) as q82,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(85)], 2) as q85,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(88)], 2) as q88,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(90)], 2) as q90,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(92)], 2) as q92,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(95)], 2) as q95,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(98)], 2) as q98,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(100)], 2) as q100,\n",
    "COUNT(*) as sample_count\n",
    "\n",
    "FROM\n",
    "(\n",
    "SELECT\n",
    "    UNIX_SECONDS(TIMESTAMP_TRUNC(log_time, DAY)) as StartTimeStamp,\n",
    "    --  web100_log_entry.snap.StartTimeStamp as StartTimeStamp,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/[0-9]+/mlab1.(dfw02|lga03)/.*\") AS sitename,\n",
    "    8 * (\n",
    "        web100_log_entry.snap.HCThruOctetsAcked / (\n",
    "        web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)\n",
    "    ) AS rate_mbps\n",
    "FROM\n",
    "    `mlab-sandbox.batch.sidestream*`\n",
    "WHERE\n",
    "\n",
    "    REGEXP_CONTAINS(test_id, r\"\\d\\d\\d\\d/\\d\\d/[0-9]+/mlab1.(dfw02|lga03)/.*\")\n",
    "    AND (\n",
    "             betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\")\n",
    "          OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\")\n",
    "          OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\"))\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 819200\n",
    "    AND ( web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND ( web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND ( web100_log_entry.snap.State = 1 OR ( web100_log_entry.snap.State >= 5 AND web100_log_entry.snap.State <= 11))\n",
    ")\n",
    "GROUP BY\n",
    "    sitename, test_period\n",
    "ORDER BY\n",
    "    sitename, test_period\n",
    "\"\"\")\n",
    "df_ss_trial_pct = pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keys = list(df_ss_trial_pct.keys())\n",
    "#n = df_ss_trial_pct.drop(['q100', 'sample_count'], axis=1)\n",
    "#print keys\n",
    "##print n.transpose()\n",
    "#del keys[1] # q100 is out of order.\n",
    "\n",
    "#for row in n:\n",
    "#    print row\n",
    "#plt.plot(n['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730671 1303.9458692 854\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "print len(df_ss), max(df_ss['rate_mbps']), int(math.sqrt(len(df_ss['rate_mbps'])))\n",
    "\n",
    "hosts = [\n",
    "    ['mlab1.lga03', 'mlab1.lga04'],\n",
    "    ['mlab1.lga05', 'mlab1.lga06'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "for i, host_row in enumerate(hosts):\n",
    "    for j, host in enumerate(host_row):\n",
    "        for period in ['+1w', '0w', '1w', '2w', '3w']: #, '4w', '5w']:\n",
    "            ds = df_ss[ (df_ss['period'] == period) & (df_ss['hostname'] == host) & (df_ss['slice'] == 'ndt') ]\n",
    "            label = 'pdf-%s (%d)' % (period, len(ds['rate_mbps']))\n",
    "            if len(ds) == 0:\n",
    "                continue\n",
    "            r = [math.log10(x) for x in ds['rate_mbps']]\n",
    "            n, bins, patches = axes[i, j].hist(r, int(math.sqrt(len(ds['rate_mbps']))),\n",
    "                                       histtype='step', normed=1, label=label, ls='-')\n",
    "#            n, bins, patches = axes[i, j].hist(ds['rate_mbps'], int(math.sqrt(len(ds['rate_mbps']))),\n",
    "#                               histtype='step', label=label, ls='-')\n",
    "#            n, bins, patches = axes[i, j].hist(ds['rate_mbps'], len(ds['rate_mbps']),\n",
    "#                               histtype='step', normed=1, cumulative=True, label='cdf-' + period,\n",
    "#                               ls='-')\n",
    "\n",
    "        axes[i, j].set_xlim(math.log10(0.1), math.log10(1000))\n",
    "        axes[i, j].set_axisbelow(True)\n",
    "        axes[i, j].legend(loc=2)\n",
    "        axes[i, j].grid(color='#dddddd')\n",
    "        axes[i, j].set_title(host)\n",
    "        axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "\n",
    "fig.suptitle('Sidestream Download Rate PDFs over three week period (0w is trial)')\n",
    "plt.show()\n",
    "print len(bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Historical (mlab-sandbox.batch) - Sidestream by Period & Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variations, for each period:\n",
    "# * all sidestream connections from each period.\n",
    "# * all sidestream connections from each period and slice\n",
    "# * all sidestream connections from each period and slice and from same cohort.\n",
    "# * some sidestream connections from each period and slice and from same cohort, grouped by ts & remote_ip.\n",
    "# * some sidestream connections from each period and slice and from same cohort, grouped by only by remote_ip.\n",
    "\n",
    "result = query.sync_query(\n",
    "    \"\"\"#standardSQL                                                                    \n",
    "    -- Only works for mlab1 addresses. May not work on all machines.\n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "    CASE \n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 1 THEN 'ndt'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'samknows'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'neubot'\n",
    "        ELSE 'other' \n",
    "    END AS slice,\n",
    "    CASE                                                                          \n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-07-26 00:00:00\", \"2017-07-30 00:00:00\") THEN '07-26 to 29'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-08-12 00:00:00\", \"2017-08-16 00:00:00\") THEN '08-12 to 16'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-10-30 00:00:00\", \"2017-11-02 00:00:00\") THEN '10-30 to 11-02'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-11-26 00:00:00\", \"2017-12-01 00:00:00\") THEN '11-29 to 12-03'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-21 00:00:00\", \"2018-02-25 00:00:00\") THEN '02-21 to 25'\n",
    "    ELSE 'bad'                                                                    \n",
    "    END AS period,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,                                   \n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /                                \n",
    "      (web100_log_entry.snap.SndLimTimeRwin +                                     \n",
    "       web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as rate_mbps   \n",
    "FROM\n",
    "    `mlab-sandbox.batch.sidestream*`                                              \n",
    "WHERE\n",
    "        (  betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-07-26 00:00:00\", \"2017-07-30 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-08-12 00:00:00\", \"2017-08-16 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-11-26 00:00:00\", \"2017-12-01 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-21 00:00:00\", \"2018-02-25 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2017-10-30 00:00:00\", \"2017-11-02 00:00:00\") )\n",
    "  AND (test_id LIKE '%mlab1.dfw%')            \n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "  AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "    (web100_log_entry.snap.State >= 5 AND                                       \n",
    "    web100_log_entry.snap.State <= 11))\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN(\n",
    "    (SELECT\n",
    "     remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "         web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "         count(*) as c1\n",
    "      FROM\n",
    "         `mlab-sandbox.batch.sidestream*`\n",
    "      WHERE\n",
    "          (test_id LIKE '%mlab1.dfw0%')\n",
    "        AND TIMESTAMP_SECONDS(web100_log_entry.log_time) >= TIMESTAMP(\"2017-08-12 00:00:00\")\n",
    "        AND TIMESTAMP_SECONDS(web100_log_entry.log_time) < TIMESTAMP(\"2017-08-16 00:00:00\")\n",
    "        AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "         web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "         count(*) as c2\n",
    "      FROM\n",
    "         `mlab-sandbox.batch.sidestream*`\n",
    "      WHERE\n",
    "          (test_id LIKE '%mlab1.dfw0%')\n",
    "        AND TIMESTAMP_SECONDS(web100_log_entry.log_time) >= TIMESTAMP(\"2017-11-26 00:00:00\")\n",
    "        AND TIMESTAMP_SECONDS(web100_log_entry.log_time) < TIMESTAMP(\"2017-11-30 00:00:00\")\n",
    "        AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 10\n",
    "    ) USING (remote_ip))\n",
    "  )\n",
    "    \n",
    "\n",
    "GROUP BY\n",
    "  hostname, slice, period, ts, rate_mbps\n",
    "    \"\"\")\n",
    "df_ss = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "#df_ss_count_raw = df_ss_count\n",
    "\n",
    "result = query.sync_query(\n",
    "    \"\"\"#standardSQL                                                                    \n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "\n",
    "SELECT\n",
    "   hostname, ts, count(*) as count\n",
    "FROM (\n",
    "    SELECT\n",
    "        REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "        UNIX_SECONDS(TIMESTAMP_TRUNC(log_time, DAY)) AS ts                            \n",
    "    FROM\n",
    "        `mlab-sandbox.batch.sidestream*`                                              \n",
    "    WHERE\n",
    "      REGEXP_CONTAINS(test_id, r\"mlab1.(dfw|lga|iad|lax|atl|nuq)[0-9]{2}.*\")     \n",
    "      AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "      AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "      AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "        web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "        web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "      AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "        web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "        web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "      AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "        (web100_log_entry.snap.State >= 5 AND                                       \n",
    "        web100_log_entry.snap.State <= 11))\n",
    "\n",
    "    GROUP BY\n",
    "      hostname, ts, web100_log_entry.connection_spec.remote_ip, web100_log_entry.connection_spec.remote_port, web100_log_entry.connection_spec.local_port, web100_log_entry.connection_spec.local_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, ts\n",
    "ORDER BY\n",
    "  hostname, ts\n",
    "    \"\"\")\n",
    "df_ss_count = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hosts = [\n",
    "    #['mlab1.dfw02', 'mlab1.dfw03', 'mlab1.dfw04', 'mlab1.dfw05'],\n",
    "    ['mlab1.dfw02', 'mlab1.dfw05'],\n",
    "]\n",
    "\n",
    "periods_list = [\n",
    "    #(datetime.datetime(2017,  8, 16), datetime.datetime(2017,  8, 23)),\n",
    "    (datetime.datetime(2017,  8, 23), datetime.datetime(2017,   8, 28)),\n",
    "    (datetime.datetime(2017,  8, 28), datetime.datetime(2017,  10, 14)),\n",
    "    #(datetime.datetime(2017, 10, 14), datetime.datetime(2017,  11, 22)),\n",
    "    #(datetime.datetime(2017, 11, 22), datetime.datetime(2018,  1,  7)),\n",
    "    (datetime.datetime(2017, 10, 14), datetime.datetime(2017,  12, 7)),\n",
    "    (datetime.datetime(2017, 12,  7), datetime.datetime(2018,  1,  12)),\n",
    "    (datetime.datetime(2018,  1, 12), datetime.datetime(2018,  1,  21)),\n",
    "    (datetime.datetime(2018,  1, 21), datetime.datetime(2018,  2,  7)),\n",
    "    (datetime.datetime(2018,  2, 7),  datetime.datetime(2018,  3,  7)),\n",
    "    #(datetime.datetime(2017, 11, 22), datetime.datetime(2018,  2, 21)),\n",
    "    #(datetime.datetime(2018,  2, 21), datetime.datetime(2018,  3,  7)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved (datetime.datetime(2017, 8, 23, 0, 0), datetime.datetime(2017, 8, 28, 0, 0)) mlab1.dfw02 819\n",
      "saved (datetime.datetime(2017, 8, 23, 0, 0), datetime.datetime(2017, 8, 28, 0, 0)) mlab1.dfw05 754\n",
      "saved (datetime.datetime(2017, 8, 28, 0, 0), datetime.datetime(2017, 10, 14, 0, 0)) mlab1.dfw02 284\n",
      "saved (datetime.datetime(2017, 8, 28, 0, 0), datetime.datetime(2017, 10, 14, 0, 0)) mlab1.dfw05 413\n",
      "saved (datetime.datetime(2017, 10, 14, 0, 0), datetime.datetime(2017, 12, 7, 0, 0)) mlab1.dfw02 586\n",
      "saved (datetime.datetime(2017, 10, 14, 0, 0), datetime.datetime(2017, 12, 7, 0, 0)) mlab1.dfw05 578\n",
      "saved (datetime.datetime(2017, 12, 7, 0, 0), datetime.datetime(2018, 1, 12, 0, 0)) mlab1.dfw02 1809\n",
      "saved (datetime.datetime(2017, 12, 7, 0, 0), datetime.datetime(2018, 1, 12, 0, 0)) mlab1.dfw05 1191\n",
      "saved (datetime.datetime(2018, 1, 12, 0, 0), datetime.datetime(2018, 1, 21, 0, 0)) mlab1.dfw02 2481\n",
      "saved (datetime.datetime(2018, 1, 12, 0, 0), datetime.datetime(2018, 1, 21, 0, 0)) mlab1.dfw05 1618\n",
      "saved (datetime.datetime(2018, 1, 21, 0, 0), datetime.datetime(2018, 2, 7, 0, 0)) mlab1.dfw02 2120\n",
      "saved (datetime.datetime(2018, 1, 21, 0, 0), datetime.datetime(2018, 2, 7, 0, 0)) mlab1.dfw05 1121\n",
      "saved (datetime.datetime(2018, 2, 7, 0, 0), datetime.datetime(2018, 3, 7, 0, 0)) mlab1.dfw02 2599\n",
      "saved (datetime.datetime(2018, 2, 7, 0, 0), datetime.datetime(2018, 3, 7, 0, 0)) mlab1.dfw05 1409\n"
     ]
    }
   ],
   "source": [
    "# STREAMS WITH MATCHING COHORTS\n",
    "def start_and_end(d):\n",
    "    s = d.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    e = (d + datetime.timedelta(days=4)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return s, e\n",
    "\n",
    "df_hosts = []\n",
    "for i, periods in enumerate(periods_list):\n",
    "    a_s, a_e = start_and_end(periods[0])\n",
    "    b_s, b_e = start_and_end(periods[1])\n",
    "    df_hosts.append({})\n",
    "    #if i not in [2]:\n",
    "    #    print 'skipping', periods_list[i]\n",
    "    #    continue\n",
    "    for host in hosts[0]:\n",
    "        result = query.sync_query(\"\"\"\n",
    "#standardSQL                                                                    \n",
    "    -- Only works for mlab1 addresses. May not work on all machines.\n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "    slice,\n",
    "    period,\n",
    "    hostname,\n",
    "    remote_ip,\n",
    "    AVG(sum_rate_mbps) as sum_rate_mbps\n",
    "\n",
    "FROM (\n",
    "\n",
    "SELECT\n",
    "   slice,\n",
    "   period,\n",
    "   hostname,\n",
    "   remote_ip,\n",
    "   --AVG(rate_mbps) as rate_mbps,\n",
    "   --APPROX_QUANTILES(rate_mbps, 101)[ORDINAL(50)] as med_rate_mbps,\n",
    "   --MAX(rate_mbps) as max_rate_mbps,\n",
    "   SUM(rate_mbps) as sum_rate_mbps\n",
    "    \n",
    "FROM (\n",
    "\n",
    "SELECT\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    CASE \n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 1 THEN 'ndt'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'samknows'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'neubot'\n",
    "        ELSE 'other' \n",
    "    END AS slice,\n",
    "    CASE                                                                          \n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "            THEN '\"\"\"+a_s+\"\"\"'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "            THEN '\"\"\"+b_s+\"\"\"'\n",
    "    ELSE 'bad'                                                                    \n",
    "    END AS period,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,                                   \n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /                                \n",
    "      (web100_log_entry.snap.SndLimTimeRwin +                                     \n",
    "       web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as rate_mbps   \n",
    "FROM\n",
    "    `mlab-sandbox.batch.sidestream*`                                              \n",
    "WHERE\n",
    "        (  betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "        )\n",
    "  AND (test_id LIKE '%\"\"\"+host+\"\"\"%')            \n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "  AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "    (web100_log_entry.snap.State >= 5 AND                                       \n",
    "    web100_log_entry.snap.State <= 11))\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN(\n",
    "    (SELECT\n",
    "     remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "         web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "         count(*) as c1\n",
    "      FROM\n",
    "         `mlab-sandbox.batch.sidestream*`\n",
    "      WHERE\n",
    "          (test_id LIKE '%\"\"\"+host+\"\"\"%')\n",
    "        AND betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "        AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "         web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "         count(*) as c2\n",
    "      FROM\n",
    "         `mlab-sandbox.batch.sidestream*`\n",
    "      WHERE\n",
    "          (test_id LIKE '%\"\"\"+host+\"\"\"%')\n",
    "        AND betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "        AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 10\n",
    "    ) USING (remote_ip))\n",
    "  )\n",
    "    \n",
    "GROUP BY\n",
    "  hostname, slice, period, ts, web100_log_entry.connection_spec.remote_ip, rate_mbps\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, slice, period, ts,  remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, slice, period, remote_ip\n",
    "\"\"\")\n",
    "        df_hosts[i][host] = pd.DataFrame(result)\n",
    "        print 'saved', periods_list[i], host, len(df_hosts[i][host])\n",
    "#df_ss = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ds = df_hosts[0]['mlab1.dfw02']\n",
    "#a = ds[ (ds['slice'] == 'samknows') & (ds['period'] == '2017-08-23 00:00:00')]\n",
    "##b = ds[ (ds['slice'] == 'samknows') & (ds['period'] == '2017-08-28 00:00:00')]\n",
    "#a, b\n",
    "\n",
    "#pd.merge(a, b,  how='left', left_on=['hostname', 'remote_ip', 'slice'], right_on = ['hostname', 'remote_ip', 'slice'])\n",
    "\n",
    "#a.join(b.set_index('remote_ip'), on='remote_ip', lsuffix='_a', rsuffix='_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved (datetime.datetime(2017, 8, 16, 0, 0), datetime.datetime(2017, 8, 23, 0, 0)) mlab1.dfw02 31008\n",
      "saved (datetime.datetime(2017, 8, 16, 0, 0), datetime.datetime(2017, 8, 23, 0, 0)) mlab1.dfw03 29927\n",
      "saved (datetime.datetime(2017, 8, 16, 0, 0), datetime.datetime(2017, 8, 23, 0, 0)) mlab1.dfw04 29941\n",
      "saved (datetime.datetime(2017, 8, 16, 0, 0), datetime.datetime(2017, 8, 23, 0, 0)) mlab1.dfw05 31220\n",
      "saved (datetime.datetime(2017, 8, 23, 0, 0), datetime.datetime(2017, 8, 28, 0, 0)) mlab1.dfw02 31113\n",
      "saved (datetime.datetime(2017, 8, 23, 0, 0), datetime.datetime(2017, 8, 28, 0, 0)) mlab1.dfw03 30294\n",
      "saved (datetime.datetime(2017, 8, 23, 0, 0), datetime.datetime(2017, 8, 28, 0, 0)) mlab1.dfw04 29945\n",
      "saved (datetime.datetime(2017, 8, 23, 0, 0), datetime.datetime(2017, 8, 28, 0, 0)) mlab1.dfw05 31137\n",
      "saved (datetime.datetime(2017, 8, 28, 0, 0), datetime.datetime(2017, 11, 22, 0, 0)) mlab1.dfw02 31078\n",
      "saved (datetime.datetime(2017, 8, 28, 0, 0), datetime.datetime(2017, 11, 22, 0, 0)) mlab1.dfw03 31273\n",
      "saved (datetime.datetime(2017, 8, 28, 0, 0), datetime.datetime(2017, 11, 22, 0, 0)) mlab1.dfw04 31348\n",
      "saved (datetime.datetime(2017, 8, 28, 0, 0), datetime.datetime(2017, 11, 22, 0, 0)) mlab1.dfw05 31382\n",
      "saved (datetime.datetime(2017, 11, 22, 0, 0), datetime.datetime(2018, 2, 21, 0, 0)) mlab1.dfw02 33344\n",
      "saved (datetime.datetime(2017, 11, 22, 0, 0), datetime.datetime(2018, 2, 21, 0, 0)) mlab1.dfw03 33696\n",
      "saved (datetime.datetime(2017, 11, 22, 0, 0), datetime.datetime(2018, 2, 21, 0, 0)) mlab1.dfw04 33405\n",
      "saved (datetime.datetime(2017, 11, 22, 0, 0), datetime.datetime(2018, 2, 21, 0, 0)) mlab1.dfw05 32253\n",
      "saved (datetime.datetime(2018, 2, 21, 0, 0), datetime.datetime(2018, 3, 7, 0, 0)) mlab1.dfw02 39887\n",
      "saved (datetime.datetime(2018, 2, 21, 0, 0), datetime.datetime(2018, 3, 7, 0, 0)) mlab1.dfw03 38836\n",
      "saved (datetime.datetime(2018, 2, 21, 0, 0), datetime.datetime(2018, 3, 7, 0, 0)) mlab1.dfw04 36768\n",
      "saved (datetime.datetime(2018, 2, 21, 0, 0), datetime.datetime(2018, 3, 7, 0, 0)) mlab1.dfw05 37670\n"
     ]
    }
   ],
   "source": [
    "# ALL STREAMS per PERIOD\n",
    "\n",
    "def start_and_end(d):\n",
    "    s = d.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    e = (d + datetime.timedelta(days=4)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return s, e\n",
    "\n",
    "df_hosts = []\n",
    "for i, periods in enumerate(periods_list):\n",
    "    a_s, a_e = start_and_end(periods[0])\n",
    "    b_s, b_e = start_and_end(periods[1])\n",
    "    df_hosts.append({})\n",
    "    #if i not in [2]:\n",
    "    #    print 'skipping', periods_list[i]\n",
    "    #    continue\n",
    "    for host in hosts[0]:\n",
    "        result = query.sync_query(\"\"\"\n",
    "#standardSQL                                                                    \n",
    "    -- Only works for mlab1 addresses. May not work on all machines.\n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "   slice,\n",
    "   period,\n",
    "   hostname,\n",
    "   AVG(rate_mbps) as rate_mbps,\n",
    "   APPROX_QUANTILES(rate_mbps, 101)[ORDINAL(50)] as med_rate_mbps,\n",
    "   MAX(rate_mbps) as max_rate_mbps,\n",
    "   SUM(rate_mbps) as sum_rate_mbps\n",
    "    \n",
    "FROM (\n",
    "\n",
    "SELECT\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    CASE \n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 1 THEN 'ndt'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'samknows'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'neubot'\n",
    "        ELSE 'other' \n",
    "    END AS slice,\n",
    "    CASE                                                                          \n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "            THEN '\"\"\"+a_s+\"\"\"'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "            THEN '\"\"\"+b_s+\"\"\"'\n",
    "    ELSE 'bad'                                                                    \n",
    "    END AS period,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,                                   \n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /                                \n",
    "      (web100_log_entry.snap.SndLimTimeRwin +                                     \n",
    "       web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as rate_mbps   \n",
    "FROM\n",
    "    `mlab-sandbox.batch.sidestream*`                                              \n",
    "WHERE\n",
    "        (  betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "        )\n",
    "  AND (test_id LIKE '%\"\"\"+host+\"\"\"%')            \n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "  AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "    (web100_log_entry.snap.State >= 5 AND                                       \n",
    "    web100_log_entry.snap.State <= 11))\n",
    "    \n",
    "GROUP BY\n",
    "  hostname, slice, period, ts, web100_log_entry.connection_spec.remote_ip, rate_mbps\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, slice, period,   remote_ip -- CAST(ts/4 AS INT64),\n",
    "\"\"\")\n",
    "        df_hosts[i][host] = pd.DataFrame(result)\n",
    "        print 'saved', periods_list[i], host, len(df_hosts[i][host])\n",
    "#df_ss = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidestream CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10079 507.5426899233347\n"
     ]
    }
   ],
   "source": [
    "title = 'CDF - per-slice sidestream Download Rate CDFs'\n",
    "print len(df_ss), max(df_ss['rate_mbps']) # 'sqrt', int(math.sqrt(len(df['rate_mbps'])))\n",
    "\n",
    "hosts = [\n",
    "    ['mlab1.dfw02', 'mlab1.dfw03', 'mlab1.dfw04'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(hosts[0]), figsize=(13, 10))\n",
    "\n",
    "for i, host_row in enumerate(hosts):\n",
    "    for j, host in enumerate(host_row):\n",
    "        if len(df_ss[ df_ss['hostname'] == host ]) == 0:\n",
    "            print 'skipping', host\n",
    "            continue\n",
    "        for period in ['08-12 to 16', '11-29 to 12-03', '02-21 to 25']: #set(df['period']):\n",
    "            ds = df_ss[ (df_ss['period'] == period) & (df_ss['hostname'] == host) ]\n",
    "            for k, slicename in enumerate(['ndt', 'samknows']): # set(ds['slice']):\n",
    "                #print 'sqrt', int(math.sqrt(len(ds['rate_mbps'])))\n",
    "                d = ds[ ds['slice'] == slicename ]\n",
    "                if len(d) == 0:\n",
    "                    continue\n",
    "                ax = axes[k, j]\n",
    "                #n, bins, patches = ax.hist(ds['rate_mbps'], len(ds['rate_mbps']),\n",
    "                #                   histtype='step', normed=1, cumulative=True, label='cdf-' + period,\n",
    "                #                   ls='-')\n",
    "                r = [math.log10(x) for x in d['rate_mbps']]\n",
    "                n, bins, patches = ax.hist(d['rate_mbps'], int(math.sqrt(len(d['rate_mbps']))),\n",
    "                                   histtype='step', normed=1, cumulative=True, label=('cdf-' + period + '-' + slicename), \n",
    "                                   ls='-')\n",
    "\n",
    "                ax.set_xlim(1, 200)       \n",
    "                ax.set_axisbelow(True)\n",
    "        #ax.semilogx()\n",
    "                ax.legend(loc=4, fontsize='x-small')\n",
    "\n",
    "                ax.grid(color='#dddddd')\n",
    "                ax.set_title(host)\n",
    "        #labels = ['%.2f' % math.pow(10, float(l)) for l in ax.get_xticks()]\n",
    "        #ax.xaxis.set_major_formatter(logFormatter)\n",
    "        \n",
    "\n",
    "fig.suptitle(title)\n",
    "\n",
    "plt.show()\n",
    "#print n, len(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidestream PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10079 507.5426899233347\n"
     ]
    }
   ],
   "source": [
    "title = 'PDF - per-slice sidestream Download Rate PDFs'\n",
    "print len(df_ss), max(df_ss['rate_mbps']) # 'sqrt', int(math.sqrt(len(df['rate_mbps'])))\n",
    "\n",
    "hosts = [\n",
    "    ['mlab1.dfw02', 'mlab1.dfw03', 'mlab1.dfw04'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(hosts[0]), figsize=(13, 10))\n",
    "for i, host_row in enumerate(hosts):\n",
    "    for j, host in enumerate(host_row):\n",
    "        if len(df_ss[ df_ss['hostname'] == host ]) == 0:\n",
    "            print 'skipping', host\n",
    "            continue\n",
    "        for period in ['08-12 to 16', '02-21 to 25']: # '11-29 to 12-03',  #set(df['period']):\n",
    "            ds = df_ss[ (df_ss['period'] == period) & (df_ss['hostname'] == host) ]\n",
    "            for k, slicename in enumerate(['ndt', 'samknows']): # set(ds['slice']):\n",
    "                #print 'sqrt', int(math.sqrt(len(ds['rate_mbps'])))\n",
    "                d = ds[ ds['slice'] == slicename ]\n",
    "                if len(d) == 0:\n",
    "                    continue\n",
    "                ax = axes[k, j]\n",
    "\n",
    "                label = 'pdf-%s-%s (%d)' % (period, slicename, len(d['rate_mbps']))\n",
    "                r = [math.log10(x) for x in d['rate_mbps']]\n",
    "                n, bins, patches = ax.hist(r, int(math.sqrt(len(d['rate_mbps']))),\n",
    "                                   histtype='step', normed=1, label=label, \n",
    "                                   ls='-')\n",
    "\n",
    "                #ax.set_xlim(1, 200)       \n",
    "                #ax.semilogx()\n",
    "                #ax.set_ylim(0, 1.4)\n",
    "                ax.set_axisbelow(True)\n",
    "                ax.legend(loc=2, fontsize='x-small')\n",
    "\n",
    "                ax.grid(color='#dddddd')\n",
    "                ax.set_title(host)\n",
    "                ax.xaxis.set_major_formatter(logFormatter)\n",
    "        \n",
    "fig.suptitle(title)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF, CDF, & Switch - by Site and Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last\n",
      "last\n"
     ]
    }
   ],
   "source": [
    "title = 'PDF, CDF, Switch - slice sidestream Download Rates'\n",
    "#print len(df_ss), max(df_ss['rate_mbps']) # 'sqrt', int(math.sqrt(len(df['rate_mbps'])))\n",
    "\n",
    "d = None\n",
    "df=None\n",
    "label2date = {}\n",
    "#slices = ['samknows', 'ndt']\n",
    "slices = ['samknows']\n",
    "colors = plt.cm.Dark2.colors\n",
    "colors = plt.cm.tab10.colors\n",
    "p2c = {}\n",
    "c=0\n",
    "for i, host_row in enumerate(hosts):\n",
    "    for j, host in enumerate(host_row):\n",
    "\n",
    "        rows = 4\n",
    "        cols = len(periods_list)\n",
    "        fig = plt.figure(figsize=(4 * cols, 13))\n",
    "        axes = [\n",
    "            [None] * cols,\n",
    "            [None] * cols,\n",
    "            [None] * cols,\n",
    "            None,\n",
    "        ]\n",
    "        \n",
    "        for p, times in enumerate(periods_list):\n",
    "            axes[0][p] = plt.subplot2grid((rows, cols), (0, p))\n",
    "            axes[1][p] = plt.subplot2grid((rows, cols), (1, p))\n",
    "            axes[2][p] = plt.subplot2grid((rows, cols), (2, p))\n",
    "\n",
    "            for k, slicename in enumerate(slices):\n",
    "                \n",
    "                df_ss = df_hosts[p][host]\n",
    "                if len(df_ss) == 0:\n",
    "                    print 'skipping', host, 'no data'\n",
    "                    continue\n",
    "                if len(df_ss[ df_ss['hostname'] == host ]) == 0:\n",
    "                    print 'skipping', host\n",
    "                    continue\n",
    "                a = None\n",
    "                al = None\n",
    "                for period in times:\n",
    "                    period_str = period.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    if period_str not in p2c:\n",
    "                        p2c[period_str] = colors[c]\n",
    "                        c += 1\n",
    "                    ds = df_ss[ (df_ss['period'] == period_str) &\n",
    "                                (df_ss['hostname'] == host) &\n",
    "                                (df_ss['slice'] == slicename) ]\n",
    "\n",
    "                    if len(ds) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Top\n",
    "                    ax = axes[0][p]\n",
    "                    if a is None:\n",
    "                        a = [math.log10(x) for x in ds['sum_rate_mbps']]\n",
    "                        al = 'pdf-%s-%s (%d)' % (period_str, slicename, len(a))\n",
    "                        label2date[al] = period\n",
    "                        ac = p2c[period_str]\n",
    "                    else:\n",
    "                        b = [math.log10(x) for x in ds['sum_rate_mbps']]\n",
    "                        bl = 'pdf-%s-%s (%d)' % (period_str, slicename, len(b))\n",
    "                        label2date[bl] = period\n",
    "                        bc = p2c[period_str]\n",
    "                        \n",
    "                        n, bins, patches = ax.hist(\n",
    "                            (a,b), int(math.sqrt(len(a))),\n",
    "                            histtype='step', normed=1, label=(al,bl), ls='-',\n",
    "                            color=(ac,bc))\n",
    "\n",
    "                        ax.set_axisbelow(True)\n",
    "                        ax.legend(fontsize='x-small', loc='upper center', bbox_to_anchor=(0.5, 1.3))\n",
    "                        ax.grid(color='#dddddd')\n",
    "                        ax.set_title(host)\n",
    "                        ax.xaxis.set_major_formatter(logFormatter)\n",
    "\n",
    "                    # Middle\n",
    "                    ax = axes[1][p]\n",
    "                    label = 'cdf-%s-%s (%d)' % (period, slicename, len(ds['sum_rate_mbps']))\n",
    "                    r = [math.log10(x) for x in ds['sum_rate_mbps']]\n",
    "                    n, bins, patches = ax.hist(r, len(r), # ds['rate_mbps']\n",
    "                                               histtype='step', normed=1, cumulative=True, label=label, ls='-',\n",
    "                                               color=p2c[period_str])\n",
    "                    ax.xaxis.set_major_formatter(logFormatter)\n",
    "\n",
    "                    ax.set_axisbelow(True)\n",
    "                    ax.grid(color='#dddddd')\n",
    "                    ax.set_title(host)\n",
    "                    if p != 0:\n",
    "                        ax.set_yticklabels([])\n",
    "                        \n",
    "                if True:\n",
    "                    # Bottom\n",
    "                    t_a, t_b = times\n",
    "                    p_a, p_b = t_a.strftime(\"%Y-%m-%d %H:%M:%S\"), t_b.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    a = df_ss[ (df_ss['slice'] == slicename) & (df_ss['period'] == p_a) ]\n",
    "                    b = df_ss[ (df_ss['slice'] == slicename) & (df_ss['period'] == p_b) ]\n",
    "\n",
    "                    columns = ['hostname', 'remote_ip', 'slice']\n",
    "                    d = pd.merge(a, b,  how='left', left_on=columns, right_on=columns)\n",
    "                    #print d\n",
    "                    ax = axes[2][p]\n",
    "                    \n",
    "                    label = 'scatter-%s (%d)' % (slicename, len(d['sum_rate_mbps_x']))\n",
    "                    \n",
    "                    ax.plot([0.1, 1000], [0.1, 1000], color='r', alpha=0.1)\n",
    "                    ax.scatter(d['sum_rate_mbps_x'], d['sum_rate_mbps_y'], s=2, alpha=0.2)\n",
    "                    \n",
    "                    #ax.scatter([100], [200])\n",
    "                    ax.set_xlim(.1, 1000)\n",
    "                    ax.set_ylim(.1, 1000)\n",
    "                    \n",
    "                    #ax.set_xlabel('slow')\n",
    "                    #ax.set_ylabel('fast')\n",
    "                    ax.set_xlabel(p_a)\n",
    "                    ax.set_ylabel(p_b)\n",
    "\n",
    "                    \n",
    "                    #ax.xaxis.set_major_formatter(logFormatter)\n",
    "\n",
    "                    #ax.set_axisbelow(True)\n",
    "                    ax.grid(color='#dddddd')\n",
    "                    ax.semilogx()\n",
    "                    ax.semilogy()\n",
    "\n",
    "                    #ax.set_title(host)\n",
    "                    #if p != 0:\n",
    "                    #    ax.set_yticklabels([])\n",
    "                        \n",
    "            axes[0][p].set_xlim(math.log10(.1), math.log10(1100))\n",
    "            axes[1][p].set_xlim(math.log10(.1), math.log10(1100))\n",
    "\n",
    "        if True:\n",
    "            print 'last'\n",
    "            axes[3] = plt.subplot2grid((rows, cols), (3, 0), colspan=cols)\n",
    "            ax = axes[3]\n",
    "        \n",
    "            ds = df_disco[ df_disco['hostname'] == host ]\n",
    "            ax.plot_date(dates.epoch2num(ds['ts']), ds['pct_discards'], ls='-', ms=0, label='switch', color='mediumpurple')\n",
    "        \n",
    "            ax.set_title(host)\n",
    "            ax.set_ylim(-0.01, 1)\n",
    "            ax.tick_params(axis='x', labelrotation=90)\n",
    "            ax.grid(color='#dddddd')\n",
    "            #ax.legend(loc=4, fontsize='x-small') \n",
    " \n",
    "            # Color switch regions with the PDF periods based on legend colors.\n",
    "            for p in range(0, len(periods_list)):\n",
    "                h, l = axes[0][p].get_legend_handles_labels()\n",
    "                for k, line in enumerate(h):\n",
    "                    s = label2date[l[k]]\n",
    "                    e = s + datetime.timedelta(days=4)\n",
    "                    color = h[k].get_edgecolor()\n",
    "                    ax.axvspan(dates.date2num(s), dates.date2num(e), alpha=0.5, color=color)\n",
    "\n",
    "            ax.set_ylabel('% discard timebins')                \n",
    "            ax2 = ax.twinx()\n",
    "        \n",
    "            ds = df_ss_count[ df_ss_count['hostname'] == host ]\n",
    "            ax2.plot_date(dates.epoch2num(ds['ts']), ds['count'], ls='-', ms=0, label='sidestream')\n",
    "\n",
    "            if p != 4:\n",
    "                ax2.set_yticklabels([])\n",
    "            else:\n",
    "                ax2.set_ylabel('Sidestream Flow Count')\n",
    "\n",
    "            ax2.grid(color='#dddddd')\n",
    "            ax.legend(loc=3, fontsize='small') \n",
    "            ax2.legend(loc=1, fontsize='small') \n",
    "\n",
    "     \n",
    "        axes[0][0].set_ylabel('PDF')\n",
    "        axes[1][0].set_ylabel('CDF')\n",
    "        #axes[2, 0].set_ylabel('% discard timebins')    \n",
    "        #axes[2, 0].set_ylabel('Sidestream Flow Count')    \n",
    "\n",
    "        fig.suptitle(title) # + ('\\n%s' % [period.strftime(\"%Y-%m-%d %H:%M:%S\") for period in times]))\n",
    "        fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "        plt.show()\n",
    "\n",
    "#plt.hist2d(\n",
    "#    df_ss[ (df_ss['period'] == '08-12 to 16') & (df_ss['hostname'] == 'mlab1.dfw02') & (df_ss['slice'] == 'samknows') ],\n",
    "#    df_ss[ (df_ss['period'] == '02-21 to 25') & (df_ss['hostname'] == 'mlab1.dfw02') & (df_ss['slice'] == 'samknows') ],\n",
    "#    bins=40,\n",
    "#)\n",
    "#print n, len(bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
