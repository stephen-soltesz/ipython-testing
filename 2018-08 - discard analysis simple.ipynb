{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/soltesz/.local/lib/python2.7/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# Enables figures loading outside of browser.\n",
    "# If not run, figures will load inline.\n",
    "%matplotlib\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Depends on: pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')\n",
    "\n",
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def run_query(query, project='mlab-sandbox'):\n",
    "    client = bigquery.Client(project=project)\n",
    "    job = client.query(query)\n",
    "\n",
    "    results = collections.defaultdict(list)\n",
    "    for row in job.result(timeout=300):\n",
    "        for key in row.keys():\n",
    "            results[key].append(row.get(key))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def unlog(x, pos):\n",
    "    \"\"\"Formats the x axis for histograms taken on the log of values.\"\"\"\n",
    "    v = math.pow(10, x)\n",
    "    frac, whole = math.modf(v)\n",
    "    if frac > 0:\n",
    "        return '%.1f' % v\n",
    "    else:\n",
    "        return '%d' % whole\n",
    "    \n",
    "    \n",
    "def hist(vals, bin_count, log=True, cdf=False):\n",
    "    \"\"\"Produces hist or cdf values for smooth plots.\"\"\"\n",
    "    if log:\n",
    "        r = [math.log10(x) for x in vals]\n",
    "    else:\n",
    "        r = vals\n",
    "        \n",
    "    m, bins = np.histogram(r, bin_count, normed=True)\n",
    "    m = m.astype(float)\n",
    "\n",
    "    tops = m\n",
    "    if cdf:\n",
    "        tops = np.cumsum(m)\n",
    "        total = sum(m)\n",
    "        tops = [float(t) / total for t in tops ]\n",
    "    \n",
    "    return tops, bins\n",
    "\n",
    "\n",
    "logFormatter = matplotlib.ticker.FuncFormatter(unlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_scatter(\n",
    "    df, xname, yname,\n",
    "    fig_by='', axes_by='', group_by='',\n",
    "    figsize=(6,8), axes=(1,1),\n",
    "    xlabel='', ylabel='',\n",
    "    xlim=(), ylim=(),\n",
    "    fx=list, fy=list,\n",
    "    ylog=False, suptitle='', title='', legend={}):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        xname: str, name of column to use as x-axis.\n",
    "        yname: str, name of column to use as y-axis.\n",
    "        fig_by: str, name of column to split data into multiple figures.\n",
    "        axes_by: str, name of column to arrange into a single panel.\n",
    "        group_by: str, name of column to plot common split_by and group_by columns.\n",
    "        figsize: (int, int), dimensions of figure.\n",
    "        axes: (int, int), arrangement of axes within figure.\n",
    "        xlabel: str, \n",
    "        ylabel: str, \n",
    "        fx: func,\n",
    "        fy: func,\n",
    "        xlim: (xmin, xmax),\n",
    "        ylim: (ymin, ymax),\n",
    "        ylog: bool,\n",
    "        title: str,\n",
    "        legend: **legend_args,\n",
    "    \"\"\"\n",
    "    for f in sorted(set(['default'] if not fig_by else df[fig_by])):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.subplots(axes[0], axes[1], squeeze=False)\n",
    "        ax_index = list(itertools.product(range(axes[0]), range(axes[1])))\n",
    "        \n",
    "        df_fig = df if f == 'default' else df[df[fig_by] == f]\n",
    "        for p, a in enumerate(sorted(set(['default'] if not axes_by else df_fig[axes_by]))):\n",
    "\n",
    "            df_axes = df_fig if a == 'default' else df_fig[df_fig[axes_by] == a]\n",
    "            i, j = ax_index[p]\n",
    "            for g in sorted(set(['default'] if not group_by else df_axes[group_by])):\n",
    "                df_g = df_axes if g == 'default' else df_axes[df_axes[group_by] == g]\n",
    "\n",
    "                x = fx(df_g[xname])\n",
    "                y = fy(df_g[yname])\n",
    "\n",
    "                ax[i][j].scatter(x, y, s=1, label=g)\n",
    "\n",
    "            if i != len(ax)-1:\n",
    "                ax[i][j].set_xticklabels([])\n",
    "\n",
    "            if title:\n",
    "                ax[i][j].set_title(title.format(figure=f, axis=a, group=g))\n",
    "            if ylabel:\n",
    "                ax[i][j].set_ylabel(ylabel.format(figure=f, axis=a, group=g))\n",
    "            if xlabel:\n",
    "                ax[i][j].set_xlabel(xlabel.format(figure=f, axis=a, group=g))\n",
    "\n",
    "            if xlim:\n",
    "                ax[i][j].set_xlim(xlim)\n",
    "            if ylim:\n",
    "                ax[i][j].set_ylim(ylim)\n",
    "            ax[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "            ax[i][j].grid(color='#dddddd')\n",
    "            ax[i][j].legend(fontsize='x-small', **legend)\n",
    "            if ylog:\n",
    "                ax[i][j].semilogy()\n",
    "\n",
    "        fig.suptitle(title.format(f))\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(\n",
    "    df, cname, bins=None,\n",
    "    fig_by='', axes_by='', group_by='',\n",
    "    figsize=(6,8), axes=(1,1),\n",
    "    xlabel='', ylabel='',\n",
    "    xlim=(), ylim=(),\n",
    "    fy=list,\n",
    "    xlog=False, suptitle='', title='', legend={}, cdf=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        cname: str, name of column to use as data source.\n",
    "        fig_by: str, name of column to split data into multiple figures.\n",
    "        axes_by: str, name of column to arrange into a single panel.\n",
    "        group_by: str, name of column to plot common split_by and group_by columns.\n",
    "        figsize: (int, int), dimensions of figure.\n",
    "        axes: (int, int), arrangement of axes within figure.\n",
    "        xlabel: str, \n",
    "        ylabel: str, \n",
    "        fx: func,\n",
    "        fy: func,\n",
    "        xlim: (xmin, xmax),\n",
    "        ylim: (ymin, ymax),\n",
    "        xlog: bool,\n",
    "        title: str,\n",
    "        suptitle: str,\n",
    "        legend: **legend_args,\n",
    "        cdf: bool,\n",
    "    \"\"\"\n",
    "    for f in sorted(set(['default'] if not fig_by else df[fig_by])):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.subplots(axes[0], axes[1], squeeze=False)\n",
    "        ax_index = list(itertools.product(range(axes[0]), range(axes[1])))\n",
    "        \n",
    "        df_fig = df if f == 'default' else df[df[fig_by] == f]\n",
    "        for p, a in enumerate(sorted(set(['default'] if not axes_by else df_fig[axes_by]))):\n",
    "\n",
    "            df_axes = df_fig if a == 'default' else df_fig[df_fig[axes_by] == a]\n",
    "            if p >= len(ax_index):\n",
    "                print 'SKIPPING', p, f, a, 'too few axes positions'\n",
    "                continue\n",
    "                \n",
    "            i, j = ax_index[p]\n",
    "            for g in sorted(set(['default'] if not group_by else df_axes[group_by])):\n",
    "                df_g = df_axes if g == 'default' else df_axes[df_axes[group_by] == g]\n",
    "\n",
    "                r = df_g[cname]\n",
    "                if bins is None:\n",
    "                    size = int(math.sqrt(len(r)))\n",
    "                else:\n",
    "                    size = bins(r)\n",
    "                h_tops, h_bins = hist(r, size, log=xlog , cdf=cdf)\n",
    "                ax[i][j].plot(h_bins[:-1], h_tops, label=('%s-%s-%s' % (cname, a, g)))\n",
    "\n",
    "            if i != len(ax)-1:\n",
    "                ax[i][j].set_xticklabels([])\n",
    "\n",
    "            if title:\n",
    "                ax[i][j].set_title(title.format(figure=f, axis=a, group=g))\n",
    "            if ylabel:\n",
    "                ax[i][j].set_ylabel(ylabel.format(figure=f, axis=a, group=g))\n",
    "            if xlabel:\n",
    "                ax[i][j].set_xlabel(xlabel.format(figure=f, axis=a, group=g))\n",
    "\n",
    "            if xlim:\n",
    "                ax[i][j].set_xlim(xlim)\n",
    "            if ylim:\n",
    "                ax[i][j].set_ylim(ylim)\n",
    "\n",
    "            ax[i][j].grid(color='#dddddd')\n",
    "            ax[i][j].legend(fontsize='x-small', **legend)\n",
    "            if xlog:\n",
    "                ax[i][j].xaxis.set_major_formatter(logFormatter)\n",
    "\n",
    "\n",
    "        if suptitle:\n",
    "            fig.suptitle(suptitle.format(figure=f))\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLINK UTILIZATION OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_disco_pct = run_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,  \n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(50)] as bytes_50th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(90)] as bytes_90th\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab1.[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_scatter(\n",
    "    df_disco_pct, 'ts', 'bytes_50th', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    suptitle='Daily Median Uplink Utilization',\n",
    "    ylabel=\"Median Uplink {axis}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e4, 1e9),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':3, 'ncol':7, 'columnspacing':1},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packets Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_disco_packets = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "    \n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "    \n",
    "  WHERE\n",
    "    metric LIKE 'switch.unicast.uplink.tx'\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    \n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(value) AS total\n",
    "  \n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "  \n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "  \n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "  \n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_disco_packets, 'ts', 'total', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    suptitle='Daily Packets',\n",
    "    ylabel=\"Total Packets {axis}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e7, 1e10),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':3, 'ncol':7, 'columnspacing':1},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DISCARDS OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_disco = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "    \n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "    \n",
    "  WHERE\n",
    "    metric LIKE 'switch.discards.uplink.tx'\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    \n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(value) AS total_discards\n",
    "  \n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "  \n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "  \n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "  \n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_disco, 'ts', 'total_discards', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    suptitle='Daily Packet Discards',\n",
    "    ylabel=\"Total Discards {axis}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e2, 1e6),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily DISCO discard ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_disco_ratio = run_query(\"\"\"\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    (metric LIKE 'switch.discards.uplink.tx' OR metric LIKE 'switch.unicast.uplink.tx')\n",
    "    AND (hostname LIKE '%mlab1.lga%' OR hostname LIKE '%mlab1.dfw%' OR hostname LIKE '%mlab1.nuq%')\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  hostname,\n",
    "  day,\n",
    "  ts,\n",
    "  IF(total > 0, discards / total, 0) as ratio\n",
    "FROM (\n",
    "SELECT\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(IF(metric = \"switch.discards.uplink.tx\", value, 0)) AS discards,\n",
    "  SUM(IF(metric = \"switch.unicast.uplink.tx\", value, 0)) AS total\n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "HAVING\n",
    "  discards < total\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    ")\n",
    "GROUP BY\n",
    "  hostname, day, ts, ratio\n",
    "HAVING\n",
    "  ratio < 0.01\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_scatter(\n",
    "    df_disco_ratio, 'ts', 'ratio', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    suptitle='Daily Packet Loss Ratio',\n",
    "    ylabel=\"Discard Ratio {axis}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e-6, 1e-1),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDT Median Download Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ndt_median = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "\n",
    "  FROM\n",
    "  \n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    \n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  REGEXP_EXTRACT(server_hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(server_hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "  APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps\n",
    "  \n",
    "FROM\n",
    "  mlab_ndt\n",
    "\n",
    "GROUP BY\n",
    "  day,\n",
    "  server_hostname\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_ndt_median, 'day', 'download_mbps', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    suptitle='Median NDT Download Rates',\n",
    "    ylabel=\"Mbps {axis}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1, 100),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDT Segs Retrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOT ENOUGH HISTORICAL NDT DATA TO GET FULL TIMELINE.\n",
    "\n",
    "if False:\n",
    "    df_ndt_retrans = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(log_time, DAY) AS day,\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  SAFE_DIVIDE(SUM(web100_log_entry.snap.SegsRetrans), SUM(web100_log_entry.snap.SegsOut)) AS median_ratio\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq|ord|atl|lax)\\d\\d\")\n",
    "  AND connection_spec.data_direction = 1\n",
    "  AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "GROUP BY\n",
    "  day,\n",
    "  hostname\n",
    "HAVING\n",
    "  median_ratio is not NULL\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ndt_retrans = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as hostname,\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    log_time,\n",
    "    web100_log_entry.snap.SegsRetrans as SegsRetrans,\n",
    "    web100_log_entry.snap.SegsOut as SegsOut\n",
    "\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "\n",
    "  WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"(lga|dfw|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "  \n",
    "  GROUP BY\n",
    "    connection_spec.server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    SegsRetrans,\n",
    "    SegsOut\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  REGEXP_EXTRACT(hostname, r\"([a-z]{3})[0-9]{2}\") as metro,\n",
    "  REGEXP_EXTRACT(hostname, r\"([a-z]{3}[0-9]{2})\") as site,\n",
    "  day,\n",
    "  APPROX_QUANTILES(ratio, 101)[ORDINAL(50)] AS median_ratio,\n",
    "  count(*) as count\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    hostname,\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "    MAX(SAFE_DIVIDE(SegsRetrans, SegsOut)) as ratio\n",
    "\n",
    "  FROM\n",
    "    mlab_ndt\n",
    "\n",
    "  GROUP BY\n",
    "    hostname,\n",
    "    day,\n",
    "    remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, day\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_ndt_retrans, 'day', 'median_ratio', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    suptitle='Median NDT SegsRetran/SegsOut Ratio Rates',\n",
    "    ylabel=\"Retransmission {axis}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e-6, 1e-1),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def box(x, y, text):\n",
    "    plt.text(x, y, text,\n",
    "        bbox=dict(boxstyle=\"round\",\n",
    "              ec=(.5, 0.5, 1., 0.25),\n",
    "              fc=(.5, 0.8, 1., 0.25),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11762\n"
     ]
    }
   ],
   "source": [
    "## COMBINED SegsRetrans & Switch Discards\n",
    "\n",
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "print len(df_ndt_retrans)\n",
    "\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Ratio ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "\n",
    "        c = 0\n",
    "        for s in sorted(set(df_ndt_retrans['site'])):\n",
    "            if site in s:\n",
    "                ds = df_ndt_retrans[ (df_ndt_retrans['site'] == s) ]\n",
    "                d = [pd.to_datetime(t) for t in ds['day']]\n",
    "                axes[i][j].scatter(d, ds['median_ratio'], s=1, label=s, c=colors[c])\n",
    "                c += 1\n",
    "\n",
    "        axes[i][j].set_ylim(1e-6, 1e-1)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):       \n",
    "\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "        c = 0\n",
    "        for h in set(df_disco_ratio['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco_ratio[ (df_disco_ratio['hostname'] == h) ]\n",
    "                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "                axes[i][j].scatter(d, ds['ratio'], s=1, label=h[6:11], c=colors[c])\n",
    "                c += 1\n",
    "\n",
    "box(pd.to_datetime(\"2016-10-30\"), 5e-3, u\"Segs Retransmit ↘\")\n",
    "box(pd.to_datetime(\"2016-10-30\"), 9e-6, u\"Switch Discards ↗\")\n",
    "                \n",
    "fig.suptitle('Retrans & Switch Discard Rates')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One Week Performance Distributions -- Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_disco_dist = run_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    CASE\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "        ELSE 'what'\n",
    "    END as period,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND ( -- One week - Sunday to Saturday.\n",
    "          TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-25\")\n",
    "       OR TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-25\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print df_disco_dist.keys()\n",
    "print len(df_disco_dist)\n",
    "\n",
    "\n",
    "for h in set(df_disco_dist['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist['period'])):\n",
    "        ds = df_disco_dist[ (df_disco_dist['name'] == h) & (df_disco_dist['period'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    df_disco_dist, 'bps', fig_by='name', group_by='period', suptitle='Uplink Utilization Distribution', title='{figure}',\n",
    "    cdf=False, xlog=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One Day Performance Distributions -- before & after\n",
    "\n",
    "Harder to notice changes wrt before  after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_disco_dist4 = run_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND (\n",
    "        TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-18\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-25\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-11\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-05\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-10\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print df_disco_dist4.keys()\n",
    "print len(df_disco_dist4)\n",
    "\n",
    "for h in set(df_disco_dist4['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist4['day'])):\n",
    "        ds = df_disco_dist4[ (df_disco_dist4['name'] == h) & (df_disco_dist4['day'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    df_disco_dist4, 'bps', lambda r: int(1.5 * math.sqrt(len(r))),\n",
    "    fig_by='name', group_by='day', suptitle='Uplink Utilization Distribution',\n",
    "    title='{figure}',\n",
    "    cdf=False, xlog=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NDT test distributions - Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "  (    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps)\n",
    "    \n",
    "SELECT\n",
    "  REGEXP_EXTRACT(server_hostname, r'mlab1.([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(server_hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  --web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "  --(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  MAX(download_mbps) AS download_mbps\n",
    "  --APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps\n",
    "\n",
    "FROM\n",
    "  mlab_ndt\n",
    "WHERE\n",
    "  remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 10\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  server_hostname,\n",
    "  period,\n",
    "  remote_ip\n",
    "  --download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print df_ndt_dist.keys()\n",
    "print len(df_ndt_dist)\n",
    "\n",
    "def hist(vals, bin_count, log=True, cdf=False):\n",
    "    \"\"\"Produces hist or cdf values for smooth plots.\"\"\"\n",
    "    if log:\n",
    "        r = [math.log10(x) for x in vals]\n",
    "    else:\n",
    "        r = vals\n",
    "        \n",
    "    m, bins = np.histogram(r, bin_count, normed=True)\n",
    "    m = m.astype(float)\n",
    "\n",
    "    tops = m\n",
    "    if cdf:\n",
    "        tops = np.cumsum(m)\n",
    "        total = sum(m)\n",
    "        tops = [float(t) / total for t in tops ]\n",
    "    \n",
    "    return tops, bins\n",
    "\n",
    "seq = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
    "\n",
    "for site in set([v[6:9] for v in set(df_ndt_dist['name'])]):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "    for p, h in enumerate(sorted([h for h in set(df_ndt_dist['name']) if site in h])):\n",
    "        before = None\n",
    "        r_before = None\n",
    "\n",
    "        for day in ['before-2w', 'after-2w']:\n",
    "            ds = df_ndt_dist[ (df_ndt_dist['name'] == h) & (df_ndt_dist['period'] == day) ]\n",
    "            r = ds['download_mbps']\n",
    "            #print h, len(r)\n",
    "            if not len(r):\n",
    "                continue\n",
    "\n",
    "            size = int(math.sqrt(len(r)))\n",
    "            \n",
    "            if day == 'after-2w':\n",
    "                size = before                 # Test before vs after\n",
    "                result = stats.ks_2samp(r, r_before)\n",
    "                #if result.pvalue < 0.01:     # print 'diff', h, result  # Test itself.\n",
    "                a, b = train_test_split(r, test_size=0.5)\n",
    "                result = stats.ks_2samp(a, b)\n",
    "                #if result.pvalue < 0.01:\n",
    "                    #print 'same', h, result\n",
    "                    #print '================================='\n",
    "\n",
    "            else:\n",
    "                before = size\n",
    "                r_before = r\n",
    "            \n",
    "                \n",
    "            #tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=True , cdf=True)\n",
    "            #tops, bins = hist(r, int(math.sqrt(len(r))), log=True , cdf=True)\n",
    "            #print size, h, day\n",
    "            #tops, bins = hist(r, size, log=True , cdf=True)\n",
    "            tops, bins = hist(r, size, log=True , cdf=True)\n",
    "            #tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=False , cdf=True)           \n",
    "            #tops, bins = hist(r, len(r), log=False , cdf=True)            \n",
    "            \n",
    "\n",
    "            #tops_a, bins_a = hist(a, int(1 * math.sqrt(len(a))), log=True, cdf=True)\n",
    "            #tops_b, bins_b = hist(b, int(1 * math.sqrt(len(b))), log=True, cdf=True)\n",
    "            if p > len(seq)-1:\n",
    "                print 'skipping', h\n",
    "                continue\n",
    "            i, j = seq[p]\n",
    "            #print h, len(bins), len(tops)\n",
    "            axes[i, j].plot(bins[:-1], tops, label='cdf-'+h[6:11] + '-' + str(day))\n",
    "            #axes[i, j].plot(bins_a[:-1], tops_a, label=h[6:11] + '-' + str(day)+'-a')\n",
    "            #axes[i, j].plot(bins_b[:-1], tops_b, label=h[6:11] + '-' + str(day)+'-b')\n",
    "            axes[i, j].set_title(h[6:11])\n",
    "            #axes[i, j].set_xlim(-10, 1000)\n",
    "            #axes[i, j].set_xlim(math.log10(.25), math.log10(1000))\n",
    "            axes[i, j].set_xlim(math.log10(.1), math.log10(1000))\n",
    "            axes[i, j].grid(color='#dddddd')\n",
    "            axes[i, j].legend(loc=2, fontsize='x-small')\n",
    "            #axes[i, j].set_ylim(-0.1, 1.1)\n",
    "            axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('NDT Download Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    df_ndt_dist, 'download_mbps', lambda r: int(math.sqrt(len(r))),\n",
    "    fig_by='metro',\n",
    "    axes_by='name', group_by='period', suptitle='NDT Download Distributions',\n",
    "    title='{axis}', axes=(3, 2),\n",
    "    xlim=(math.log10(.1), math.log10(1000)),\n",
    "    cdf=True, xlog=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ndt_variance = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "  (    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps)\n",
    "\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(server_hostname, r'mlab[1-4].([a-z]{3})[0-9]{2}.*') AS metro,\n",
    "  REGEXP_EXTRACT(server_hostname, r'mlab[1-4].([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  remote_ip,\n",
    "  STDDEV(download_mbps) AS download_stddev\n",
    "\n",
    "FROM\n",
    "  mlab_ndt\n",
    "WHERE\n",
    "  remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 5\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 5\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  server_hostname,\n",
    "  period,\n",
    "  remote_ip\n",
    "  --download_mbps\n",
    "\n",
    "HAVING download_stddev is not NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63423\n"
     ]
    }
   ],
   "source": [
    "print len(df_ndt_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: does not preserve binsize across group_by. Each line re-calculates the bin size.\n",
    "plot_hist(\n",
    "    df_ndt_variance, 'download_stddev', lambda r: int(math.sqrt(len(r))),\n",
    "    fig_by='metro',\n",
    "    axes_by='site', group_by='period', suptitle='Distribution of Stddev of NDT Downloads per remote_ip',\n",
    "    title='{axis}', axes=(3, 2),\n",
    "    xlim=(math.log10(.01), math.log10(1000)),\n",
    "    cdf=False, xlog=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
    "\n",
    "skip = 'mlab1.lga02'\n",
    "for site in set([v[6:9] for v in set(df_ndt_variance['hostname'])]):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "    for p, h in enumerate(sorted([h for h in set(df_ndt_variance['hostname']) if site in h])):\n",
    "        before = None\n",
    "        for day in ['before-2w', 'after-2w']:\n",
    "            ds = df_ndt_variance[ (df_ndt_variance['hostname'] == h) & (df_ndt_variance['period'] == day) ]\n",
    "            r = ds['download_stddev']\n",
    "            print h, len(r)\n",
    "            if not len(r) or h == skip:\n",
    "                continue\n",
    "\n",
    "            size = int(math.sqrt(len(r)))\n",
    "            if day == 'after-2w':\n",
    "                size = before\n",
    "            else:\n",
    "                before = size\n",
    "\n",
    "            tops, bins = hist(r, size, log=True, cdf=False)\n",
    "            if p > len(seq)-1:\n",
    "                print 'skipping', h\n",
    "                continue\n",
    "            i, j = seq[p]\n",
    "\n",
    "            axes[i, j].plot(bins[:-1], tops, label='cdf-'+h[6:11] + '-' + str(day))\n",
    "    \n",
    "            axes[i, j].set_title(h[6:11])\n",
    "            #axes[i, j].set_xlim(-10, 1000)\n",
    "            #axes[i, j].set_xlim(math.log10(.25), math.log10(1000))\n",
    "            axes[i, j].set_xlim(math.log10(.01), math.log10(1000))\n",
    "            axes[i, j].grid(color='#dddddd')\n",
    "            axes[i, j].legend(loc=2, fontsize='x-small')\n",
    "            #axes[i, j].set_ylim(-0.1, 1.1)\n",
    "            axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Distribution of Stddev of NDT Downloads per remote_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ndt_all = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r\"([a-z]{3})[0-9]{2}\") as metro,\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r\"([a-z]{3}[0-9]{2})\") as site,\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    log_time,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "\n",
    "  FROM\n",
    "  \n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"(lga|dfw|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    AND anomalies.no_meta is not true\n",
    "    \n",
    "  \n",
    "  GROUP BY\n",
    "    connection_spec.server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  metro,\n",
    "  site,\n",
    "  day,\n",
    " --  AVG(download_mbps) as download_mbps,\n",
    "  APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps,\n",
    "  count(*) as count\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    metro,\n",
    "    site,\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "    -- APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps\n",
    "    MAX(download_mbps) as download_mbps\n",
    "  FROM\n",
    "    mlab_ndt\n",
    "\n",
    "  GROUP BY\n",
    "    metro, site, day, remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  metro, site, day\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfw\n",
      "lga\n",
      "nuq\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_ndt_all, 'day', 'download_mbps', axes_by='metro', group_by='site', axes=(3, 1),\n",
    "    suptitle='Median NDT Download Rates',\n",
    "    ylabel=\"Mbps {axis}\",\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(0, 50),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l],\n",
    "    legend={'loc':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
