{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enables figures loading outside of browser.\n",
    "# If not run, figures will load inline.\n",
    "#%matplotlib\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Depends on: pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')\n",
    "\n",
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def run_query(query, project='measurement-lab'):\n",
    "    client = bigquery.Client(project=project)\n",
    "    job = client.query(query)\n",
    "\n",
    "    results = collections.defaultdict(list)\n",
    "    for row in job.result(timeout=3000):\n",
    "        for key in row.keys():\n",
    "            results[key].append(row.get(key))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def unlog(x, pos):\n",
    "    \"\"\"Formats the x axis for histograms taken on the log of values.\"\"\"\n",
    "    v = math.pow(10, x)\n",
    "    frac, whole = math.modf(v)\n",
    "    if frac > 0:\n",
    "        return '%.1f' % v\n",
    "    else:\n",
    "        return '%d' % whole\n",
    "    \n",
    "    \n",
    "def hist(vals, bin_count, log=True, cdf=False):\n",
    "    \"\"\"Produces hist or cdf values for smooth plots.\"\"\"\n",
    "    if log:\n",
    "        r = [math.log10(x) for x in vals]\n",
    "    else:\n",
    "        r = vals\n",
    "        \n",
    "    m, bins = np.histogram(r, bin_count, normed=True)\n",
    "    m = m.astype(float)\n",
    "\n",
    "    tops = m\n",
    "    if cdf:\n",
    "        tops = np.cumsum(m)\n",
    "        total = sum(m)\n",
    "        tops = [float(t) / total for t in tops]\n",
    "    \n",
    "    return tops, bins\n",
    "\n",
    "\n",
    "logFormatter = matplotlib.ticker.FuncFormatter(unlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_df(\n",
    "    df, xname='', yname='',\n",
    "    cname='', bins=None, cdf=False,\n",
    "    xlog=None, ylog=False,\n",
    "    fig_by='', axes_by='', group_by='',\n",
    "    figsize=(6,8), axes=(1,1),\n",
    "    label='{group}',\n",
    "    xlabel='', ylabel='',\n",
    "    xlim=(), ylim=(),\n",
    "    fx=list, fy=list,\n",
    "    suptitle='',\n",
    "    title='',\n",
    "    legend={},\n",
    "    figmap=None,\n",
    "    fxn=None,\n",
    "    info=False):\n",
    "    \"\"\"Creates a scatter or histogram plot from df, split on mulitple dimension.\n",
    "\n",
    "    plot_df helps plot structured data frames as simple scatter or histograms by\n",
    "    slicing the dataframe along distinct values of some column names. For\n",
    "    example, a df that includes a \"state\" column could be used to create a new\n",
    "    figure for every state with the `fig_by=\"state\"` parameter. Within a single\n",
    "    figure, it's possible to slice the data into multiple axes using another\n",
    "    column, for example one named \"city\" using `axes_by=\"city\"`.\n",
    "\n",
    "    Args:\n",
    "      df: pandas.DataFrame, structured data to plot.\n",
    "      xname: str, name of df column to use as x-axis. Use only with yname.\n",
    "      yname: str, name of df column to use as y-axis. Use only with xname.\n",
    "      cname: str, name of df column to calculate the histogram. Use only with\n",
    "          cdf, and bins.\n",
    "      cdf: bool, whether to plot histogram as a CDF. Default is as a PDF. Use\n",
    "          only with cname and bins.\n",
    "      bins: int or callable, the number of histogram bins. May be a function.\n",
    "      xlog: bool, whether to take the log of histogram. Use only with cname.\n",
    "      ylog: bool, whether to plot the y axis using semilog scale.\n",
    "      fig_by: str, name of column where distinct values split data into multiple\n",
    "          figures.\n",
    "      axes_by: str, name of column where distinct values split data into\n",
    "          multiple axis panels on a single figure.\n",
    "      group_by: str, name of column where distinct values are all plotted on the\n",
    "          same axis.\n",
    "      figsize: (int, int), dimensions of figure. Default (6, 8).\n",
    "      axes: (int, int), arrangement of axes within figure. Default (1, 1).\n",
    "      label: str, the legend format per data series. Used as a format string.  x\n",
    "          Other parameters available are {figure}, {axis}, {size}.\n",
    "          Default {group}.\n",
    "      xlabel: str, the xlabel value. Used as a format string. Other parameters\n",
    "          available are {figure}, {axis}, {size}.\n",
    "      ylabel: str, the ylabel value. Used as a format string like xlabel.\n",
    "      xlim: (xmin, xmax), explicitly set minimum and maximum values of x axis.\n",
    "      ylim: (ymin, ymax), explicitly set minimum and maximum values of y axis.\n",
    "      fx: func, if set, operate on x axis series data before plotting.\n",
    "      fy: func, if set, operate on y axis series data before plotting.\n",
    "      suptitle: str, figure title.\n",
    "      title: str, axis title.\n",
    "      legend: **legend_args,\n",
    "      figmap: the figmap value returned by an earlier cal of plot_df. May be\n",
    "          used to overlay values from multiple data frames. Must use the same\n",
    "          fig_by, axes_by, and group_by values.\n",
    "      fxn: callable that accepts parameters (r, **kwargs). Kwargs will include\n",
    "          figure, axis, group, names and data set size. Only called for\n",
    "          histogram plots.\n",
    "      info: bool, whether to log additional info messages.\n",
    "    Returns:\n",
    "      dict of str to (figures, axes) tuples\n",
    "    \"\"\"\n",
    "    def log_info(f):\n",
    "        if info:\n",
    "            print f\n",
    "\n",
    "    if figmap is None:\n",
    "        log_info('new figmap')\n",
    "        figmap = {}\n",
    "    scatter = None\n",
    "    if (xname and yname):\n",
    "        scatter = True\n",
    "    if cname:\n",
    "        scatter = False\n",
    "    if scatter is None:\n",
    "        raise Exception('Provide xname and yname or cname')\n",
    "\n",
    "    default_names = set(['default'])\n",
    "\n",
    "    figure_names = set(df[fig_by]) if fig_by else default_names\n",
    "    for f in sorted(figure_names):\n",
    "        if f in figmap:\n",
    "            log_info('loading figmap for %s' % f)\n",
    "            fig, ax, ax_index = figmap[f]\n",
    "        else:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            ax = fig.subplots(axes[0], axes[1], squeeze=False)\n",
    "            ax_index = list(itertools.product(range(axes[0]), range(axes[1])))\n",
    "            log_info('saving figmap for %s' % f)\n",
    "            figmap[f] = (fig, ax, ax_index)\n",
    "\n",
    "        df_fig = df if f == 'default' else df[df[fig_by] == f]\n",
    "\n",
    "        axes_names = set(df_fig[axes_by]) if axes_by else default_names\n",
    "        for p, a in enumerate(sorted(axes_names)):\n",
    "            if p >= len(ax_index):\n",
    "                print 'SKIPPING', p, f, a, 'too few axes positions'\n",
    "                continue\n",
    "\n",
    "            if a == 'default':\n",
    "                df_axes = df_fig\n",
    "            else:\n",
    "                df_axes = df_fig[df_fig[axes_by] == a]\n",
    "\n",
    "            i, j = ax_index[p]\n",
    "            group_names = set(df_axes[group_by]) if group_by else default_names\n",
    "            for g in sorted(group_names):\n",
    "                if g == 'default':\n",
    "                    df_g = df_axes\n",
    "                else:\n",
    "                    df_g = df_axes[df_axes[group_by] == g]\n",
    "\n",
    "                if scatter:\n",
    "                    x = fx(df_g[xname])\n",
    "                    y = fy(df_g[yname])\n",
    "                    l = label.format(figure=f, axis=a, group=g)\n",
    "                    ax[i][j].scatter(x, y, s=1, label=l)\n",
    "\n",
    "                else:\n",
    "                    r = df_g[cname]\n",
    "                    if bins is None:\n",
    "                        size = int(math.sqrt(len(r)))\n",
    "                    else:\n",
    "                        size = bins(r)\n",
    "                    if fxn:\n",
    "                        result = fxn(r, figure=f, axis=a, group=g, size=size)\n",
    "                    log_info(\"%s %s %s %s %s\" % (f, a, g, size, len(r)))\n",
    "                    h_tops, h_bins = hist(r, size, log=xlog , cdf=cdf)\n",
    "                    l = label.format(figure=f, axis=a, group=g, size=size, result=result)\n",
    "                    ax[i][j].plot(h_bins[:-1], h_tops, label=l)\n",
    "\n",
    "            if i != len(ax)-1:\n",
    "                ax[i][j].set_xticklabels([])\n",
    "\n",
    "            if title:\n",
    "                ax[i][j].set_title(title.format(figure=f, axis=a, group=g))\n",
    "            if ylabel:\n",
    "                ax[i][j].set_ylabel(ylabel.format(figure=f, axis=a, group=g))\n",
    "            if xlabel:\n",
    "                ax[i][j].set_xlabel(xlabel.format(figure=f, axis=a, group=g))\n",
    "\n",
    "            if xlim:\n",
    "                ax[i][j].set_xlim(xlim)\n",
    "            if ylim:\n",
    "                ax[i][j].set_ylim(ylim)\n",
    "\n",
    "            ax[i][j].grid(color='#dddddd')\n",
    "            ax[i][j].legend(fontsize='x-small', **legend)\n",
    "            if scatter:\n",
    "                ax[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "            if xlog:\n",
    "                ax[i][j].xaxis.set_major_formatter(logFormatter)\n",
    "            if ylog:\n",
    "                ax[i][j].semilogy()\n",
    "\n",
    "        if suptitle:\n",
    "            fig.suptitle(suptitle.format(figure=f))\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    return figmap\n",
    "\n",
    "\n",
    "def plot_scatter(df, xname, yname, **kwargs):\n",
    "    return plot_df(df, xname=xname, yname=yname, **kwargs)\n",
    "\n",
    "\n",
    "def plot_hist(df, cname, bins=None, **kwargs):\n",
    "    return plot_df(df, cname=cname, bins=bins, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Median Uplink Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_disco_pct = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS ts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "        metric = 'switch.octets.uplink.tx'\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  GROUP BY\n",
    "    hostname, metric, ts, value\n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "  UPPER(REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*')) AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(ts, DAY)) AS ts,  \n",
    "  0.8 * APPROX_QUANTILES(value, 101)[ORDINAL(50)] as bytes_50th\n",
    "\n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "\n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "\n",
    "GROUP BY\n",
    "  hostname, ts\n",
    "\n",
    "ORDER BY\n",
    "  hostname, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': (<Figure size 600x800 with 3 Axes>,\n",
       "  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a3310050>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a289f650>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a009b610>]],\n",
       "        dtype=object),\n",
       "  [(0, 0), (1, 0), (2, 0)])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_disco_pct, 'ts', 'bytes_50th',\n",
    "    axes_by='metro', group_by='site',\n",
    "    axes=(3, 1),\n",
    "    suptitle='Daily Median Uplink Utilization',\n",
    "    ylabel=\"Mbps\",\n",
    "    title='{axis}',\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e4, 1e9),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':3, 'ncol':7, 'columnspacing':1},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Packet Loss Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_disco_ratio = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS ts,\n",
    "    sample.value AS value\n",
    "    \n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "    \n",
    "  WHERE\n",
    "        (metric LIKE 'switch.discards.uplink.tx'\n",
    "      OR metric LIKE 'switch.unicast.uplink.tx')\n",
    "    AND REGEXP_CONTAINS(hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    \n",
    "  GROUP BY\n",
    "    hostname, metric, ts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  UPPER(REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3})[0-9]{2}.*')) AS metro,\n",
    "  REGEXP_EXTRACT(hostname, r'mlab1.([a-z]{3}[0-9]{2}).*') AS site,\n",
    "  hostname,\n",
    "  ts,\n",
    "  IF(total > 0, discards / total, 0) as ratio\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    hostname,\n",
    "    UNIX_SECONDS(TIMESTAMP_TRUNC(ts, DAY)) AS ts,\n",
    "    SUM(IF(metric = \"switch.discards.uplink.tx\", value, 0)) AS discards,\n",
    "    SUM(IF(metric = \"switch.unicast.uplink.tx\", value, 0)) AS total\n",
    "  FROM\n",
    "    measurementlab_switch_dedup\n",
    "  WHERE\n",
    "    hostname IS NOT NULL\n",
    "  GROUP BY\n",
    "    hostname, ts\n",
    "  HAVING\n",
    "    discards < total\n",
    "  ORDER BY\n",
    "    hostname, ts\n",
    ")\n",
    "GROUP BY\n",
    "  hostname, ts, ratio\n",
    "HAVING\n",
    "  ratio < 0.01\n",
    "ORDER BY\n",
    "  hostname, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': (<Figure size 600x800 with 3 Axes>,\n",
       "  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f399f7e1710>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a3f9a910>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a0e6b810>]],\n",
       "        dtype=object),\n",
       "  [(0, 0), (1, 0), (2, 0)])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_disco_ratio, 'ts', 'ratio',\n",
    "    axes_by='metro', group_by='site',\n",
    "    axes=(3, 1),\n",
    "    suptitle='Daily Packet Loss Ratio (discards / unicast)',\n",
    "    ylabel=\"Ratio\",\n",
    "    title='{axis}',\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e-6, 1e-1),\n",
    "    fx=lambda l: [pd.to_datetime(t, unit='s') for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median NDT Download Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_ndt_all = run_query(\"\"\"\n",
    "WITH measurementlab_ndt_dedup AS (\n",
    "  SELECT\n",
    "    UPPER(REGEXP_EXTRACT(connection_spec.server_hostname, r\"([a-z]{3})[0-9]{2}\")) as metro,\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r\"([a-z]{3}[0-9]{2})\") as site,\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    log_time,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "\n",
    "  FROM\n",
    "    `measurement-lab.release.ndt_all`\n",
    "\n",
    "  WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"(lga|dfw|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "\n",
    "  GROUP BY\n",
    "    connection_spec.server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  metro,\n",
    "  site,\n",
    "  day,\n",
    "  APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps,\n",
    "  count(*) as count\n",
    "\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    metro,\n",
    "    site,\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "    MAX(download_mbps) as download_mbps\n",
    "\n",
    "  FROM\n",
    "    measurementlab_ndt_dedup\n",
    "\n",
    "  GROUP BY\n",
    "    metro, site, day, remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  metro, site, day\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': (<Figure size 600x800 with 3 Axes>,\n",
       "  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f399fe14610>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a13380d0>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f399900e0d0>]],\n",
       "        dtype=object),\n",
       "  [(0, 0), (1, 0), (2, 0)])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_ndt_all, 'day', 'download_mbps',\n",
    "    axes_by='metro', group_by='site',\n",
    "    axes=(3, 1),\n",
    "    suptitle='Median NDT Download Rates',\n",
    "    ylabel=\"Mbps\",\n",
    "    title='{axis}',\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(0, 50),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l],\n",
    "    legend={'loc':3, 'ncol':7, 'columnspacing':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median NDT Retransmission Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ndt_retrans = run_query(\"\"\"\n",
    "WITH measurementlab_ndt_dedup AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as hostname,\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    log_time,\n",
    "    web100_log_entry.snap.SegsRetrans as SegsRetrans,\n",
    "    web100_log_entry.snap.SegsOut as SegsOut\n",
    "\n",
    "  FROM\n",
    "    `measurement-lab.release.ndt_all`\n",
    "\n",
    "  WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"(lga|dfw|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "  \n",
    "  GROUP BY\n",
    "    connection_spec.server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    SegsRetrans,\n",
    "    SegsOut\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  UPPER(REGEXP_EXTRACT(hostname, r\"([a-z]{3})[0-9]{2}\")) as metro,\n",
    "  REGEXP_EXTRACT(hostname, r\"([a-z]{3}[0-9]{2})\") as site,\n",
    "  day,\n",
    "  APPROX_QUANTILES(ratio, 101)[ORDINAL(50)] AS median_ratio,\n",
    "  count(*) as count\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    hostname,\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "    MAX(SAFE_DIVIDE(SegsRetrans, SegsOut)) as ratio\n",
    "\n",
    "  FROM\n",
    "    measurementlab_ndt_dedup\n",
    "\n",
    "  GROUP BY\n",
    "    hostname,\n",
    "    day,\n",
    "    remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, day\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': (<Figure size 600x800 with 3 Axes>,\n",
       "  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a3838090>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a2ae6590>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a26a2210>]],\n",
       "        dtype=object),\n",
       "  [(0, 0), (1, 0), (2, 0)])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_scatter(\n",
    "    df_ndt_retrans, 'day', 'median_ratio',\n",
    "    axes_by='metro', group_by='site',\n",
    "    axes=(3, 1),\n",
    "    suptitle='Median NDT Retransmission Ratio - (SegsRetran / SegsOut)',\n",
    "    ylabel=\"Ratio\",\n",
    "    title='{axis}',\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    ylim=(1e-6, 1e-1),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l],\n",
    "    legend={'loc':2},\n",
    "    ylog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Distribution of NDT Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_ndt_variance = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "  (    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.dfw\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    "    \n",
    "), measurementlab_ndt_dedup AS (\n",
    "\n",
    "  SELECT\n",
    "    UPPER(REGEXP_EXTRACT(connection_spec.server_hostname, r'mlab[1-4].([a-z]{3})[0-9]{2}.*')) AS metro,\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r'mlab[1-4].([a-z]{3}[0-9]{2}).*') AS site,\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    log_time,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "\n",
    "  FROM\n",
    "    `measurement-lab.release.ndt_all`\n",
    "\n",
    "  WHERE\n",
    "      (TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\"))\n",
    "    AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"(dfw)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"45.56.98.222\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"2600:3c03::f03c:91ff:fe33:819\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.225.75.192\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.192.37.249\"\n",
    "    AND web100_log_entry.connection_spec.remote_ip != \"35.193.254.117\"\n",
    "    AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "\n",
    "  GROUP BY\n",
    "    connection_spec.server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "  metro,\n",
    "  site,\n",
    "  hostname,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  remote_ip,\n",
    "  STDDEV(download_mbps) AS download_stddev,\n",
    "  (STDDEV(download_mbps) / AVG(download_mbps)) AS download_cv,\n",
    "  MAX(download_mbps) AS download_max,\n",
    "  MIN(download_mbps) AS download_min,\n",
    "  AVG(download_mbps) AS download_avg\n",
    "\n",
    "FROM\n",
    "  measurementlab_ndt_dedup\n",
    "  \n",
    "WHERE\n",
    "  remote_ip IN(\n",
    "    SELECT remote_ip\n",
    "    FROM (\n",
    "      SELECT   remote_ip, count(*) as c1\n",
    "      FROM     measurementlab_ndt_dedup\n",
    "      WHERE    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "      GROUP BY remote_ip\n",
    "      HAVING   c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT   remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM     measurementlab_ndt_dedup\n",
    "      WHERE    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-25\")\n",
    "      GROUP BY remote_ip\n",
    "      HAVING   c2 > 10\n",
    "    ) USING (remote_ip)) \n",
    "\n",
    "GROUP BY\n",
    "  metro, site, hostname, period, remote_ip\n",
    "\n",
    "HAVING\n",
    "  download_stddev is not NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'DFW': (<Figure size 900x700 with 6 Axes>,\n",
       "  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f399e35cc10>,\n",
       "          <matplotlib.axes._subplots.AxesSubplot object at 0x7f39a2d81550>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a2f60390>,\n",
       "          <matplotlib.axes._subplots.AxesSubplot object at 0x7f399fa9f390>],\n",
       "         [<matplotlib.axes._subplots.AxesSubplot object at 0x7f39a0ea5390>,\n",
       "          <matplotlib.axes._subplots.AxesSubplot object at 0x7f39a3167390>]],\n",
       "        dtype=object),\n",
       "  [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = {}\n",
    "def ks_compare(r, figure='', axis='', group='', size=''):\n",
    "    values[\"%s-%s-%s\" % (figure, axis, group)] = r\n",
    "    if group == 'before-2w':\n",
    "        after = values[\"%s-%s-%s\" % (figure, axis, 'after-2w')]\n",
    "        result = stats.ks_2samp(r, after)\n",
    "        if result.pvalue < 0.01:\n",
    "            print 'diff', figure, axis, result\n",
    "        return result.pvalue\n",
    "    return 0\n",
    "            \n",
    "plot_hist(\n",
    "    df_ndt_variance, 'download_max', lambda r: int(math.sqrt(len(r))),\n",
    "    fig_by='metro', axes_by='site', group_by='period',\n",
    "    suptitle='Distribution of NDT Downloads - MAX(per remote_ip)',\n",
    "    label='{group} (p: {result:.2f})',\n",
    "    title='{axis}', axes=(3, 2),\n",
    "    xlim=(math.log10(.01), math.log10(1000)),\n",
    "    cdf=False, xlog=True, figsize=(9, 7),\n",
    "    fxn=ks_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = {}\n",
    "def ks_compare(r, figure='', axis='', group='', size=''):\n",
    "    values[\"%s-%s-%s\" % (figure, axis, group)] = r\n",
    "    if group == 'before-2w':\n",
    "        after = values[\"%s-%s-%s\" % (figure, axis, 'after-2w')]\n",
    "        result = stats.ks_2samp(r, after)\n",
    "        if result.pvalue < 0.01:\n",
    "            print 'diff', figure, axis, result\n",
    "        return result.pvalue\n",
    "    return ''\n",
    "\n",
    "plot_hist(\n",
    "    df_ndt_variance, 'download_avg', lambda r: int(math.sqrt(len(r))),\n",
    "    fig_by='metro', axes_by='site', group_by='period',\n",
    "    suptitle='Distribution of NDT Downloads - AVERAGE(per remote_ip)',\n",
    "    label='{group} ({size})',\n",
    "    title='{axis}', axes=(3, 2),\n",
    "    xlim=(math.log10(.01), math.log10(1000)),\n",
    "    cdf=False, xlog=True, figsize=(9, 7),\n",
    "    fxn=ks_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDT Test Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running query dfw01 Fri Aug 31 16:14:21 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def query(site):\n",
    "    print 'running query', site, time.ctime()\n",
    "    return \"\"\"\n",
    "CREATE TEMPORARY FUNCTION\n",
    "  timeBin(ts_usec INT64,\n",
    "    size INT64) AS ( CAST(TRUNC(ts_usec / 1e6 / 10) * 10 AS INT64) );\n",
    "\n",
    "WITH ndt_test_ids_with_discards AS (\n",
    "  SELECT\n",
    "    ndt.test_id as test_id,\n",
    "    SUM(disco.discards) AS discards\n",
    "  FROM (\n",
    "    SELECT\n",
    "      hostname,\n",
    "      UNIX_SECONDS(sample.timestamp) - 10 AS tstart,\n",
    "      UNIX_SECONDS(sample.timestamp) AS tend,\n",
    "      sample.value AS discards\n",
    "    FROM\n",
    "      `measurement-lab.base_tables.switch*`,\n",
    "      UNNEST(sample) AS sample\n",
    "    WHERE\n",
    "      metric = 'switch.discards.uplink.tx'\n",
    "      AND sample.timestamp BETWEEN TIMESTAMP(\"2016-06-01\") AND TIMESTAMP(\"2018-08-01\")\n",
    "      AND hostname = \"mlab1.\"\"\"+site+\"\"\".measurement-lab.org\"\n",
    "    GROUP BY\n",
    "      hostname,\n",
    "      tstart,\n",
    "      tend,\n",
    "      discards\n",
    "    HAVING\n",
    "      discards > 0\n",
    "  ) AS disco\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      REGEXP_EXTRACT(connection_spec.server_hostname, r\"(mlab1.\"\"\"+site+\"\"\".measurement-lab.org)\") as hostname,\n",
    "      timeBin(web100_log_entry.snap.StartTimeStamp, 10) AS tstart,\n",
    "      timeBin(web100_log_entry.snap.StartTimeStamp, 10) + 20 AS tend,\n",
    "      test_id\n",
    "\n",
    "    FROM\n",
    "      `measurement-lab.release.ndt_all`\n",
    "    WHERE\n",
    "          log_time BETWEEN TIMESTAMP(\"2016-06-01\") AND TIMESTAMP(\"2018-08-01\")\n",
    "      AND connection_spec.data_direction = 1\n",
    "      AND (connection_spec.server_hostname = \"mlab1.\"\"\"+site+\"\"\".measurement-lab.org\" OR connection_spec.server_hostname = \"ndt.iupui.mlab1.\"\"\"+site+\"\"\".measurement-lab.org\")\n",
    "      AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "      AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "      AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "\n",
    "    GROUP BY\n",
    "      test_id,\n",
    "      hostname,\n",
    "      tstart,\n",
    "      tend ) AS ndt\n",
    "  ON (disco.hostname = ndt.hostname\n",
    "      AND (disco.tstart = ndt.tstart OR disco.tend = ndt.tend))\n",
    "  GROUP BY\n",
    "    test_id\n",
    "  )\n",
    "\n",
    "\n",
    "SELECT\n",
    "  day, metro, site, hostname, discards, COUNT(*) as count\n",
    "\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "    UPPER(REGEXP_EXTRACT(connection_spec.server_hostname, r'mlab[1-4].([a-z]{3})[0-9]{2}.*')) AS metro,\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r'mlab[1-4].([a-z]{3}[0-9]{2}).*') AS site,\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    CASE\n",
    "      WHEN test_id IN(SELECT test_id from ndt_test_ids_with_discards) THEN 'non-zero'\n",
    "      ELSE 'zero'\n",
    "    END as discards\n",
    "\n",
    "  FROM\n",
    "    `measurement-lab.release.ndt_all`\n",
    "    \n",
    "  WHERE\n",
    "        log_time BETWEEN TIMESTAMP(\"2016-06-01\") AND TIMESTAMP(\"2018-08-01\")\n",
    "    AND connection_spec.data_direction = 1\n",
    "    AND (connection_spec.server_hostname = \"mlab1.\"\"\"+site+\"\"\".measurement-lab.org\" OR connection_spec.server_hostname = \"ndt.iupui.mlab1.\"\"\"+site+\"\"\".measurement-lab.org\")\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  )\n",
    "GROUP BY\n",
    "  day, metro, site, hostname, discards\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_test_counts = pd.concat([\n",
    "    run_query(query(\"dfw01\")),\n",
    "    run_query(query(\"dfw02\")),\n",
    "    run_query(query(\"dfw03\")),\n",
    "    run_query(query(\"dfw04\")),\n",
    "    run_query(query(\"dfw05\")),\n",
    "    run_query(query(\"dfw06\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: does not preserve binsize across group_by. Each line re-calculates the bin size.\n",
    "print df_test_counts.keys()\n",
    "plot_scatter(\n",
    "    df_test_counts, 'day', 'count',\n",
    "    fig_by='metro', axes_by='site', group_by='discards',\n",
    "    suptitle='NDT Test Counts (with or without discards)',\n",
    "    label='{group}',\n",
    "    title='{axis}',\n",
    "    axes=(3, 2), figsize=(12, 10),\n",
    "    ylim=(-200, 30000),\n",
    "    xlim=(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\")),\n",
    "    fx=lambda l: [pd.to_datetime(t) for t in l])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
