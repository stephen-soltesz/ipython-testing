{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# Enables figures loading outside of browser.\n",
    "# If not run, figures will load inline.\n",
    "%matplotlib\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Depends on: pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')\n",
    "\n",
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def run_query(query, project='mlab-sandbox'):\n",
    "    #print query\n",
    "    client = bigquery.Client(project=project)\n",
    "    job = client.query(query)\n",
    "\n",
    "    results = collections.defaultdict(list)\n",
    "    for row in job.result(timeout=300):\n",
    "        for key in row.keys():\n",
    "            results[key].append(row.get(key))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def unlog(x, pos):\n",
    "    \"\"\"Formats the x axis for histograms taken on the log of values.\"\"\"\n",
    "    v = math.pow(10, x)\n",
    "    frac, whole = math.modf(v)\n",
    "    if frac > 0:\n",
    "        return '%.1f' % v\n",
    "    else:\n",
    "        return '%d' % whole\n",
    "\n",
    "logFormatter = matplotlib.ticker.FuncFormatter(unlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLINK UTILIZATION OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_disco_pct = run_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "\n",
    "  name AS hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,  \n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(50)] as bytes_50th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(90)] as bytes_90th\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab1.[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.octets.uplink.tx'\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  name IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,'50th Percentile Uplink Utilization Over time')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw', 'lga', 'nuq'],\n",
    "#    ['sea', 'atl', 'den'],\n",
    "#    ['mia', 'nuq', 'ord'],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        if j != 0:\n",
    "            axes[j].set_yticklabels([])\n",
    "        if i != len(sites)-1:\n",
    "            axes[j].set_xticklabels([])\n",
    "        for h in set(df_disco['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco_pct[ (df_disco_pct['hostname'] == h) & (df_disco_pct['bytes_50th'] > 1e5) ]\n",
    "                axes[j].plot_date(dates.epoch2num(ds['ts']), ds['bytes_50th'], ls='-', ms=0, label=h[6:11])\n",
    "                #axes[i, j].scatter(dates.epoch2num(ds['ts']), ds['bytes_50th'], s=1, label=h[6:11])\n",
    "\n",
    "        axes[j].set_title(site)\n",
    "        axes[j].set_ylim(1e4, 1e9)\n",
    "        axes[j].set_xlim(dates.epoch2num(1464739200), dates.epoch2num(1534204800))\n",
    "        axes[j].tick_params(axis='x', labelrotation=90)\n",
    "        axes[j].grid(color='#dddddd')\n",
    "        axes[j].legend(loc=3, ncol=4, fontsize='x-small', columnspacing=1)\n",
    "        axes[j].semilogy()\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "fig.suptitle('50th Percentile Uplink Utilization Over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DISCARDS OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_disco = run_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  name AS hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(IF(metric = 'switch.discards.uplink.tx', value, 0)) AS total_discards\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.discards.uplink.tx'\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  name IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,'Discards over time')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw', 'lga', 'nuq'],\n",
    "#     ['lga'],\n",
    "#    ['sea', 'atl', 'den'],\n",
    "#    ['mia', 'nuq', 'ord'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None, None, None],\n",
    "]\n",
    "\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "#  axes[0][c] = plt.subplot2grid((2, cols), (0, c))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((1, 3), (i, j))\n",
    "        if j != 0:\n",
    "            axes[i][j].set_yticklabels([])\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "        for h in set(df_disco['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco[ (df_disco['hostname'] == h) & (df_disco['total_discards'] > 100)& (df_disco['total_discards'] < 1000000)]\n",
    "                axes[i][j].plot_date(dates.epoch2num(ds['ts']), ds['total_discards'], ls='-', ms=0, label=h[6:11])\n",
    "\n",
    "        axes[i][j].set_title(site)\n",
    "        axes[i][j].set_ylim(100, 1000000)\n",
    "        axes[i][j].set_xlim(dates.epoch2num(1464739200), dates.epoch2num(1534204800))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-35)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "fig.suptitle('Discards over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Week Performance Distributions -- Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_disco_dist = pd.DataFrame(query.sync_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    CASE\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")          THEN 'before-2w'\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")          THEN 'before-1w'\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")          THEN 'after-1w'\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\")          THEN 'after-2w'\n",
    "        ELSE 'what'\n",
    "    END as period,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND ( -- One week - Sunday to Saturday.\n",
    "          TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-25\")\n",
    "       OR TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-18\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'bps', u'day', u'metric', u'name', u'period', u'sts'], dtype='object')\n",
      "517494\n"
     ]
    }
   ],
   "source": [
    "print df_disco_dist.keys()\n",
    "print len(df_disco_dist)\n",
    "\n",
    "\n",
    "\n",
    "for h in set(df_disco_dist['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist['period'])):\n",
    "        ds = df_disco_dist[ (df_disco_dist['name'] == h) & (df_disco_dist['period'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Day Performance Distributions -- before & after\n",
    "\n",
    "Harder to notice changes wrt before  after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_disco_dist = pd.DataFrame(query.sync_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND (\n",
    "        TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-18\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-25\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-11\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-05\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-10\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df_disco_dist.keys()\n",
    "print len(df_disco_dist)\n",
    "\n",
    "\n",
    "\n",
    "for h in set(df_disco_dist['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist['day'])):\n",
    "        ds = df_disco_dist[ (df_disco_dist['name'] == h) & (df_disco_dist['day'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NDT test distributions - Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "-- ALL NDT tests before and after the flow-control configuration change.\n",
    "   \n",
    "SELECT\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    CASE\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")\n",
    "       THEN 'before-2w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")\n",
    "       THEN 'before-1w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")\n",
    "       THEN 'after-1w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\")\n",
    "       THEN 'after-2w'\n",
    "     ELSE 'what'\n",
    "    END as period,\n",
    "\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,\n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /\n",
    "      (web100_log_entry.snap.SndLimTimeRwin +\n",
    "       web100_log_entry.snap.SndLimTimeCwnd +\n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as download_mbps   \n",
    "FROM\n",
    "   `measurement-lab.base_tables.ndt*`\n",
    "   \n",
    "WHERE       \n",
    "    ( -- One week - Sunday to Saturday.\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "    \n",
    "GROUP BY\n",
    "  name, period, ts, download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "  -- Weekly cohort - NDT tests before and after the flow-control configuration change.\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-18\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-1w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-11\") THEN 'after-1w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)) AS download_mbps\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  ( -- One week - Sunday to Saturday.\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-18\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-11\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN( (\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\")\n",
    "        AND TIMESTAMP(\"2018-02-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\")\n",
    "        AND TIMESTAMP(\"2018-02-25\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\")\n",
    "        AND TIMESTAMP(\"2018-03-11\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\")\n",
    "        AND TIMESTAMP(\"2018-03-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)) )\n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "  -- Two-week cohorts - NDT tests before and after the flow-control configuration change.\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  --web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "  (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  --MAX(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  --APPROX_QUANTILES(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)), 101)[ORDINAL(50)] as download_mbps\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  ( -- One week - Sunday to Saturday.\n",
    "       TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  --remote_ip\n",
    "  download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'download_mbps', u'name', u'period'], dtype='object')\n",
      "1245080\n",
      "mlab1.dfw01 29573\n",
      "171 mlab1.dfw01 before-2w\n",
      "mlab1.dfw01 40542\n",
      "171 mlab1.dfw01 after-2w\n",
      "mlab1.dfw02 35691\n",
      "188 mlab1.dfw02 before-2w\n",
      "mlab1.dfw02 50937\n",
      "188 mlab1.dfw02 after-2w\n",
      "mlab1.dfw03 36493\n",
      "191 mlab1.dfw03 before-2w\n",
      "mlab1.dfw03 51686\n",
      "191 mlab1.dfw03 after-2w\n",
      "mlab1.dfw04 34892\n",
      "186 mlab1.dfw04 before-2w\n",
      "mlab1.dfw04 51277\n",
      "186 mlab1.dfw04 after-2w\n",
      "mlab1.dfw05 42975\n",
      "207 mlab1.dfw05 before-2w\n",
      "mlab1.dfw05 58017\n",
      "207 mlab1.dfw05 after-2w\n",
      "mlab1.dfw06 35763\n",
      "189 mlab1.dfw06 before-2w\n",
      "mlab1.dfw06 52020\n",
      "189 mlab1.dfw06 after-2w\n",
      "mlab1.lga02 1198\n",
      "34 mlab1.lga02 before-2w\n",
      "mlab1.lga02 66686\n",
      "34 mlab1.lga02 after-2w\n",
      "mlab1.lga03 44086\n",
      "209 mlab1.lga03 before-2w\n",
      "mlab1.lga03 55160\n",
      "209 mlab1.lga03 after-2w\n",
      "mlab1.lga04 43125\n",
      "207 mlab1.lga04 before-2w\n",
      "mlab1.lga04 53630\n",
      "207 mlab1.lga04 after-2w\n",
      "mlab1.lga05 51216\n",
      "226 mlab1.lga05 before-2w\n",
      "mlab1.lga05 62131\n",
      "226 mlab1.lga05 after-2w\n",
      "mlab1.lga06 44109\n",
      "210 mlab1.lga06 before-2w\n",
      "mlab1.lga06 55263\n",
      "210 mlab1.lga06 after-2w\n",
      "mlab1.lga07 35647\n",
      "188 mlab1.lga07 before-2w\n",
      "mlab1.lga07 47340\n",
      "188 mlab1.lga07 after-2w\n",
      "mlab1.nuq02 17504\n",
      "132 mlab1.nuq02 before-2w\n",
      "mlab1.nuq02 18800\n",
      "132 mlab1.nuq02 after-2w\n",
      "mlab1.nuq03 11755\n",
      "108 mlab1.nuq03 before-2w\n",
      "mlab1.nuq03 18552\n",
      "108 mlab1.nuq03 after-2w\n",
      "mlab1.nuq04 12805\n",
      "113 mlab1.nuq04 before-2w\n",
      "mlab1.nuq04 13861\n",
      "113 mlab1.nuq04 after-2w\n",
      "mlab1.nuq05 17707\n",
      "133 mlab1.nuq05 before-2w\n",
      "mlab1.nuq05 18710\n",
      "133 mlab1.nuq05 after-2w\n",
      "mlab1.nuq06 17486\n",
      "132 mlab1.nuq06 before-2w\n",
      "mlab1.nuq06 18443\n",
      "132 mlab1.nuq06 after-2w\n"
     ]
    }
   ],
   "source": [
    "print df_ndt_dist.keys()\n",
    "print len(df_ndt_dist)\n",
    "\n",
    "def hist(vals, bin_count, log=True, cdf=False):\n",
    "    \"\"\"Produces hist or cdf values for smooth plots.\"\"\"\n",
    "    if log:\n",
    "        r = [math.log10(x) for x in vals]\n",
    "    else:\n",
    "        r = vals\n",
    "        \n",
    "    m, bins = np.histogram(r, bin_count, normed=True)\n",
    "    m = m.astype(float)\n",
    "\n",
    "    tops = m\n",
    "    if cdf:\n",
    "        tops = np.cumsum(m)\n",
    "        total = sum(m)\n",
    "        tops = [float(t) / total for t in tops ]\n",
    "    \n",
    "    return tops, bins\n",
    "\n",
    "seq = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
    "\n",
    "for site in set([v[6:9] for v in set(df_ndt_dist['name'])]):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "    for p, h in enumerate(sorted([h for h in set(df_ndt_dist['name']) if site in h])):\n",
    "        before = None\n",
    "        for day in ['before-2w', 'after-2w']:\n",
    "            ds = df_ndt_dist[ (df_ndt_dist['name'] == h) & (df_ndt_dist['period'] == day) ]\n",
    "            r = ds['download_mbps']\n",
    "            print h, len(r)\n",
    "            if not len(r):\n",
    "                continue\n",
    "\n",
    "            size = int(math.sqrt(len(r)))\n",
    "            if day == 'after-2w':\n",
    "                size = before\n",
    "            else:\n",
    "                before = size\n",
    "            \n",
    "                \n",
    "#            tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=True , cdf=True)\n",
    "            #tops, bins = hist(r, int(math.sqrt(len(r))), log=True , cdf=True)\n",
    "            print size, h, day\n",
    "            tops, bins = hist(r, size, log=True , cdf=False)\n",
    "#            tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=False , cdf=True)           \n",
    "#            tops, bins = hist(r, len(r), log=False , cdf=True)            \n",
    "            \n",
    "            #a, b = train_test_split(r, test_size=0.5)\n",
    "            #z = a\n",
    "            #print 'same', stats.ks_2samp(a,b)\n",
    "\n",
    "\n",
    "            #tops_a, bins_a = hist(a, int(1 * math.sqrt(len(a))), log=True, cdf=True)\n",
    "            #tops_b, bins_b = hist(b, int(1 * math.sqrt(len(b))), log=True, cdf=True)\n",
    "            if p > len(seq)-1:\n",
    "                print 'skipping', h\n",
    "                continue\n",
    "            i, j = seq[p]\n",
    "#            print h, len(bins), len(tops)\n",
    "            axes[i, j].plot(bins[:-1], tops, label='cdf-'+h[6:11] + '-' + str(day))\n",
    "            #axes[i, j].plot(bins_a[:-1], tops_a, label=h[6:11] + '-' + str(day)+'-a')\n",
    "            #axes[i, j].plot(bins_b[:-1], tops_b, label=h[6:11] + '-' + str(day)+'-b')\n",
    "            axes[i, j].set_title(h[6:11])\n",
    "            #axes[i, j].set_xlim(-10, 1000)\n",
    "            axes[i, j].set_xlim(math.log10(.25), math.log10(1000))\n",
    "            axes[i, j].grid(color='#dddddd')\n",
    "            axes[i, j].legend(loc=4, fontsize='x-small')\n",
    "            axes[i, j].set_ylim(-0.1, 1.1)\n",
    "            axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('NDT Download Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<matplotlib.lines.Line2D object at 0x7fee75327390>]\n"
     ]
    }
   ],
   "source": [
    "len(n), len(bins[:-1])\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "\n",
    "axes.plot(bins[:-1], n)\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "print f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   9  15  19  30  27  23  32  26  22  21  30  33  32  25  38  51\n",
      "  46  33  47  42  53  45  48  63  52  65  81  64  90  55  49  78  88  95\n",
      " 110 101  82  91 115 115 137 111 113 107 121 118 175 162 175 211 224 228\n",
      " 310 434 183 166 165 102  80 109 108 128 137 133 116 113  57  50  21  26\n",
      "  18  21  19  10  27  22  20   7   8] [-0.24250399 -0.2029988  -0.16349361 -0.12398841 -0.08448322 -0.04497803\n",
      " -0.00547284  0.03403235  0.07353754  0.11304274  0.15254793  0.19205312\n",
      "  0.23155831  0.2710635   0.3105687   0.35007389  0.38957908  0.42908427\n",
      "  0.46858946  0.50809465  0.54759985  0.58710504  0.62661023  0.66611542\n",
      "  0.70562061  0.74512581  0.784631    0.82413619  0.86364138  0.90314657\n",
      "  0.94265176  0.98215696  1.02166215  1.06116734  1.10067253  1.14017772\n",
      "  1.17968292  1.21918811  1.2586933   1.29819849  1.33770368  1.37720887\n",
      "  1.41671407  1.45621926  1.49572445  1.53522964  1.57473483  1.61424003\n",
      "  1.65374522  1.69325041  1.7327556   1.77226079  1.81176598  1.85127118\n",
      "  1.89077637  1.93028156  1.96978675  2.00929194  2.04879714  2.08830233\n",
      "  2.12780752  2.16731271  2.2068179   2.24632309  2.28582829  2.32533348\n",
      "  2.36483867  2.40434386  2.44384905  2.48335425  2.52285944  2.56236463\n",
      "  2.60186982  2.64137501  2.6808802   2.7203854   2.75989059  2.79939578\n",
      "  2.83890097  2.87840616  2.91791136  2.95741655] 81 82\n"
     ]
    }
   ],
   "source": [
    "m, bins = np.histogram(r, int(math.sqrt(len(ds['download_mbps']))))\n",
    "print m, bins, len(m), len(bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
