{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "# Enables figures to load outside of browser.\n",
    "%matplotlib\n",
    "\n",
    "# Enables figures to load inline in the browser.\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def run_query(query, project='mlab-sandbox'):\n",
    "    client = bigquery.Client(project=project)\n",
    "    job = client.query(query)\n",
    "\n",
    "    results = collections.defaultdict(list)\n",
    "    for row in job.result(timeout=300):\n",
    "        for key in row.keys():\n",
    "            results[key].append(row.get(key))\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unlog(x, pos):\n",
    "    v = math.pow(10, x)\n",
    "    frac, whole = math.modf(v)\n",
    "    if frac > 0:\n",
    "        return '%.1f' % v\n",
    "    else:\n",
    "        return '%d' % whole\n",
    "\n",
    "logFormatter = matplotlib.ticker.FuncFormatter(unlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_disco = run_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  name AS hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(IF(metric = 'switch.octets.uplink.tx' AND (value * 8 / 10000000) >= 500, 1, 0)) as saturated_events_500,\n",
    "  SUM(IF(metric = 'switch.octets.uplink.tx' AND (value * 8 / 10000000) >= 800, 1, 0)) as saturated_events_800,\n",
    "  SUM(IF(metric = 'switch.octets.uplink.tx' AND (value * 8 / 10000000) >= 800, 1, 0)) as saturated_events_900\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `mlab-sandbox.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.octets.uplink.tx'\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  name IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90567"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_disco[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites = [\n",
    "    ['dfw', 'lga', 'iad'],\n",
    "    ['lax', 'atl', 'den'],\n",
    "    ['sea', 'nuq', 'ord'], # MIA is low utilization.\n",
    "]\n",
    "\n",
    "#sites = [\n",
    "#    ['dfw', 'lga', 'iad'],\n",
    "#    ['lax', 'atl',  'nuq'], #  'ord', # MIA is low utilization. 'den', 'sea' low enough.\n",
    "#]\n",
    "\n",
    "cols = len(sites[0])\n",
    "fig = plt.figure(figsize=(4 * cols, 4 * cols))\n",
    "axes = [\n",
    "    [None] * cols,\n",
    "    [None] * cols,\n",
    "    [None] * cols,\n",
    "]\n",
    "\n",
    "for r, siter in enumerate(sites):\n",
    "    for c, site in enumerate(siter):\n",
    "        #for x, rate in enumerate(['98th']):\n",
    "            axes[r][c] = plt.subplot2grid((3, cols), (r, c))\n",
    "            if c != 0:\n",
    "                #axes[r][c].set_yticklabels([])\n",
    "                pass\n",
    "            else:\n",
    "                axes[r][c].set_ylabel('% Saturated Timebins')\n",
    "\n",
    "            if r != 2:\n",
    "                axes[r][c].set_xticklabels([])\n",
    "\n",
    "            prefix = 'mlab1.' + site\n",
    "            ds_sites = df_disco[ df_disco['hostname'].str.contains(prefix) ]\n",
    "            for h in sorted(set(ds_sites[ ds_sites['hostname'].str.contains(prefix) ]['hostname'])):\n",
    "                ds = ds_sites[ (ds_sites['hostname'].str.contains(h)) ]\n",
    "                axes[r][c].plot_date(\n",
    "                    dates.epoch2num(ds['ts']),\n",
    "                    100 * ds['saturated_events_500'] / 8640, ls='-', ms=0, label=h[6:11])\n",
    "\n",
    "            axes[r][c].set_title(site)\n",
    "            #axes[r][c].set_ylim(0, 1.5)\n",
    "            axes[r][c].tick_params(axis='x', labelrotation=90)\n",
    "            axes[r][c].grid(color='#dddddd')\n",
    "            axes[r][c].legend(loc=2, fontsize='x-small', ncol=2)\n",
    "\n",
    "fig.suptitle('Daily Saturation Events in US Sites @ 500 Mbps')\n",
    "#fig.tight_layout()\n",
    "#fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discards over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Daily Rate over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90th Percentile Over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SS COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DISCO RATES 90th PERCENTILE\n",
    "\n",
    "df_disco_max = run_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "  name AS hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  \n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(50)] as bytes_50th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(90)] as bytes_90th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(98)] as bytes_98th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(99)] as bytes_99th,\n",
    "  MAX(value) as bytes_max\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `mlab-sandbox.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.octets.uplink.tx'\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  name IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_ss_count = run_query(\n",
    "    \"\"\"#standardSQL                                                                    \n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "\n",
    "SELECT\n",
    "   hostname, ts, count(*) as count\n",
    "FROM (\n",
    "    SELECT\n",
    "        REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "        UNIX_SECONDS(TIMESTAMP_TRUNC(log_time, DAY)) AS ts                            \n",
    "    FROM\n",
    "        -- `mlab-sandbox.batch.sidestream*`                                              \n",
    "         `mlab-sandbox.gfr.sidestream_*`\n",
    "    WHERE\n",
    "      REGEXP_CONTAINS(test_id, r\"mlab1.(dfw|lga|iad|lax|atl|nuq)[0-9]{2}.*\")     \n",
    "      AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "      AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "      AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "        web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "        web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "      AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "        web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "        web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "      AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "        (web100_log_entry.snap.State >= 5 AND                                       \n",
    "        web100_log_entry.snap.State <= 11))\n",
    "\n",
    "    GROUP BY\n",
    "      hostname, ts, web100_log_entry.connection_spec.remote_ip, web100_log_entry.connection_spec.remote_port, web100_log_entry.connection_spec.local_port, web100_log_entry.connection_spec.local_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, ts\n",
    "ORDER BY\n",
    "  hostname, ts\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MIA, DEN, and SEA are relatively low utilization.\n",
    "# NUQ, ORD show trends less dramatic than those below.\n",
    "# LGA usage appeared to dramatically lower around 2018-01.\n",
    "\n",
    "# Highest utilization sites.\n",
    "sites = [\n",
    "    'dfw', 'iad', 'lax', 'atl'\n",
    "]\n",
    "\n",
    "cols = len(sites)\n",
    "fig = plt.figure(figsize=(4 * cols, 6))\n",
    "axes = [\n",
    "    [None] * cols,\n",
    "    [None] * cols,\n",
    "]\n",
    "\n",
    "for c, site in enumerate(sites):\n",
    "    axes[0][c] = plt.subplot2grid((2, cols), (0, c))\n",
    "    axes[1][c] = plt.subplot2grid((2, cols), (1, c))\n",
    "    prefix = 'mlab1.' + site\n",
    "\n",
    "    r = 0\n",
    "    if c > 0:\n",
    "        # Hide ylabels after the first column.\n",
    "        axes[r][c].set_yticklabels([])\n",
    "    else:\n",
    "        axes[r][c].set_ylabel('Mbps')\n",
    "\n",
    "    # Extract all hostnames that contain the \"mlab1.<site>\" prefix.\n",
    "    ds_sites = df_disco_max[ df_disco_max['hostname'].str.contains(prefix) ]\n",
    "    for host in sorted(set(ds_sites['hostname'])):\n",
    "        # Plot each host on the current axis.\n",
    "        ds = ds_sites[ (ds_sites['hostname'].str.contains(host)) ]\n",
    "        axes[r][c].plot_date(\n",
    "            dates.epoch2num(ds['ts']),\n",
    "            ds['bytes_90th'] * 8 / 10000000,\n",
    "            ls='-', ms=0, label=host[6:11] + '-90th')\n",
    "\n",
    "    axes[r][c].set_title(site)\n",
    "    axes[r][c].set_ylim(100, 1000)\n",
    "    axes[r][c].set_xticklabels([])\n",
    "    axes[r][c].tick_params(axis='x', labelrotation=90)\n",
    "    axes[r][c].grid(color='#dddddd')\n",
    "    axes[r][c].legend(loc=2, fontsize='x-small', ncol=2)\n",
    "    \n",
    "    r = 1\n",
    "    if c > 0:\n",
    "        axes[r][c].set_yticklabels([])\n",
    "    else:\n",
    "        axes[r][c].set_ylabel('Connection Counts')\n",
    "\n",
    "    ds_sites = df_ss_count[ df_ss_count['hostname'].str.contains(prefix) ]\n",
    "    for host in sorted(set(ds_sites['hostname'])):\n",
    "        ds = ds_sites[ (ds_sites['hostname'].str.contains(host)) ]\n",
    "        axes[r][c].plot_date(\n",
    "            dates.epoch2num(ds['ts']),\n",
    "            ds['count'],\n",
    "            ls='-', ms=0, label=host[6:11])\n",
    "\n",
    "    axes[r][c].set_ylim(0, 25000)\n",
    "    axes[r][c].tick_params(axis='x', labelrotation=90)\n",
    "    axes[r][c].grid(color='#dddddd')\n",
    "    axes[r][c].legend(loc=2, fontsize='x-small', ncol=2)            \n",
    "\n",
    "fig.suptitle('Daily 98th Percentile Switch Traffic & TCP Connection Counts Per Metro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent of Timebins with Discards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Packets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Packet Discard Ratios (Switch Loss Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Flow-Control Trial (measurement-lab.public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ss_trial = run_query(\"\"\"\n",
    "#standardSQL                                                                    \n",
    "    -- Only works for mlab1 addresses. May not work on all machines.\n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "    CASE \n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 1 THEN 'ndt'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'samknows'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 9 THEN 'neubot'\n",
    "        ELSE 'other' \n",
    "    END AS slice,\n",
    "    CASE\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-01-26 00:00:00\", \"2018-01-27 00:00:00\") THEN '5w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-02 00:00:00\", \"2018-02-03 00:00:00\") THEN '4w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-09 00:00:00\", \"2018-02-10 00:00:00\") THEN '3w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\") THEN '2w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\") THEN '1w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\") THEN '0w'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-09 00:00:00\", \"2018-03-10 00:00:00\") THEN '+1w'\n",
    "    ELSE 'unknown'                                                                    \n",
    "    END AS period,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,                                   \n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /                                \n",
    "      (web100_log_entry.snap.SndLimTimeRwin +                                     \n",
    "       web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as rate_mbps   \n",
    "FROM\n",
    "   -- `measurement-lab.public.sidestream`\n",
    "   -- `mlab-sandbox.batch.sidestream*`\n",
    "    `mlab-sandbox.gfr.sidestream_*`\n",
    "WHERE\n",
    "        (  betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-01-26 00:00:00\", \"2018-01-27 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-02 00:00:00\", \"2018-02-03 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-09 00:00:00\", \"2018-02-10 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\")\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-09 00:00:00\", \"2018-03-10 00:00:00\")\n",
    "        )\n",
    "  AND REGEXP_CONTAINS(test_id, r\"mlab1.(dfw\\d\\d)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "  AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "    (web100_log_entry.snap.State >= 5 AND                                       \n",
    "    web100_log_entry.snap.State <= 11))\n",
    "    \n",
    "GROUP BY\n",
    "  hostname, slice, period, ts, rate_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "df_ss_trial_pct = run_query(\"\"\"\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "CASE\n",
    "WHEN betweenTimes(StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\") THEN CONCAT(sitename, '-2w')\n",
    "WHEN betweenTimes(StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\") THEN CONCAT(sitename, '-1w')\n",
    "WHEN betweenTimes(StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\") THEN CONCAT(sitename, '-0w (flow)')\n",
    "ELSE 'unknown'\n",
    "END AS test_period,\n",
    "\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(10)], 2) as q10,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(12)], 2) as q12,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(15)], 2) as q15,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(18)], 2) as q18,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(20)], 2) as q20,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(22)], 2) as q22,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(25)], 2) as q25,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(28)], 2) as q28,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(30)], 2) as q30,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(32)], 2) as q32,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(35)], 2) as q35,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(38)], 2) as q38,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(40)], 2) as q40,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(42)], 2) as q42,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(45)], 2) as q45,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(48)], 2) as q48,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(50)], 2) as q50,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(52)], 2) as q52,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(55)], 2) as q55,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(58)], 2) as q58,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(60)], 2) as q60,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(62)], 2) as q62,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(65)], 2) as q65,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(68)], 2) as q68,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(70)], 2) as q70,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(72)], 2) as q72,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(75)], 2) as q75,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(78)], 2) as q78,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(80)], 2) as q80,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(82)], 2) as q82,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(85)], 2) as q85,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(88)], 2) as q88,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(90)], 2) as q90,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(92)], 2) as q92,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(95)], 2) as q95,\n",
    "round(APPROX_QUANTILES(rate_mbps, 101) [ORDINAL(98)], 2) as q98\n",
    "\n",
    "FROM\n",
    "(\n",
    "SELECT\n",
    "    UNIX_SECONDS(TIMESTAMP_TRUNC(log_time, DAY)) as StartTimeStamp,\n",
    "    --  web100_log_entry.snap.StartTimeStamp as StartTimeStamp,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/[0-9]+/mlab1.(dfw02|lga03)/.*\") AS sitename,\n",
    "    8 * (\n",
    "        web100_log_entry.snap.HCThruOctetsAcked / (\n",
    "        web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)\n",
    "    ) AS rate_mbps\n",
    "FROM\n",
    "    -- `mlab-sandbox.batch.sidestream*`\n",
    "     `mlab-sandbox.gfr.sidestream_*`\n",
    "WHERE\n",
    "\n",
    "    REGEXP_CONTAINS(test_id, r\"\\d\\d\\d\\d/\\d\\d/[0-9]+/mlab1.(dfw02|lga03)/.*\")\n",
    "    AND (\n",
    "             betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-16 00:00:00\", \"2018-02-17 00:00:00\")\n",
    "          OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-02-23 00:00:00\", \"2018-02-24 00:00:00\")\n",
    "          OR betweenTimes(web100_log_entry.snap.StartTimeStamp, \"2018-03-02 00:00:00\", \"2018-03-03 00:00:00\"))\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 819200\n",
    "    AND ( web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND ( web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND ( web100_log_entry.snap.State = 1 OR ( web100_log_entry.snap.State >= 5 AND web100_log_entry.snap.State <= 11))\n",
    ")\n",
    "GROUP BY\n",
    "    sitename, test_period\n",
    "ORDER BY\n",
    "    sitename, test_period\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transpose the long list of quantiles.\n",
    "\n",
    "# Save test_period names, so we can name the quantile values after transpose.\n",
    "test_periods = df_ss_trial_pct['test_period']\n",
    "\n",
    "n = df_ss_trial_pct.drop(['test_period'], axis=1)\n",
    "t = n.transpose()\n",
    "t.columns = test_periods\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(11,6))\n",
    "\n",
    "# Reformat the percentile column names as integer numbers.\n",
    "percentiles = [int(v[1:]) for v in list(sorted(n.keys()))]\n",
    "for period in test_periods:\n",
    "    axes.plot(percentiles, t[period], label=period)\n",
    "\n",
    "axes.legend(loc=2)\n",
    "axes.set_ylabel('Mbps')\n",
    "axes.set_xlabel('Percentiles')\n",
    "axes.grid(color='#dddddd')\n",
    "fig.suptitle('Sidestream comparing Flow-control trial to earlier periods')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Historical (mlab-sandbox.batch) - Sidestream by Period & Slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Variations, for each period:\n",
    "\n",
    "* all sidestream connections from each period.\n",
    "* all sidestream connections from each period and slice\n",
    "* all sidestream connections from each period and slice and from same cohort.\n",
    "* some sidestream connections from each period and slice and from same cohort, grouped by ts & remote_ip.\n",
    "* some sidestream connections from each period and slice and from same cohort, grouped by only by remote_ip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hosts = [\n",
    "     'mlab1.dfw02', 'mlab1.dfw05',\n",
    "     'mlab1.iad01', 'mlab1.iad02', 'mlab1.iad03', 'mlab1.iad04', 'mlab1.iad05',\n",
    "     'mlab1.lax02', 'mlab1.lax03', 'mlab1.lax04', 'mlab1.lax05',\n",
    "     'mlab1.lga02', 'mlab1.lga03', 'mlab1.lga04', 'mlab1.lga05', 'mlab1.lga06',\n",
    "     'mlab1.atl02', 'mlab1.atl03', 'mlab1.atl04', 'mlab1.atl05',\n",
    "]\n",
    "\n",
    "hosts = [\n",
    "    'mlab1.dfw02'\n",
    "]\n",
    "\n",
    "#   (datetime.datetime(2017,  8, 23), datetime.datetime(2017,   8, 28)),\n",
    "#   (datetime.datetime(2017,  8, 28), datetime.datetime(2017,  10, 14)),\n",
    "periods_list = [\n",
    "    (datetime.datetime(2017, 10, 14), datetime.datetime(2017,  12, 7)),\n",
    "    (datetime.datetime(2017, 12,  7), datetime.datetime(2018,  1,  12)),\n",
    "    (datetime.datetime(2018,  1, 12), datetime.datetime(2018,  1,  21)),\n",
    "    (datetime.datetime(2018,  1, 21), datetime.datetime(2018,  2,  1)),\n",
    "    (datetime.datetime(2018,  2, 1),  datetime.datetime(2018,  3,  1)),\n",
    "    (datetime.datetime(2018,  3, 1),  datetime.datetime(2018,  3,  10)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 0 (0, '2017-10-14 00:00:00', '2017-12-07 00:00:00') mlab1.dfw02 128\n",
      "saved 1 (1, '2017-12-07 00:00:00', '2018-01-12 00:00:00') mlab1.dfw02 192\n",
      "saved 2 (2, '2018-01-12 00:00:00', '2018-01-21 00:00:00') mlab1.dfw02 383\n",
      "saved 3 (3, '2018-01-21 00:00:00', '2018-02-01 00:00:00') mlab1.dfw02 277\n",
      "saved 4 (4, '2018-02-01 00:00:00', '2018-03-01 00:00:00') mlab1.dfw02 240\n",
      "saved 5 (5, '2018-03-01 00:00:00', '2018-03-10 00:00:00') mlab1.dfw02 359\n"
     ]
    }
   ],
   "source": [
    "# STREAMS WITH MATCHING COHORTS\n",
    "def start_and_end(d):\n",
    "    s = d.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    e = (d + datetime.timedelta(days=4)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return s, e\n",
    "\n",
    "df_hosts = collections.defaultdict(collections.defaultdict)\n",
    "for i, periods in enumerate(periods_list):\n",
    "    a_s, a_e = start_and_end(periods[0])\n",
    "    b_s, b_e = start_and_end(periods[1])\n",
    "    for host in hosts:\n",
    "        result = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "-- Only works for mlab1 addresses. May not work on all machines.\n",
    "CREATE TEMPORARY FUNCTION sliceFromIP(ipaddr STRING)\n",
    "    AS ( MOD(CAST(REGEXP_EXTRACT(ipaddr, r'[:.]([0-9]+)$') AS INT64), 64) - 10 );\n",
    "\n",
    "CREATE TEMPORARY FUNCTION betweenTimes(ts INT64, starttime STRING, endtime STRING)\n",
    "    AS ( TIMESTAMP_SECONDS(ts) >= TIMESTAMP(starttime) AND TIMESTAMP_SECONDS(ts) <= TIMESTAMP(endtime) );\n",
    "\n",
    "SELECT\n",
    "    slice,\n",
    "    period,\n",
    "    hostname,\n",
    "    remote_ip,\n",
    "    AVG(sum_rate_mbps) as sum_rate_mbps\n",
    "\n",
    "FROM (\n",
    "\n",
    "SELECT\n",
    "   slice,\n",
    "   period,\n",
    "   hostname,\n",
    "   remote_ip,\n",
    "   --AVG(rate_mbps) as rate_mbps,\n",
    "   --APPROX_QUANTILES(rate_mbps, 101)[ORDINAL(50)] as med_rate_mbps,\n",
    "   --MAX(rate_mbps) as max_rate_mbps,\n",
    "   SUM(rate_mbps) as sum_rate_mbps\n",
    "    \n",
    "FROM (\n",
    "\n",
    "SELECT\n",
    "    web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "    CASE \n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 1 THEN 'ndt'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7 THEN 'samknows'\n",
    "        WHEN sliceFromIP(web100_log_entry.connection_spec.local_ip) = 9 THEN 'neubot'\n",
    "        ELSE 'other' \n",
    "    END AS slice,\n",
    "    CASE                                                                          \n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "            THEN '\"\"\"+a_s+\"\"\"'\n",
    "        WHEN betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "            THEN '\"\"\"+b_s+\"\"\"'\n",
    "    ELSE 'bad'                                                                    \n",
    "    END AS period,\n",
    "    REGEXP_EXTRACT(test_id, r\"\\d\\d\\d\\d/\\d\\d/\\d\\d/(mlab[1-4].[a-z]{3}[0-9]{2})\") AS hostname,\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,                                   \n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /                                \n",
    "      (web100_log_entry.snap.SndLimTimeRwin +                                     \n",
    "       web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as rate_mbps   \n",
    "FROM\n",
    "    `mlab-sandbox.batch.sidestream*`                                              \n",
    "WHERE\n",
    "        (  betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "        OR betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "        )\n",
    "  AND (test_id LIKE '%\"\"\"+host+\"\"\"%')            \n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000 -- 819200                          \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "  AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "    (web100_log_entry.snap.State >= 5 AND                                       \n",
    "    web100_log_entry.snap.State <= 11))\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN(\n",
    "    (SELECT\n",
    "     remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "         web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "         count(*) as c1\n",
    "      FROM\n",
    "         `mlab-sandbox.batch.sidestream*`\n",
    "      WHERE\n",
    "          (test_id LIKE '%\"\"\"+host+\"\"\"%')\n",
    "        AND betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+a_s+\"\"\"', '\"\"\"+a_e+\"\"\"')\n",
    "        AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "        AND web100_log_entry.snap.HCThruOctetsAcked >= 819200                          \n",
    "        AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "            web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "            web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "        AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "            web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "            web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "        AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "            (web100_log_entry.snap.State >= 5 AND                                       \n",
    "            web100_log_entry.snap.State <= 11))\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "         web100_log_entry.connection_spec.remote_ip as remote_ip,\n",
    "         count(*) as c2\n",
    "      FROM\n",
    "         `mlab-sandbox.batch.sidestream*`\n",
    "      WHERE\n",
    "          (test_id LIKE '%\"\"\"+host+\"\"\"%')\n",
    "        AND betweenTimes(web100_log_entry.snap.StartTimeStamp, '\"\"\"+b_s+\"\"\"', '\"\"\"+b_e+\"\"\"')\n",
    "        AND sliceFromIP(web100_log_entry.connection_spec.local_ip) = 7\n",
    "                AND web100_log_entry.snap.HCThruOctetsAcked >=  819200                          \n",
    "        AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "            web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "            web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "        AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "            web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "            web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "        AND (web100_log_entry.snap.State = 1 OR                                       \n",
    "            (web100_log_entry.snap.State >= 5 AND                                       \n",
    "            web100_log_entry.snap.State <= 11))\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 10\n",
    "    ) USING (remote_ip))\n",
    "  )\n",
    "    \n",
    "GROUP BY\n",
    "  hostname, slice, period, ts, web100_log_entry.connection_spec.remote_ip, rate_mbps\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, slice, period, ts,  remote_ip\n",
    ")\n",
    "\n",
    "GROUP BY\n",
    "  hostname, slice, period, remote_ip\n",
    "\"\"\")\n",
    "        date_i = (i, a_s, b_s)\n",
    "        df_hosts[host][date_i] = result\n",
    "        print 'saved', i, date_i, host, len(df_hosts[host][date_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidestream CDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidestream PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF, CDF, & Switch - by Site and Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping mlab1.lax02 no data\n",
      "skipping mlab1.lax02 no data\n",
      "skipping mlab1.lax02 no data\n",
      "skipping mlab1.lga02 no data\n",
      "skipping mlab1.lga02 no data\n",
      "skipping mlab1.lga02 no data\n",
      "skipping mlab1.lga02 no data\n",
      "skipping mlab1.lga02 no data\n",
      "skipping mlab1.lga02 no data\n",
      "skipping mlab1.lga03 no data\n",
      "skipping mlab1.lga03 no data\n",
      "skipping mlab1.lga03 no data\n",
      "skipping mlab1.lga03 no data\n",
      "skipping mlab1.lga04 no data\n",
      "skipping mlab1.lga04 no data\n",
      "skipping mlab1.lga05 no data\n",
      "skipping mlab1.lga05 no data\n",
      "skipping mlab1.lga05 no data\n",
      "skipping mlab1.lga06 no data\n"
     ]
    }
   ],
   "source": [
    "title = 'PDF, CDF, Switch - slice sidestream Download Rates'\n",
    "\n",
    "label2date = {}\n",
    "slices = ['samknows']\n",
    "colors = plt.cm.tab10.colors\n",
    "p2c = {}\n",
    "c=0\n",
    "\n",
    "for i, host in enumerate(sorted(df_hosts.keys())):\n",
    "    rows = 4\n",
    "    cols = len(periods_list)\n",
    "    fig = plt.figure(figsize=(4 * cols, 13))\n",
    "    axes = [\n",
    "        [None] * cols,\n",
    "        [None] * cols,\n",
    "        [None] * cols,\n",
    "         None,\n",
    "    ]\n",
    "\n",
    "    for p, (x, p_a, p_b) in enumerate(sorted(df_hosts[host])):\n",
    "        axes[0][p] = plt.subplot2grid((rows, cols), (0, p))\n",
    "        axes[1][p] = plt.subplot2grid((rows, cols), (1, p))\n",
    "        axes[2][p] = plt.subplot2grid((rows, cols), (2, p))\n",
    "\n",
    "        for k, slicename in enumerate(slices):\n",
    "                \n",
    "            df_ss = df_hosts[host][(x, p_a, p_b)]\n",
    "            if len(df_ss) == 0:\n",
    "                print 'skipping', host, 'no data'\n",
    "                continue\n",
    "            if len(df_ss[ df_ss['hostname'] == host ]) == 0:\n",
    "                print 'skipping', host\n",
    "                continue\n",
    "\n",
    "            a = df_ss[ (df_ss['slice'] == slicename) & (df_ss['period'] == p_a) ]\n",
    "            b = df_ss[ (df_ss['slice'] == slicename) & (df_ss['period'] == p_b) ]\n",
    "\n",
    "            columns = ['hostname', 'remote_ip', 'slice']\n",
    "            ds = pd.merge(a, b,  how='left', left_on=columns, right_on=columns)\n",
    "            for period_str in [p_a, p_b]:\n",
    "                if period_str not in p2c:\n",
    "                    p2c[period_str] = colors[c]\n",
    "                    c += 1\n",
    "\n",
    "            if len(ds['sum_rate_mbps_x'].dropna()) == 0 or len(ds['sum_rate_mbps_y'].dropna()) == 0:\n",
    "                continue\n",
    "\n",
    "            # Top\n",
    "            ax = axes[0][p]\n",
    "            for period, l in [(p_a, ds['sum_rate_mbps_x']), (p_b, ds['sum_rate_mbps_y'])]:\n",
    "                vals = [math.log10(x) for x in l.dropna()]\n",
    "                period_str = period\n",
    "                label = 'pdf-%s-%s (%d)' % (period_str, slicename, len(vals))\n",
    "                label2date[label] = period\n",
    "                    \n",
    "                sqrt_bins = int(math.sqrt(len(vals)))\n",
    "                n, bins, patches = ax.hist(\n",
    "                    vals, sqrt_bins,\n",
    "                    histtype='step', normed=1, label=label, ls='-', color=p2c[period_str])\n",
    "\n",
    "            ax.set_axisbelow(True)\n",
    "            ax.legend(fontsize='x-small', loc='upper center', bbox_to_anchor=(0.5, 1.3))\n",
    "            ax.grid(color='#dddddd')\n",
    "            ax.set_title(host)\n",
    "            ax.xaxis.set_major_formatter(logFormatter)\n",
    "\n",
    "            # Middle\n",
    "            ax = axes[1][p]\n",
    "            for period, l in [(p_a, ds['sum_rate_mbps_x']), (p_b, ds['sum_rate_mbps_y'])]:\n",
    "                vals = [math.log10(x) for x in  l.dropna()]\n",
    "                period_str = period\n",
    "                label = 'cdf-%s-%s (%d)' % (period_str, slicename, len(vals))\n",
    "\n",
    "                n, bins, patches = ax.hist(\n",
    "                    vals, len(vals),\n",
    "                    histtype='step', normed=1, cumulative=True, label=label, ls='-', color=p2c[period_str])\n",
    "\n",
    "            ax.xaxis.set_major_formatter(logFormatter)\n",
    "            ax.set_axisbelow(True)\n",
    "\n",
    "            ax.grid(color='#dddddd')\n",
    "            ax.set_title(host)\n",
    "            if p != 0:\n",
    "                ax.set_yticklabels([])\n",
    "\n",
    "            # Scatter.\n",
    "            ax = axes[2][p]\n",
    "                    \n",
    "            label = 'scatter-%s (%d)/(%d)' % (slicename, len(ds['sum_rate_mbps_x']), len(ds['sum_rate_mbps_y']))\n",
    "                    \n",
    "            ax.plot([0.1, 1000], [0.1, 1000], color='r', alpha=0.1)\n",
    "            ax.add_patch(\n",
    "                matplotlib.patches.Polygon(\n",
    "                    [[.1, .1], [1000, .1], [1000, 1000], [.1, .1]], closed=True,\n",
    "                    fill=True, color=p2c[p_b], alpha=0.1))\n",
    "            ax.add_patch(\n",
    "                matplotlib.patches.Polygon(\n",
    "                    [[.1, .1], [.1, 1000], [1000, 1000], [.1, .1]], closed=True,\n",
    "                    fill=True, color=p2c[p_a], alpha=0.1))\n",
    "            ax.scatter(ds['sum_rate_mbps_y'], ds['sum_rate_mbps_x'], s=2, alpha=0.3, label=label)\n",
    "                    \n",
    "            ax.set_xlim(.1, 1000)\n",
    "            ax.set_ylim(.1, 1000)\n",
    "                    \n",
    "            ax.set_xlabel(p_b)\n",
    "            ax.set_ylabel(p_a)\n",
    "\n",
    "            ax.grid(color='#dddddd')\n",
    "            ax.semilogx()\n",
    "            ax.semilogy()\n",
    "            ax.legend(fontsize='x-small')\n",
    "\n",
    "            axes[0][p].set_xlim(math.log10(.1), math.log10(1100))\n",
    "            axes[1][p].set_xlim(math.log10(.1), math.log10(1100))\n",
    "\n",
    "    # Bottom\n",
    "    axes[3] = plt.subplot2grid((rows, cols), (3, 0), colspan=cols)\n",
    "    ax = axes[3]\n",
    "        \n",
    "    ds = df_disco[ df_disco['hostname'] == host ]\n",
    "    ax.plot_date(dates.epoch2num(ds['ts']), ds['pct_discards'], ls='-', ms=0, label='switch', color='mediumpurple')\n",
    "        \n",
    "    ax.set_title(host)\n",
    "    ax.set_ylim(-0.01, 1)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "    ax.grid(color='#dddddd')\n",
    " \n",
    "    # Color switch regions with the PDF periods based on legend colors.\n",
    "    for p in range(0, len(df_hosts[host])):\n",
    "        h, l = axes[0][p].get_legend_handles_labels()\n",
    "        for k, line in enumerate(h):\n",
    "            s = label2date[l[k]]\n",
    "            s = datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\")\n",
    "            e = s + datetime.timedelta(days=4)\n",
    "            color = h[k].get_edgecolor()\n",
    "            ax.axvspan(dates.date2num(s), dates.date2num(e), alpha=0.5, color=color)\n",
    "\n",
    "    ax.set_ylabel('% discard timebins')                \n",
    "    ax2 = ax.twinx()\n",
    "        \n",
    "    ds = df_ss_count[ df_ss_count['hostname'] == host ]\n",
    "            \n",
    "    ax2.plot_date(dates.epoch2num(ds['ts']), ds['count'], ls='-', ms=0, label='sidestream')\n",
    "    ax2.set_ylabel('Sidestream Flow Count')\n",
    "    ax2.grid(color='#dddddd')\n",
    "            \n",
    "    ax.legend(loc=3, fontsize='small') \n",
    "    ax2.legend(loc=1, fontsize='small') \n",
    "\n",
    "     \n",
    "    axes[0][0].set_ylabel('PDF')\n",
    "    axes[1][0].set_ylabel('CDF')  \n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
