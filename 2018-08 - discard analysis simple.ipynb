{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# Enables figures loading outside of browser.\n",
    "# If not run, figures will load inline.\n",
    "%matplotlib\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Depends on: pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')\n",
    "\n",
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def run_query(query, project='mlab-sandbox'):\n",
    "    #print query\n",
    "    client = bigquery.Client(project=project)\n",
    "    job = client.query(query)\n",
    "\n",
    "    results = collections.defaultdict(list)\n",
    "    for row in job.result(timeout=300):\n",
    "        for key in row.keys():\n",
    "            results[key].append(row.get(key))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def unlog(x, pos):\n",
    "    \"\"\"Formats the x axis for histograms taken on the log of values.\"\"\"\n",
    "    v = math.pow(10, x)\n",
    "    frac, whole = math.modf(v)\n",
    "    if frac > 0:\n",
    "        return '%.1f' % v\n",
    "    else:\n",
    "        return '%d' % whole\n",
    "\n",
    "logFormatter = matplotlib.ticker.FuncFormatter(unlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLINK UTILIZATION OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_pct = run_query(\"\"\"\n",
    "#standardSQL\n",
    "SELECT\n",
    "\n",
    "  name AS hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,  \n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(50)] as bytes_50th,\n",
    "  APPROX_QUANTILES(value, 101)[ORDINAL(90)] as bytes_90th\n",
    "\n",
    "FROM (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab1.[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    metric LIKE 'switch.octets.uplink.tx'\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "WHERE\n",
    "  name IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Median Uplink ' + site.upper())\n",
    "\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "        for h in set(df_disco_pct['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco_pct[ (df_disco_pct['hostname'] == h) & (df_disco_pct['bytes_50th'] > 1e5) ]\n",
    "                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "                axes[i][j].scatter(d, ds['bytes_50th'], s=1, label=h[6:11])\n",
    "                \n",
    "        axes[i][j].set_ylim(1e4, 1e9)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=3, ncol=7, fontsize='x-small', columnspacing=1)\n",
    "        axes[i][j].semilogy()\n",
    "\n",
    "fig.suptitle('Daily Median Uplink Utilization')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packets Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_packets = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "    \n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "    \n",
    "  WHERE\n",
    "    metric LIKE 'switch.unicast.uplink.tx'\n",
    "    \n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(value) AS total\n",
    "  \n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "  \n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "  \n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "  \n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Total Packets ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "\n",
    "        for h in set(df_disco_packets['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco_packets[ (df_disco_packets['hostname'] == h) & (df_disco_packets['total'] > 100) ]\n",
    "                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "                axes[i][j].scatter(d, ds['total'], s=1, label=h[6:11])\n",
    "\n",
    "        axes[i][j].set_ylim(1e7, 1e10)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=3, ncol=7, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.suptitle('Daily Packets')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DISCARDS OVER TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "    \n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "    \n",
    "  WHERE\n",
    "    metric LIKE 'switch.discards.uplink.tx'\n",
    "    \n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(value) AS total_discards\n",
    "  \n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "  \n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "  \n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "  \n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Total Discards ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "        #else:\n",
    "        #    axes[i][j].set_xlabel('Date')\n",
    "            \n",
    "        for h in set(df_disco['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco[ (df_disco['hostname'] == h) & (df_disco['total_discards'] > 100) ]\n",
    "                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "                axes[i][j].scatter(d, ds['total_discards'], s=1, label=h[6:11])\n",
    "\n",
    "        axes[i][j].set_ylim(100, 1000000)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.suptitle('Daily Packet Discards')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily DISCO discard ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_ratio = run_query(\"\"\"\n",
    "WITH measurementlab_switch_dedup AS (\n",
    "  SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "    sample.timestamp AS sts,\n",
    "    sample.value AS value\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "    (metric LIKE 'switch.discards.uplink.tx' OR metric LIKE 'switch.unicast.uplink.tx')\n",
    "    AND (hostname LIKE '%lga%' OR hostname LIKE '%dfw%' OR hostname LIKE '%nuq%')\n",
    "  GROUP BY\n",
    "    hostname, metric, sts, value\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  hostname,\n",
    "  day,\n",
    "  ts,\n",
    "  IF(total > 0, discards / total, 0) as ratio\n",
    "FROM (\n",
    "SELECT\n",
    "  hostname,\n",
    "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", TIMESTAMP_TRUNC(sts, DAY)) AS day,\n",
    "  UNIX_SECONDS(TIMESTAMP_TRUNC(sts, DAY)) AS ts,\n",
    "  SUM(IF(metric = \"switch.discards.uplink.tx\", value, 0)) AS discards,\n",
    "  SUM(IF(metric = \"switch.unicast.uplink.tx\", value, 0)) AS total\n",
    "FROM\n",
    "  measurementlab_switch_dedup\n",
    "WHERE\n",
    "  hostname IS NOT NULL\n",
    "GROUP BY\n",
    "  hostname, day, ts\n",
    "HAVING\n",
    "  discards < total\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    ")\n",
    "GROUP BY\n",
    "  hostname, day, ts, ratio\n",
    "HAVING\n",
    "  ratio < 0.01\n",
    "ORDER BY\n",
    "  hostname, day, ts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Discard Ratio ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "            \n",
    "        for h in set(df_disco_ratio['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_disco_ratio[ (df_disco_ratio['hostname'] == h) ]\n",
    "                d = [pd.to_datetime(t, unit='s') for t in ds['ts']]\n",
    "                axes[i][j].scatter(d, ds['ratio'], s=1, label=h[6:11])\n",
    "\n",
    "        axes[i][j].set_ylim(1e-6, 1e-2)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.suptitle('Daily Packet Loss Ratios')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDT Median Download Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_median = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "\n",
    "  FROM\n",
    "  \n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "    AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "    AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "    AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps\n",
    ")\n",
    "    \n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(log_time, DAY) as day,\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  APPROX_QUANTILES(download_mbps, 101)[ORDINAL(98)] as download_mbps\n",
    "\n",
    "FROM\n",
    "  mlab_ndt\n",
    "\n",
    "GROUP BY\n",
    "  day,\n",
    "  hostname\n",
    "\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8015\n"
     ]
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "print len(df_ndt_median)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('Median Downloads ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "            \n",
    "        for h in set(df_ndt_median['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_ndt_median[ (df_ndt_median['hostname'] == h) ]\n",
    "                d = [pd.to_datetime(t) for t in ds['day']]\n",
    "                axes[i][j].scatter(d, ds['download_mbps'], s=1, label=h[6:11])\n",
    "\n",
    "        #axes[i][j].set_ylim(100, 1000000)\n",
    "        axes[i][j].set_ylim(10, 1000)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.suptitle('Median NDT Downloads')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDT Segs Retrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT ENOUGH HISTORICAL NDT DATA TO GET FULL TIMELINE.\n",
    "\n",
    "df_ndt_retrans = run_query(\"\"\"\n",
    "#standardSQL\n",
    "\n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(log_time, DAY) AS day,\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  APPROX_QUANTILES(web100_log_entry.snap.SegsRetrans / web100_log_entry.snap.SegsOut, 101)[ORDINAL(50)] AS median_ratio\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND connection_spec.data_direction = 1\n",
    "  AND log_time >= TIMESTAMP(\"2016-06-01\")\n",
    "GROUP BY\n",
    "  day,\n",
    "  hostname\n",
    "ORDER BY\n",
    "  day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8015\n"
     ]
    }
   ],
   "source": [
    "sites = [\n",
    "    ['dfw'],\n",
    "    ['lga'],\n",
    "    ['nuq'],\n",
    "]\n",
    "\n",
    "axes = [\n",
    "    [None],\n",
    "    [None],\n",
    "    [None],\n",
    "]\n",
    "\n",
    "print len(df_ndt_retrans)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "for i, site_row in enumerate(sites):\n",
    "    for j, site in enumerate(site_row):\n",
    "        axes[i][j] = plt.subplot2grid((3, 1), (i, j))\n",
    "        axes[i][j].set_ylabel('SegsRetran Ratio ' + site.upper())\n",
    "        if i != len(sites)-1:\n",
    "            axes[i][j].set_xticklabels([])\n",
    "            \n",
    "        for h in set(df_ndt_retrans['hostname']):\n",
    "            if ('mlab1.' + site) in h:\n",
    "                ds = df_ndt_retrans[ (df_ndt_retrans['hostname'] == h) ]\n",
    "                d = [pd.to_datetime(t) for t in ds['day']]\n",
    "                axes[i][j].scatter(d, ds['median_ratio'], s=1, label=h[6:11])\n",
    "\n",
    "        #axes[i][j].set_ylim(100, 1000000)\n",
    "        axes[i][j].set_ylim(1e-6, 1e-2)\n",
    "        axes[i][j].set_xlim(pd.to_datetime(\"2016-05-31\"), pd.to_datetime(\"2018-08-01\"))\n",
    "        axes[i][j].tick_params(axis='x', labelrotation=-90)\n",
    "        axes[i][j].grid(color='#dddddd')\n",
    "        axes[i][j].legend(loc=2, fontsize='x-small')\n",
    "        axes[i][j].semilogy()\n",
    "        \n",
    "fig.suptitle('Median NDT SegsRetran/SegsOut')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One Week Performance Distributions -- Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_dist = run_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    CASE\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")          THEN 'before-2w'\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")          THEN 'before-1w'\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")          THEN 'after-1w'\n",
    "--        WHEN TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\")          THEN 'after-2w'\n",
    "        ELSE 'what'\n",
    "    END as period,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND ( -- One week - Sunday to Saturday.\n",
    "          TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-25\")\n",
    "       OR TIMESTAMP_TRUNC(sample.timestamp, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-18\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'bps', u'day', u'metric', u'name', u'period', u'sts'], dtype='object')\n",
      "517494\n"
     ]
    }
   ],
   "source": [
    "print df_disco_dist.keys()\n",
    "print len(df_disco_dist)\n",
    "\n",
    "\n",
    "\n",
    "for h in set(df_disco_dist['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist['period'])):\n",
    "        ds = df_disco_dist[ (df_disco_dist['name'] == h) & (df_disco_dist['period'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One Day Performance Distributions -- before & after\n",
    "\n",
    "Harder to notice changes wrt before  after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disco_dist = run_query(\"\"\"\n",
    "SELECT\n",
    "    metric,\n",
    "    REGEXP_EXTRACT(hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    TIMESTAMP_TRUNC(sample.timestamp, DAY) AS day,\n",
    "    UNIX_SECONDS(sample.timestamp) AS sts,\n",
    "    8 * sample.value / 10 AS bps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.switch*`,\n",
    "    UNNEST(sample) AS sample\n",
    "  WHERE\n",
    "       metric LIKE 'switch.octets.uplink.tx'\n",
    "    AND (\n",
    "        hostname LIKE '%mlab1.lga03%' OR hostname LIKE '%mlab1.dfw02%'\n",
    "    )\n",
    "    AND (\n",
    "        TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-18\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-02-25\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-11\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-04\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-05\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-10\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "--     OR TIMESTAMP_TRUNC(sample.timestamp, DAY) = TIMESTAMP(\"2018-03-01\")\n",
    "    )\n",
    "  GROUP BY\n",
    "    hostname, metric, sample.timestamp, bps\n",
    "  ORDER BY\n",
    "    hostname, sts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'bps', u'day', u'metric', u'name', u'sts'], dtype='object')\n",
      "69120\n"
     ]
    }
   ],
   "source": [
    "print df_disco_dist.keys()\n",
    "print len(df_disco_dist)\n",
    "\n",
    "\n",
    "\n",
    "for h in set(df_disco_dist['name']):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    for day in sorted(set(df_disco_dist['day'])):\n",
    "        ds = df_disco_dist[ (df_disco_dist['name'] == h) & (df_disco_dist['day'] == day) ]\n",
    "        r = [math.log10(x) for x in ds['bps']]\n",
    "            \n",
    "        axes.hist(r, # len(ds['bps']),\n",
    "                      int(1.5 * math.sqrt(len(ds['bps']))),\n",
    "                      histtype='step',\n",
    "                      density=1, # cumulative=True,\n",
    "                      label='cdf-'+h[6:11] + '-' + str(day), ls='-')\n",
    "\n",
    "        axes.set_title(h[6:11])\n",
    "        #axes.tick_params(axis='x', labelrotation=90)\n",
    "    axes.grid(color='#dddddd')\n",
    "    axes.legend(loc=2, fontsize='x-small')\n",
    "    axes.xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('Uplink Utilization Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NDT test distributions - Before & After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "-- ALL NDT tests before and after the flow-control configuration change.\n",
    "   \n",
    "SELECT\n",
    "    REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "    CASE\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")\n",
    "       THEN 'before-2w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")\n",
    "       THEN 'before-1w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")\n",
    "       THEN 'after-1w'\n",
    "     WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\")\n",
    "       THEN 'after-2w'\n",
    "     ELSE 'what'\n",
    "    END as period,\n",
    "\n",
    "    web100_log_entry.snap.StartTimeStamp AS ts,\n",
    "    8 * (web100_log_entry.snap.HCThruOctetsAcked /\n",
    "      (web100_log_entry.snap.SndLimTimeRwin +\n",
    "       web100_log_entry.snap.SndLimTimeCwnd +\n",
    "       web100_log_entry.snap.SndLimTimeSnd)) as download_mbps   \n",
    "FROM\n",
    "   `measurement-lab.base_tables.ndt*`\n",
    "   \n",
    "WHERE       \n",
    "    ( -- One week - Sunday to Saturday.\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") and TIMESTAMP(\"2018-02-18\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") and TIMESTAMP(\"2018-02-25\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") and TIMESTAMP(\"2018-03-11\")\n",
    "     OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") and TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) >= 9000000                             \n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin +                                   \n",
    "    web100_log_entry.snap.SndLimTimeCwnd +                                      \n",
    "    web100_log_entry.snap.SndLimTimeSnd) < 600000000                            \n",
    "    \n",
    "GROUP BY\n",
    "  name, period, ts, download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "  -- Weekly cohort - NDT tests before and after the flow-control configuration change.\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-18\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-1w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-11\") THEN 'after-1w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)) AS download_mbps\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  ( -- One week - Sunday to Saturday.\n",
    "    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-18\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-11\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\") AND TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN( (\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\")\n",
    "        AND TIMESTAMP(\"2018-02-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-18\")\n",
    "        AND TIMESTAMP(\"2018-02-25\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\")\n",
    "        AND TIMESTAMP(\"2018-03-11\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)\n",
    "    INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw02|lga03)\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-11\")\n",
    "        AND TIMESTAMP(\"2018-03-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip )\n",
    "    USING\n",
    "      (remote_ip)) )\n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "  -- Two-week cohorts - NDT tests before and after the flow-control configuration change.\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(connection_spec.server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  --web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "  (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  --MAX(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  --APPROX_QUANTILES(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd)), 101)[ORDINAL(50)] as download_mbps\n",
    "FROM\n",
    "  `measurement-lab.base_tables.ndt*`\n",
    "WHERE\n",
    "  ( -- One week - Sunday to Saturday.\n",
    "       TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  AND web100_log_entry.connection_spec.remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        web100_log_entry.connection_spec.remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        `measurement-lab.base_tables.ndt*`\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "        AND TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\")\n",
    "        AND connection_spec.data_direction = 1\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  --remote_ip\n",
    "  download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_dist = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "  (    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps)\n",
    "    \n",
    "SELECT\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS name,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  --web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "  --(8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  MAX(download_mbps) AS download_mbps\n",
    "  --APPROX_QUANTILES(download_mbps, 101)[ORDINAL(50)] as download_mbps\n",
    "\n",
    "FROM\n",
    "  mlab_ndt\n",
    "WHERE\n",
    "  remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 10\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  name,\n",
    "  period,\n",
    "  remote_ip\n",
    "  --download_mbps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'download_mbps', u'name', u'period'], dtype='object')\n",
      "48349\n",
      "mlab1.dfw01 1561\n",
      "39 mlab1.dfw01 before-2w\n",
      "mlab1.dfw01 1549\n",
      "39 mlab1.dfw01 after-2w\n",
      "mlab1.dfw02 1542\n",
      "39 mlab1.dfw02 before-2w\n",
      "mlab1.dfw02 1536\n",
      "39 mlab1.dfw02 after-2w\n",
      "mlab1.dfw03 1541\n",
      "39 mlab1.dfw03 before-2w\n",
      "mlab1.dfw03 1554\n",
      "39 mlab1.dfw03 after-2w\n",
      "mlab1.dfw04 1539\n",
      "39 mlab1.dfw04 before-2w\n",
      "mlab1.dfw04 1563\n",
      "39 mlab1.dfw04 after-2w\n",
      "mlab1.dfw05 1959\n",
      "44 mlab1.dfw05 before-2w\n",
      "mlab1.dfw05 1966\n",
      "44 mlab1.dfw05 after-2w\n",
      "mlab1.dfw06 1547\n",
      "39 mlab1.dfw06 before-2w\n",
      "mlab1.dfw06 1554\n",
      "39 mlab1.dfw06 after-2w\n",
      "mlab1.lga02 543\n",
      "23 mlab1.lga02 before-2w\n",
      "mlab1.lga02 2369\n",
      "23 mlab1.lga02 after-2w\n",
      "mlab1.lga03 1901\n",
      "43 mlab1.lga03 before-2w\n",
      "mlab1.lga03 1868\n",
      "43 mlab1.lga03 after-2w\n",
      "mlab1.lga04 1894\n",
      "43 mlab1.lga04 before-2w\n",
      "mlab1.lga04 1876\n",
      "43 mlab1.lga04 after-2w\n",
      "mlab1.lga05 2267\n",
      "47 mlab1.lga05 before-2w\n",
      "mlab1.lga05 2255\n",
      "47 mlab1.lga05 after-2w\n",
      "mlab1.lga06 1908\n",
      "43 mlab1.lga06 before-2w\n",
      "mlab1.lga06 1891\n",
      "43 mlab1.lga06 after-2w\n",
      "mlab1.lga07 1815\n",
      "42 mlab1.lga07 before-2w\n",
      "mlab1.lga07 1794\n",
      "42 mlab1.lga07 after-2w\n",
      "mlab1.nuq02 670\n",
      "25 mlab1.nuq02 before-2w\n",
      "mlab1.nuq02 678\n",
      "25 mlab1.nuq02 after-2w\n",
      "mlab1.nuq03 625\n",
      "25 mlab1.nuq03 before-2w\n",
      "mlab1.nuq03 666\n",
      "25 mlab1.nuq03 after-2w\n",
      "mlab1.nuq04 615\n",
      "24 mlab1.nuq04 before-2w\n",
      "mlab1.nuq04 611\n",
      "24 mlab1.nuq04 after-2w\n",
      "mlab1.nuq05 677\n",
      "26 mlab1.nuq05 before-2w\n",
      "mlab1.nuq05 670\n",
      "26 mlab1.nuq05 after-2w\n",
      "mlab1.nuq06 674\n",
      "25 mlab1.nuq06 before-2w\n",
      "mlab1.nuq06 671\n",
      "25 mlab1.nuq06 after-2w\n"
     ]
    }
   ],
   "source": [
    "print df_ndt_dist.keys()\n",
    "print len(df_ndt_dist)\n",
    "\n",
    "def hist(vals, bin_count, log=True, cdf=False):\n",
    "    \"\"\"Produces hist or cdf values for smooth plots.\"\"\"\n",
    "    if log:\n",
    "        r = [math.log10(x) for x in vals]\n",
    "    else:\n",
    "        r = vals\n",
    "        \n",
    "    m, bins = np.histogram(r, bin_count, normed=True)\n",
    "    m = m.astype(float)\n",
    "\n",
    "    tops = m\n",
    "    if cdf:\n",
    "        tops = np.cumsum(m)\n",
    "        total = sum(m)\n",
    "        tops = [float(t) / total for t in tops ]\n",
    "    \n",
    "    return tops, bins\n",
    "\n",
    "seq = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
    "\n",
    "for site in set([v[6:9] for v in set(df_ndt_dist['name'])]):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "    for p, h in enumerate(sorted([h for h in set(df_ndt_dist['name']) if site in h])):\n",
    "        before = None\n",
    "        for day in ['before-2w', 'after-2w']:\n",
    "            ds = df_ndt_dist[ (df_ndt_dist['name'] == h) & (df_ndt_dist['period'] == day) ]\n",
    "            r = ds['download_mbps']\n",
    "            print h, len(r)\n",
    "            if not len(r):\n",
    "                continue\n",
    "\n",
    "            size = int(math.sqrt(len(r)))\n",
    "            if day == 'after-2w':\n",
    "                size = before\n",
    "            else:\n",
    "                before = size\n",
    "            \n",
    "                \n",
    "#            tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=True , cdf=True)\n",
    "            #tops, bins = hist(r, int(math.sqrt(len(r))), log=True , cdf=True)\n",
    "            print size, h, day\n",
    "            tops, bins = hist(r, size, log=True , cdf=True)\n",
    "#            tops, bins = hist(r, int(1.8 * math.sqrt(len(r))), log=False , cdf=True)           \n",
    "#            tops, bins = hist(r, len(r), log=False , cdf=True)            \n",
    "            \n",
    "            #a, b = train_test_split(r, test_size=0.5)\n",
    "            #z = a\n",
    "            #print 'same', stats.ks_2samp(a,b)\n",
    "\n",
    "\n",
    "            #tops_a, bins_a = hist(a, int(1 * math.sqrt(len(a))), log=True, cdf=True)\n",
    "            #tops_b, bins_b = hist(b, int(1 * math.sqrt(len(b))), log=True, cdf=True)\n",
    "            if p > len(seq)-1:\n",
    "                print 'skipping', h\n",
    "                continue\n",
    "            i, j = seq[p]\n",
    "#            print h, len(bins), len(tops)\n",
    "            axes[i, j].plot(bins[:-1], tops, label='cdf-'+h[6:11] + '-' + str(day))\n",
    "            #axes[i, j].plot(bins_a[:-1], tops_a, label=h[6:11] + '-' + str(day)+'-a')\n",
    "            #axes[i, j].plot(bins_b[:-1], tops_b, label=h[6:11] + '-' + str(day)+'-b')\n",
    "            axes[i, j].set_title(h[6:11])\n",
    "            #axes[i, j].set_xlim(-10, 1000)\n",
    "            axes[i, j].set_xlim(math.log10(.25), math.log10(1000))\n",
    "            axes[i, j].grid(color='#dddddd')\n",
    "            axes[i, j].legend(loc=4, fontsize='x-small')\n",
    "            axes[i, j].set_ylim(-0.1, 1.1)\n",
    "            axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('NDT Download Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(n), len(bins[:-1])\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "\n",
    "#axes.plot(bins[:-1], n)\n",
    "#fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "#print f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m, bins = np.histogram(r, int(math.sqrt(len(ds['download_mbps']))))\n",
    "#print m, bins, len(m), len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndt_variance = run_query(\"\"\"\n",
    "WITH mlab_ndt AS (\n",
    "  SELECT\n",
    "    connection_spec.server_hostname as server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip AS remote_ip,\n",
    "    (8 * (web100_log_entry.snap.HCThruOctetsAcked / (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd))) AS download_mbps\n",
    "  FROM\n",
    "    `measurement-lab.base_tables.ndt*`\n",
    "  WHERE\n",
    "\n",
    "  (    TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "    OR TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\"))\n",
    "  AND REGEXP_CONTAINS(connection_spec.server_hostname, r\"mlab1.(dfw|lga|nuq)\\d\\d\")\n",
    "  AND web100_log_entry.snap.HCThruOctetsAcked >= 1000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) >= 9000000\n",
    "  AND (web100_log_entry.snap.SndLimTimeRwin + web100_log_entry.snap.SndLimTimeCwnd + web100_log_entry.snap.SndLimTimeSnd) < 600000000\n",
    "  AND connection_spec.data_direction = 1\n",
    "  \n",
    "  GROUP BY\n",
    "    server_hostname,\n",
    "    log_time,\n",
    "    web100_log_entry.connection_spec.remote_ip,\n",
    "    web100_log_entry.connection_spec.local_ip,\n",
    "    web100_log_entry.connection_spec.remote_port,\n",
    "    web100_log_entry.connection_spec.local_port,\n",
    "    download_mbps)\n",
    "\n",
    "\n",
    "SELECT\n",
    "  REGEXP_EXTRACT(server_hostname, r'(mlab[1-4].[a-z]{3}[0-9]{2}).*') AS hostname,\n",
    "  CASE\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\") THEN 'before-2w'\n",
    "    WHEN TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\") THEN 'after-2w'\n",
    "    ELSE 'what'\n",
    "  END AS period,\n",
    "  remote_ip,\n",
    "  STDDEV(download_mbps) AS download_stddev\n",
    "\n",
    "FROM\n",
    "  mlab_ndt\n",
    "WHERE\n",
    "  remote_ip IN(\n",
    "    SELECT\n",
    "      remote_ip\n",
    "    FROM (\n",
    "      SELECT\n",
    "        remote_ip, count(*) as c1\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-02-11\") AND TIMESTAMP(\"2018-02-25\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c1 > 10\n",
    "    ) INNER JOIN (\n",
    "      SELECT\n",
    "        remote_ip AS remote_ip, count(*) as c2\n",
    "      FROM\n",
    "        mlab_ndt\n",
    "      WHERE\n",
    "        TIMESTAMP_TRUNC(log_time, DAY) BETWEEN TIMESTAMP(\"2018-03-04\") AND TIMESTAMP(\"2018-03-18\")\n",
    "      GROUP BY\n",
    "        remote_ip\n",
    "      HAVING c2 > 10\n",
    "    ) USING (remote_ip)) \n",
    "GROUP BY\n",
    "  hostname,\n",
    "  period,\n",
    "  remote_ip\n",
    "  --download_mbps\n",
    "\n",
    "HAVING download_stddev is not NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48349\n"
     ]
    }
   ],
   "source": [
    "print len(df_ndt_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlab1.dfw01 1412\n",
      "mlab1.dfw01 1416\n",
      "mlab1.dfw02 1444\n",
      "mlab1.dfw02 1450\n",
      "mlab1.dfw03 1408\n",
      "mlab1.dfw03 1438\n",
      "mlab1.dfw04 1421\n",
      "mlab1.dfw04 1450\n",
      "mlab1.dfw05 1851\n",
      "mlab1.dfw05 1855\n",
      "mlab1.dfw06 1431\n",
      "mlab1.dfw06 1449\n",
      "mlab1.lga02 291\n",
      "mlab1.lga02 2230\n",
      "mlab1.lga03 1786\n",
      "mlab1.lga03 1717\n",
      "mlab1.lga04 1764\n",
      "mlab1.lga04 1717\n",
      "mlab1.lga05 2177\n",
      "mlab1.lga05 2098\n",
      "mlab1.lga06 1805\n",
      "mlab1.lga06 1740\n",
      "mlab1.lga07 1701\n",
      "mlab1.lga07 1646\n",
      "mlab1.nuq02 645\n",
      "mlab1.nuq02 644\n",
      "mlab1.nuq03 558\n",
      "mlab1.nuq03 629\n",
      "mlab1.nuq04 562\n",
      "mlab1.nuq04 560\n",
      "mlab1.nuq05 627\n",
      "mlab1.nuq05 638\n",
      "mlab1.nuq06 641\n",
      "mlab1.nuq06 641\n"
     ]
    }
   ],
   "source": [
    "seq = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
    "\n",
    "for site in set([v[6:9] for v in set(df_ndt_variance['hostname'])]):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "    for p, h in enumerate(sorted([h for h in set(df_ndt_variance['hostname']) if site in h])):\n",
    "        before = None\n",
    "        for day in ['before-2w', 'after-2w']:\n",
    "            ds = df_ndt_variance[ (df_ndt_variance['hostname'] == h) & (df_ndt_variance['period'] == day) ]\n",
    "            r = ds['download_stddev']\n",
    "            print h, len(r)\n",
    "            if not len(r):\n",
    "                continue\n",
    "\n",
    "            size = int(math.sqrt(len(r)))\n",
    "            if day == 'after-2w':\n",
    "                size = before\n",
    "            else:\n",
    "                before = size\n",
    "\n",
    "            tops, bins = hist(r, size, log=True, cdf=True)\n",
    "            if p > len(seq)-1:\n",
    "                print 'skipping', h\n",
    "                continue\n",
    "            i, j = seq[p]\n",
    "\n",
    "            axes[i, j].plot(bins[:-1], tops, label='cdf-'+h[6:11] + '-' + str(day))\n",
    "    \n",
    "            axes[i, j].set_title(h[6:11])\n",
    "            #axes[i, j].set_xlim(-10, 1000)\n",
    "            #axes[i, j].set_xlim(math.log10(.25), math.log10(1000))\n",
    "            axes[i, j].grid(color='#dddddd')\n",
    "            axes[i, j].legend(loc=2, fontsize='x-small')\n",
    "            #axes[i, j].set_ylim(-0.1, 1.1)\n",
    "            axes[i, j].xaxis.set_major_formatter(logFormatter)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "    fig.suptitle('NDT Download Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
